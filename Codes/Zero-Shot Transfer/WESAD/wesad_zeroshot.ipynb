{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_p = \"<path to the dataset>\"\n",
    "files = [\n",
    "    (file, sub_file)\n",
    "    for file in os.listdir(dir_p)\n",
    "    for sub_file in os.listdir(os.path.join(dir_p, file))\n",
    "]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_f = []\n",
    "for i in files:\n",
    "    x =  i[1]\n",
    "    try:\n",
    "        if x[-4:] == \".pkl\":\n",
    "            files_f.append(i)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = os.listdir(dir_p)\n",
    "\n",
    "for i in range(len(ids)):\n",
    "    dir_t = f\"{dir_p}/{files_f[i][0]}/{files_f[i][1]}\"\n",
    "    df = pd.read_pickle(dir_t)\n",
    "    print(df.keys())\n",
    "    eda = df['signal']['wrist']['EDA'].tolist()\n",
    "    id_eda = [files_f[i][0] for t in range(len(eda))]\n",
    "    label_eda = []\n",
    "    lab_v = df['label'].tolist()\n",
    "    i = 0\n",
    "    while i < len(df['label']):\n",
    "        s = int(sum(df['label'][i:i+175])/175)\n",
    "        i += 175\n",
    "        label_eda.append(s)\n",
    "    \n",
    "    print(len(eda))\n",
    "    print(id_eda)\n",
    "    print(set(id_eda))\n",
    "    print(len(id_eda))\n",
    "    print(len(label_eda))\n",
    "    print(set(label_eda))\n",
    "    new_data = pd.DataFrame({\n",
    "    'EDA': eda,\n",
    "    'PID': id_eda,\n",
    "    'label': label_eda\n",
    "    })\n",
    "    final_df = pd.concat([final_df, new_data], ignore_index=True)\n",
    "    \n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(final_df['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['PID', 'label', 'PPG']\n",
    "ppg_df = pd.DataFrame(columns=columns)\n",
    "ppg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = os.listdir(dir_p)\n",
    "\n",
    "for i in range(len(ids)):\n",
    "    dir_t = f\"{dir_p}/{files_f[i][0]}/{files_f[i][1]}\"\n",
    "    df = pd.read_pickle(dir_t)\n",
    "    print(df.keys())\n",
    "    ppg = df['signal']['wrist']['BVP'].tolist()\n",
    "    id_ppg = [files_f[i][0] for t in range(len(ppg))]\n",
    "    label_ppg = []\n",
    "    lab_v = df['label'].tolist()\n",
    "    i = 0\n",
    "    while i < len(df['label']):\n",
    "        k = int(i)\n",
    "        s = int(sum(df['label'][k:k+11])/11)\n",
    "        i += 10.9375\n",
    "        label_ppg.append(s)\n",
    "    \n",
    "    \n",
    "    new_data = pd.DataFrame({\n",
    "    'PPG': ppg,\n",
    "    'PID': id_ppg,\n",
    "    'label': label_ppg\n",
    "    })\n",
    "    ppg_df = pd.concat([ppg_df, new_data], ignore_index=True)\n",
    "    # final_df['PID'] = final_df['PID'].tolist() + id_eda\n",
    "    # final_df['label'] = final_df['label'].tolist() + label_eda\n",
    "    \n",
    "\n",
    "ppg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppg_df['PID'].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['arousal'] = 1\n",
    "final_df['valence'] = 1\n",
    "final_df\n",
    "\n",
    "for index, row in final_df.iterrows():\n",
    "    if row['label'] == 0:\n",
    "        final_df.at[index,'arousal'] = -1\n",
    "        final_df.at[index,'valence'] = -1\n",
    "\n",
    "    if row['label'] == 1:\n",
    "        final_df.at[index,'arousal'] = 0\n",
    "        final_df.at[index,'valence'] = 0\n",
    "    \n",
    "    if row['label'] == 2:\n",
    "        final_df.at[index,'arousal'] = 1\n",
    "        final_df.at[index,'valence'] = 0\n",
    "\n",
    "    if row['label'] == 3:\n",
    "        final_df.at[index,'arousal'] = 0\n",
    "        final_df.at[index,'valence'] = 1\n",
    "\n",
    "    if row['label'] == 4:\n",
    "        final_df.at[index,'arousal'] = 1\n",
    "        final_df.at[index,'valence'] = 1\n",
    "\n",
    "    if row['label'] == 5:\n",
    "        final_df.at[index,'arousal'] = -1\n",
    "        final_df.at[index,'valence'] = -1\n",
    "\n",
    "    if row['label'] == 6:\n",
    "        final_df.at[index,'arousal'] = -1\n",
    "        final_df.at[index,'valence'] = -1\n",
    "\n",
    "    if row['label'] == 7:\n",
    "        final_df.at[index,'arousal'] = -1\n",
    "        final_df.at[index,'valence'] = -1\n",
    "\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df['arousal'].tolist().count(0))\n",
    "print(final_df['arousal'].tolist().count(1))\n",
    "print(final_df['valence'].tolist().count(0))\n",
    "print(final_df['valence'].tolist().count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['valence'].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppg_df['arousal'] = 1\n",
    "ppg_df['valence'] = 1\n",
    "ppg_df\n",
    "\n",
    "for index, row in ppg_df.iterrows():\n",
    "    if row['label'] == 0:\n",
    "        ppg_df.at[index,'arousal'] = -1\n",
    "        ppg_df.at[index,'valence'] = -1\n",
    "\n",
    "    if row['label'] == 1:\n",
    "        ppg_df.at[index,'arousal'] = 0\n",
    "        ppg_df.at[index,'valence'] = 0\n",
    "    \n",
    "    if row['label'] == 2:\n",
    "        ppg_df.at[index,'arousal'] = 1\n",
    "        ppg_df.at[index,'valence'] = 0\n",
    "\n",
    "    if row['label'] == 3:\n",
    "        ppg_df.at[index,'arousal'] = 0\n",
    "        ppg_df.at[index,'valence'] = 1\n",
    "\n",
    "    if row['label'] == 4:\n",
    "        ppg_df.at[index,'arousal'] = 1\n",
    "        ppg_df.at[index,'valence'] = 1\n",
    "\n",
    "    if row['label'] == 5:\n",
    "        ppg_df.at[index,'arousal'] = -1\n",
    "        ppg_df.at[index,'valence'] = -1\n",
    "\n",
    "    if row['label'] == 6:\n",
    "        ppg_df.at[index,'arousal'] = -1\n",
    "        ppg_df.at[index,'valence'] = -1\n",
    "\n",
    "    if row['label'] == 7:\n",
    "        ppg_df.at[index,'arousal'] = -1\n",
    "        ppg_df.at[index,'valence'] = -1\n",
    "\n",
    "\n",
    "ppg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ppg_df['arousal'].tolist().count(0))\n",
    "print(ppg_df['arousal'].tolist().count(1))\n",
    "print(ppg_df['valence'].tolist().count(0))\n",
    "print(ppg_df['valence'].tolist().count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(ppg_df['arousal'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv(\"wesad_eda_zero_shot.csv\")\n",
    "ppg_df = pd.read_csv(\"wesad_ppg_zeroshot.csv\")\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# cvxEDA\n",
    "def eda_stats(y):\n",
    "    Fs = 15.625\n",
    "    yn = (y - y.mean()) / y.std()\n",
    "    [r, p, t, l, d, e, obj] = cvxEDA.cvxEDA(yn, 1. / Fs)\n",
    "    return [r, p, t, l, d, e, obj]\n",
    "\n",
    "def shannon_entropy(window):\n",
    "    p = np.abs(window) / np.sum(np.abs(window))\n",
    "    return -np.sum(p * np.log2(p + 1e-10))\n",
    "\n",
    "def first_derivative(signal):\n",
    "    if len(signal) > 1:\n",
    "        time_values = np.arange(len(signal))\n",
    "        first_derivative = np.gradient(signal, time_values)\n",
    "        return first_derivative\n",
    "    else:\n",
    "        return np.array([])\n",
    "\n",
    "\n",
    "def second_derivative(signal):\n",
    "    fd = first_derivative(signal)\n",
    "    time_values = np.arange(len(fd))\n",
    "    second_derivative = np.gradient(first_derivative)\n",
    "    return second_derivative\n",
    "\n",
    "\n",
    "def calculate_integral(window):\n",
    "    a = np.sum(np.abs(window))\n",
    "    return a\n",
    "\n",
    "def calculate_avg_power(window):\n",
    "    avg_power = np.mean(np.square(np.abs(window)))\n",
    "    return avg_power\n",
    "\n",
    "def calculate_arc_length(window):\n",
    "    diff_signal = np.diff(window)\n",
    "    arc_length = np.sum(np.sqrt(1 + np.square(diff_signal)))\n",
    "    return arc_length\n",
    "\n",
    "def slope(window):\n",
    "    if len(window) > 1:\n",
    "        time_values = np.arange(len(window))\n",
    "        slope, _ = np.polyfit(time_values, window, 1)\n",
    "        return slope\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import neurokit2 as nk\n",
    "import cvxEDA.src.cvxEDA as cvxEDA\n",
    "import scipy.stats as stats\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "from sklearn.manifold import TSNE\n",
    "warnings.filterwarnings('ignore')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = {}\n",
    "for pi in final_df['PID'].unique():\n",
    "    df_pi = final_df[final_df['PID'] == pi]\n",
    "    for vi in final_df['label'].unique():\n",
    "        # print(pi,vi)\n",
    "        df_vi =  df_pi[df_pi['label'] == vi]\n",
    "        tag = str(pi) + '_' + str(vi)\n",
    "        df_list[tag] = (df_vi)\n",
    "\n",
    "\n",
    "df_list.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list['S10_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_freq = 4\n",
    "out_df = pd.DataFrame()\n",
    "\n",
    "for i in df_list:\n",
    "\n",
    "    row_dict = {}\n",
    "\n",
    "    pid = df_list[i]['PID'].tolist()[0]\n",
    "    vid = df_list[i]['label'].tolist()[0]\n",
    "    arousal = df_list[i]['arousal'].tolist()[0]\n",
    "    valence = df_list[i]['valence'].tolist()[0]\n",
    "\n",
    "    if vid == 0 or vid == 5 or vid == 6 or vid == 7:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "\n",
    "        eda_data = np.array(df_list[i]['EDA'].tolist())\n",
    "        eda_data = eda_data.flatten()\n",
    "        print(eda_data)\n",
    "\n",
    "        eda_clean = nk.eda_clean(eda_data, sampling_rate=sampling_freq) #Cleaning eda signal using NK\n",
    "        x = np.array(eda_clean)\n",
    "    #     # r, p, t, l, d, e, obj = eda_stats(eda_clean) #cvxeda for seperating phasic and tonic\n",
    "    #     # scr = r \n",
    "    #     # scl = t \n",
    "        print(i)\n",
    "        eda = nk.eda_phasic(x, sampling_freq)\n",
    "        scr = np.array(eda['EDA_Phasic'])\n",
    "        scl = np.array(eda['EDA_Tonic'])\n",
    "\n",
    "        x_axis = np.linspace(0, scl.shape[0]/sampling_freq, scl.shape[0])\n",
    "        \n",
    "    #     #raw eda features\n",
    "        row_dict['mean'] = np.mean(x) # Mean\n",
    "        row_dict['std'] = np.std(x) # Standard Deviation\n",
    "        row_dict['min'] = np.min(x) # Minimum\n",
    "        row_dict['max'] = np.max(x) # Maximum\n",
    "        row_dict['median_eda'] = np.quantile(x,0.5) #median\n",
    "        row_dict['ku_eda'] = stats.kurtosis(x) #kurtosis\n",
    "        row_dict['sk_eda'] = stats.skew(x) #skewness\n",
    "        row_dict['dynrange'] = x.max()/x.min()#dynamic range\n",
    "        row_dict['slope'] = np.polyfit(x_axis,scl,1)[0] #slope\n",
    "        row_dict['variance'] = np.var(x) # Variance\n",
    "        row_dict['entropy'] = shannon_entropy(x) # Shannon Entropy\n",
    "        row_dict['insc'] = calculate_integral(x) # insc\n",
    "        fd = first_derivative(x)\n",
    "        row_dict['fd_mean'] = np.mean(fd)\n",
    "        row_dict['fd_std'] = np.std(fd)\n",
    "\n",
    "    #     #scr features\n",
    "        row_dict['max_scr'] = np.max(scr) #min\n",
    "        row_dict['min_scr'] = np.min(scr) #max\n",
    "        row_dict['mean_scr'] = np.mean(scr) # Mean\n",
    "        row_dict['sd_scr'] = np.std(scr) # Standard Deviation\n",
    "\n",
    "        _, info = nk.eda_peaks(scr, sampling_freq) #scr peak\n",
    "        peaks = info['SCR_Peaks']\n",
    "        amplitude = info['SCR_Amplitude']\n",
    "        recovery = info['SCR_RecoveryTime']\n",
    "        \n",
    "        row_dict['nSCR'] = len(info['SCR_Peaks']) / (x.shape[0]/sampling_freq/60) #to get the number of peaks per minute\n",
    "        row_dict['aucSCR'] = np.trapz(scr)\n",
    "        row_dict['meanAmpSCR'] = np.nanmean(amplitude)\n",
    "        row_dict['maxAmpSCR'] = np.nanmax(amplitude)\n",
    "        row_dict['meanRespSCR'] = np.nanmean(recovery)\n",
    "        row_dict['sumAmpSCR'] = np.nansum(amplitude) / (x.shape[0]/sampling_freq/60) # per minute\n",
    "        row_dict['sumRespSCR'] = np.nansum(recovery) / (x.shape[0]/sampling_freq/60) # per minute\n",
    "\n",
    "        #scl features\n",
    "        row_dict['max_scl'] = np.max(scl) #min\n",
    "        row_dict['min_scl'] = np.min(scl) #max\n",
    "        row_dict['mean_scl'] = np.mean(scl) # Mean\n",
    "        row_dict['sd_scl'] = np.std(scl) # Standard Deviation\n",
    "\n",
    "        row_dict['Participant ID'] = pid\n",
    "        row_dict['label'] = vid\n",
    "        row_dict['arousal'] = arousal\n",
    "        row_dict['valence'] = valence\n",
    "    #     row_dict['Gender'] = gender\n",
    "    #     row_dict['CMA'] = cma\n",
    "    #     row_dict['Video_ID_number'] = vnum\n",
    "    #     print(row_dict.keys())\n",
    "\n",
    "        new_row = pd.DataFrame(row_dict , index=[0])\n",
    "        out_df = pd.concat([out_df, new_row], ignore_index=True)\n",
    "\n",
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_nan = out_df.columns[out_df.isna().any()].tolist()\n",
    "columns_with_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_features = ['ku_eda','sk_eda','dynrange','slope','variance','entropy','insc','fd_mean',\n",
    "                     'max_scr','min_scr','nSCR','meanAmpSCR','meanRespSCR','sumAmpSCR','sumRespSCR']\n",
    "\n",
    "\n",
    "identifiers = [\"Participant ID\", \"label\", \"arousal\", \"valence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = out_df['arousal'].tolist()\n",
    "l = len(a_list)\n",
    "o = sum(a_list)\n",
    "z = l - o\n",
    "print(o,z,l)\n",
    "\n",
    "a_list = out_df['valence'].tolist()\n",
    "l = len(a_list)\n",
    "o = sum(a_list)\n",
    "z = l - o\n",
    "print(o,z,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_fet = out_df[['mean', 'std', 'min', 'max', 'median_eda', 'ku_eda', 'sk_eda',\n",
    "       'dynrange', 'slope', 'variance', 'entropy', 'insc', 'fd_mean', 'fd_std',\n",
    "       'max_scr', 'min_scr', 'mean_scr', 'sd_scr', 'nSCR', 'aucSCR',\n",
    "       'meanAmpSCR', 'maxAmpSCR', 'meanRespSCR', 'sumAmpSCR', 'sumRespSCR',\n",
    "       'max_scl', 'min_scl', 'mean_scl', 'sd_scl', \"Participant ID\", \"label\", \"arousal\", \"valence\"]]\n",
    "\n",
    "eda_fet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_nan = eda_fet.columns[eda_fet.isna().any()].tolist()\n",
    "columns_with_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "# List of columns to be scaled\n",
    "columns_to_scale = eda_fet.columns.difference([\"Participant ID\", \"label\", \"arousal\", \"valence\"])\n",
    "\n",
    "# Function to scale participant-wise\n",
    "def scale_participant(df):\n",
    "    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "    return df\n",
    "\n",
    "# Apply the scaling function to each participant's data\n",
    "scaled_eda_fet = eda_fet.groupby('Participant ID').apply(scale_participant).reset_index(drop=True)\n",
    "\n",
    "scaled_eda_fet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = eda_fet['arousal'].tolist()\n",
    "l = len(a_list)\n",
    "o = sum(a_list)\n",
    "z = l - o\n",
    "print(o,z,l)\n",
    "\n",
    "a_list = eda_fet['valence'].tolist()\n",
    "l = len(a_list)\n",
    "o = sum(a_list)\n",
    "z = l - o\n",
    "print(o,z,l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as mp\n",
    "import seaborn as sb\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from positional_encodings.torch_encodings import PositionalEncoding1D, PositionalEncoding2D, PositionalEncoding3D, Summer\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import torchmetrics\n",
    "from sklearn.metrics import classification_report\n",
    "from torchmetrics import F1Score\n",
    "from tqdm.autonotebook import tqdm\n",
    "import itertools\n",
    "import random \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_read(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "def lowercase(data):\n",
    "  temp  = data.lower()\n",
    "  return temp\n",
    "\n",
    "def stop_words(text):\n",
    "  stop_words_set = set(stopwords.words('english'))\n",
    "  word_tokens = word_tokenize(text)\n",
    "  filtered_sentence = [word for word in word_tokens if word.lower() not in stop_words_set]\n",
    "  return \" \".join(filtered_sentence)\n",
    "\n",
    "def punctuations(data):\n",
    "  no_punct=[words for words in data if words not in string.punctuation]\n",
    "  words_wo_punct=''.join(no_punct)\n",
    "  return words_wo_punct\n",
    "\n",
    "def lemmatize(text):\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  word_tokens = word_tokenize(text)\n",
    "  lemmatized_text = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "  return \" \".join(lemmatized_text)\n",
    "\n",
    "def preprcosess(text_data):\n",
    "    text_data['Text'] = text_data['Text Description'].apply(lambda x: lemmatize(x))\n",
    "    text_data['Text'] = text_data['Text'].apply(lambda x: stop_words(x))\n",
    "    text_data['Text'] = text_data['Text'].apply(lambda x: lowercase(x))\n",
    "    text_data['Text'] = text_data['Text'].apply(lambda x: punctuations(x))\n",
    "    return text_data\n",
    "\n",
    "def balanced_df(eda_df):\n",
    "    filtered_rows = []\n",
    "    for index, row in tqdm(eda_df.iterrows()):\n",
    "        if row['arousal_category'] == 1:\n",
    "            filtered_rows.append(row)\n",
    "\n",
    "    filtered_df = pd.DataFrame(filtered_rows)\n",
    "    balance_a_eda_df = pd.concat([eda_df, filtered_df], ignore_index=True)\n",
    "    balance_a_eda_df  = balance_a_eda_df.reset_index(drop=True)\n",
    "    return balance_a_eda_df\n",
    "\n",
    "class Text_Encoder(nn.Module):\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\", pretrained=True, trainable=True):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.model = DistilBertModel.from_pretrained(model_name).to(torch.device(\"cuda\"))\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "        \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "        \n",
    "\n",
    "\n",
    "    def text_tokens(self,batch):\n",
    "        text_embeddings = []\n",
    "        for i in range(len(batch)):\n",
    "            texts = batch[i]\n",
    "            tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "            model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(torch.device(\"cuda\"))\n",
    "                \n",
    "            # Tokenize and get embeddings\n",
    "            encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt').to(torch.device(\"cuda\"))\n",
    "            with torch.no_grad():\n",
    "                model_output = model(**encoded_input)\n",
    "\n",
    "            # Extract embeddings from the last hidden state\n",
    "            embeddings = model_output.last_hidden_state\n",
    "\n",
    "            # Get the embeddings for the [CLS] token (the first token)\n",
    "            cls_embeddings = embeddings[:, 0, :]\n",
    "\n",
    "            # Alternatively, mean pooling the token embeddings to get sentence-level embeddings\n",
    "            sentence_embeddings = torch.mean(embeddings, dim=1)\n",
    "\n",
    "            text_embeddings.append(sentence_embeddings)\n",
    "\n",
    "        return text_embeddings\n",
    "    \n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=100,  # Change projection_dim to 100\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        text_embeddings = []\n",
    "        for i in range(len(batch)):\n",
    "            x = batch[i]\n",
    "            projected = self.projection(x)\n",
    "            x = self.gelu(projected)\n",
    "            x = self.fc(x)\n",
    "            x = self.dropout(x)\n",
    "            x = x + projected\n",
    "            x = self.layer_norm(x)\n",
    "            text_embeddings.append(x)\n",
    "        return text_embeddings\n",
    "    \n",
    "class CustomMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(CustomMLP, self).__init__()\n",
    "\n",
    "        # Define hidden layer dimensions\n",
    "        hidden_dims = [50, 100]\n",
    "\n",
    "        # Create sequential layers using nn.Linear and nn.ReLU activations\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_dims[1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def get_hidden_embedding(self, x):\n",
    "        x = self.layer1(x)\n",
    "        return self.layer2(x)\n",
    "    \n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets, text):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.text  = text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = {}\n",
    "        item['data'] = torch.from_numpy(self.data[index]).float()\n",
    "        item['target'] = self.targets[index]\n",
    "        item['text'] = self.text[index]\n",
    "        return item\n",
    "    \n",
    "def windowed_preprocess(train_df, test_df):\n",
    "    train = []\n",
    "    test = []\n",
    "    for index, row in train_df.iterrows():\n",
    "        # Convert each row (Series) into a list of NumPy arrays\n",
    "        row_as_list1 = np.array([np.array(value) for value in row.to_numpy()])\n",
    "        train.append(row_as_list1)\n",
    "\n",
    "    for index, row in test_df.iterrows():\n",
    "        # Convert each row (Series) into a list of NumPy arrays\n",
    "        row_as_list2 = np.array([np.array(value) for value in row.to_numpy()])\n",
    "        test.append(row_as_list2)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n",
    "\n",
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, mlp_input_dim, mlp_output_dim, device):\n",
    "        super().__init__()\n",
    "        self.text_encoder = Text_Encoder().to(device)\n",
    "        self.eda_encoder = CustomMLP(mlp_input_dim, mlp_output_dim).to(device)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=768, projection_dim=100).to(device)\n",
    "        self.device = device\n",
    "        \n",
    "    def eda_train(self,learning_rate, beta1, beta2 , epsilon , train_dataloader):\n",
    "        criterion= nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam( self.eda_encoder.parameters(), lr=learning_rate, betas=(beta1, beta2), eps=epsilon)\n",
    "        optimizer.zero_grad()\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            data = batch['data']\n",
    "            target = batch['target']\n",
    "\n",
    "            data, target = data.to(self.device), target.to(self.device)  # Move data and target to GPU\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = self.eda_encoder(data)\n",
    "\n",
    "            target = target.unsqueeze(1).float()\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        data_values = batch['data']\n",
    "        text_values = batch['text']\n",
    "\n",
    "        eda_embeddings = self.eda_encoder.get_hidden_embedding(data_values).to(self.device)\n",
    "        text_embeddings  = self.text_encoder.text_tokens(batch[\"text\"])#.to(self.device)\n",
    "        text_embeddings = self.text_projection(text_embeddings)#.to(self.device)\n",
    "        text_embeddings = torch.stack(text_embeddings).to(self.device)\n",
    "        eda_tensor = eda_embeddings\n",
    "\n",
    "        # Calculating the Loss\n",
    "        text_embeddings = text_embeddings.squeeze(1)\n",
    "        logits = torch.matmul(text_embeddings, eda_tensor.T)\n",
    "        # print(\"Logits \",logits.shape)\n",
    "        eda_similarity = torch.matmul(eda_tensor, eda_tensor.T)\n",
    "        # print(\"eda_similarity \",eda_similarity.shape)\n",
    "        input_size = text_embeddings.size(0) * text_embeddings.size(1)\n",
    "        # print(\"Text Embedding \",text_embeddings.shape)\n",
    "        texts_similarity = torch.matmul(text_embeddings, text_embeddings.T)\n",
    "        targets = F.softmax((eda_similarity + texts_similarity) / 2, dim=-1) \n",
    "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "def cross_entropy(preds, targets, reduction='none'):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, lr_scheduler, step, device=torch.device(\"cuda\")):\n",
    "    # device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "    loss_meter = AvgMeter()\n",
    "    # print(f\"train_loader: {train_loader}\")\n",
    "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "    for batch in tqdm_object:\n",
    "        for key in batch.keys():\n",
    "            if key != \"text\":\n",
    "                # print(key)\n",
    "                batch[key] = batch[key].to(device)\n",
    "            \n",
    "        loss = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step == \"batch\":\n",
    "            lr_scheduler.step()\n",
    "        count = batch[\"data\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
    "    return loss_meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim =len(relevant_features) # EDA Features Dimension\n",
    "output_dim = 1\n",
    "device = torch.device(\"cuda\")\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fet = relevant_features\n",
    "t_accuracy = 0\n",
    "t_f1 = 0\n",
    "for i in range(1,41):\n",
    "    label_id = 2\n",
    "    model_path = \"<Path to the pretrained model>\"\n",
    "    X_test = eda_fet[fet]\n",
    "    test_y = eda_fet['arousal'].to_list()\n",
    "    label1 = \"Data of High and more arousal_category\"\n",
    "    label2 = \"Data of Low and less arousal_category\"\n",
    "    label1 = lemmatize(label1)\n",
    "    label1 = stop_words(label1)\n",
    "    label1 = lowercase(label1)\n",
    "    label1 = punctuations(label1)\n",
    "\n",
    "    label2 = lemmatize(label2)\n",
    "    label2 = stop_words(label2)\n",
    "    label2 = lowercase(label2)\n",
    "    label2 = punctuations(label2)\n",
    "    try :\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        best_model = CLIPModel(mlp_input_dim = input_dim, mlp_output_dim = output_dim, device = device).to(device)\n",
    "        best_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        pred = []\n",
    "        text_embeddings  = best_model.text_encoder.text_tokens([label1])\n",
    "        text_embeddings = best_model.text_projection(text_embeddings)\n",
    "        text_embeddings = torch.stack(text_embeddings).to(device)\n",
    "        encoded_label1 = text_embeddings.squeeze(1)\n",
    "        encoded_label1 = encoded_label1.to(device)\n",
    "\n",
    "        text_embeddings  = best_model.text_encoder.text_tokens([label2])\n",
    "        text_embeddings = best_model.text_projection(text_embeddings)\n",
    "        text_embeddings = torch.stack(text_embeddings).to(device)\n",
    "        encoded_label2 = text_embeddings.squeeze(1)\n",
    "        encoded_label2 = encoded_label2.to(device)\n",
    "        X_test = np.array(X_test)\n",
    "\n",
    "        for j in X_test:\n",
    "            if isinstance(j, np.ndarray):\n",
    "                features = best_model.eda_encoder.get_hidden_embedding(torch.from_numpy(j).float().to(device))\n",
    "                features.to(device)\n",
    "                a = torch.matmul(features, encoded_label1.T) \n",
    "                b = torch.matmul(features, encoded_label2.T)\n",
    "                # print(a)\n",
    "                # print(b)\n",
    "                value = 1 if a > b else 0\n",
    "                pred.append(value)\n",
    "            else:\n",
    "                print(f\"Skipping invalid entry: {j}\")\n",
    "            \n",
    "        print(f\"Prediction: {pred}\")\n",
    "        print(f\"True Values: {test_y}\")\n",
    "        print(classification_report(test_y, pred))\n",
    "\n",
    "        accuracy = accuracy_score(test_y, pred)\n",
    "        f1 = f1_score(test_y, pred)\n",
    "        print(f\"Label Arousal, {i}\")\n",
    "        print(f\"accuracy {accuracy}\")\n",
    "        print(f\"f1 {f1}\")\n",
    "        t_accuracy += accuracy\n",
    "        t_f1 += f1\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(t_accuracy, t_accuracy/37)\n",
    "\n",
    "print(t_f1, t_f1/37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fet = ['ku_eda','sk_eda','dynrange','slope','variance','entropy','insc','fd_mean',\n",
    "                     'max_scr','min_scr','nSCR','meanAmpSCR','meanRespSCR','sumAmpSCR','sumRespSCR']\n",
    "\n",
    "print(len(fet))\n",
    "\n",
    "for i in range(1,41):\n",
    "    label_id = 1\n",
    "    model_path = \"<Path to the pretrained model>\"\n",
    "    X_test = eda_fet[fet]\n",
    "    test_y = eda_fet['valence'].to_list()\n",
    "    label1 = \"Data of High and more positive emotion\"\n",
    "    label2 = \"Data of Low and less negative emotion\"\n",
    "    label1 = lemmatize(label1)\n",
    "    label1 = stop_words(label1)\n",
    "    label1 = lowercase(label1)\n",
    "    label1 = punctuations(label1)\n",
    "\n",
    "    label2 = lemmatize(label2)\n",
    "    label2 = stop_words(label2)\n",
    "    label2 = lowercase(label2)\n",
    "    label2 = punctuations(label2)\n",
    "    try :\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        best_model = CLIPModel(mlp_input_dim = input_dim, mlp_output_dim = output_dim, device = device).to(device)\n",
    "        best_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        pred = []\n",
    "        text_embeddings  = best_model.text_encoder.text_tokens([label1])\n",
    "        text_embeddings = best_model.text_projection(text_embeddings)\n",
    "        text_embeddings = torch.stack(text_embeddings).to(device)\n",
    "        encoded_label1 = text_embeddings.squeeze(1)\n",
    "        encoded_label1 = encoded_label1.to(device)\n",
    "\n",
    "        text_embeddings  = best_model.text_encoder.text_tokens([label2])\n",
    "        text_embeddings = best_model.text_projection(text_embeddings)\n",
    "        text_embeddings = torch.stack(text_embeddings).to(device)\n",
    "        encoded_label2 = text_embeddings.squeeze(1)\n",
    "        encoded_label2 = encoded_label2.to(device)\n",
    "        X_test = np.array(X_test)\n",
    "\n",
    "        for j in X_test:\n",
    "            if isinstance(j, np.ndarray):\n",
    "                features = best_model.eda_encoder.get_hidden_embedding(torch.from_numpy(j).float().to(device))\n",
    "                features.to(device)\n",
    "                a = torch.matmul(features, encoded_label1.T) \n",
    "                b = torch.matmul(features, encoded_label2.T)\n",
    "                # print(a)\n",
    "                # print(b)\n",
    "                value = 1 if a > b else 0\n",
    "                pred.append(value)\n",
    "            else:\n",
    "                print(f\"Skipping invalid entry: {j}\")\n",
    "            \n",
    "        print(f\"Prediction: {pred}\")\n",
    "        print(f\"True Values: {test_y}\")\n",
    "        print(classification_report(test_y, pred))\n",
    "\n",
    "        accuracy = accuracy_score(test_y, pred)\n",
    "        f1 = f1_score(test_y, pred)\n",
    "        print(f\"Label Valence, {i}\")\n",
    "        print(f\"accuracy {accuracy}\")\n",
    "        print(f\"f1 {f1}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPG Time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppg_df\n",
    "df_list = {}\n",
    "for pi in ppg_df['PID'].unique():\n",
    "    df_pi = ppg_df[ppg_df['PID'] == pi]\n",
    "    for vi in ppg_df['label'].unique():\n",
    "        # print(pi,vi)\n",
    "        df_vi =  df_pi[df_pi['label'] == vi]\n",
    "        tag = str(pi) + '_' + str(vi)\n",
    "        df_list[tag] = (df_vi)\n",
    "\n",
    "\n",
    "df_list.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list['S10_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_acc = 64\n",
    "out_ppg = pd.DataFrame()\n",
    "\n",
    "for i in df_list:\n",
    "\n",
    "    row_dict = {}\n",
    "\n",
    "    pid = df_list[i]['PID'].tolist()[0]\n",
    "    vid = df_list[i]['label'].tolist()[0]\n",
    "    arousal = df_list[i]['arousal'].tolist()[0]\n",
    "    valence = df_list[i]['valence'].tolist()[0]\n",
    "\n",
    "    if vid == 0 or vid == 5 or vid == 6 or vid == 7:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        ppg_data = np.array(df_list[i]['PPG'].tolist())\n",
    "        ppg_data = ppg_data.flatten()\n",
    "        ibidata =[]\n",
    "        row_dict['Participant ID'] = pid\n",
    "        row_dict['arousal'] = arousal\n",
    "        row_dict['valence'] = valence\n",
    "        row_dict['label'] = vid\n",
    "\n",
    "        try:\n",
    "            bvp_clean = nk.ppg_clean(ppg_data, sampling_rate=sampling_acc)\n",
    "                \n",
    "            # Perform BVP analysis if the signal is long enough\n",
    "            if len(bvp_clean) > 10 * sampling_acc:  # Ensure at least 10 seconds of data\n",
    "                bvp_signals, bvp_info = nk.ppg_process(bvp_clean, sampling_rate=sampling_acc)\n",
    "                \n",
    "                # Perform the analysis\n",
    "                analyze_df = nk.ppg_analyze(bvp_signals, sampling_rate=sampling_acc)\n",
    "                \n",
    "                # Update your data with analysis results\n",
    "                row_dict.update(analyze_df.iloc[0])\n",
    "                heart_rate = bvp_signals['PPG_Rate']\n",
    "                bpm = heart_rate.mean()\n",
    "                print(bpm)\n",
    "                row_dict['BPM'] = bpm\n",
    "                if len(ibidata) == 0:\n",
    "                    row_dict['IBI'] = 0\n",
    "                else:\n",
    "                    row_dict['IBI'] = sum(ibidata)/len(ibidata)\n",
    "                new_row = pd.DataFrame(row_dict , index=[0])\n",
    "                out_ppg = pd.concat([out_ppg, new_row], ignore_index=True)\n",
    "\n",
    "            else:\n",
    "                print(f\"Signal too short for analysis at index\")\n",
    "                continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing signal at index: {e}\")\n",
    "            continue\n",
    "\n",
    "        print(row_dict.keys())\n",
    "\n",
    "out_ppg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_nan = out_ppg.columns[out_ppg.isna().any()].tolist()\n",
    "columns_with_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as mp\n",
    "import seaborn as sb\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchmetrics\n",
    "from sklearn.metrics import classification_report\n",
    "from torchmetrics import F1Score\n",
    "from tqdm.autonotebook import tqdm\n",
    "import itertools\n",
    "import random \n",
    "import os\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def csv_read(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "def lowercase(data):\n",
    "  temp  = data.lower()\n",
    "  return temp\n",
    "\n",
    "def stop_words(text):\n",
    "  stop_words_set = set(stopwords.words('english'))\n",
    "  word_tokens = word_tokenize(text)\n",
    "  filtered_sentence = [word for word in word_tokens if word.lower() not in stop_words_set]\n",
    "  return \" \".join(filtered_sentence)\n",
    "\n",
    "def punctuations(data):\n",
    "  no_punct=[words for words in data if words not in string.punctuation]\n",
    "  words_wo_punct=''.join(no_punct)\n",
    "  return words_wo_punct\n",
    "\n",
    "def lemmatize(text):\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  word_tokens = word_tokenize(text)\n",
    "  lemmatized_text = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "  return \" \".join(lemmatized_text)\n",
    "\n",
    "def preprcosess(text_data):\n",
    "    text_data['Text'] = text_data['Text Description'].apply(lambda x: lemmatize(x))\n",
    "    text_data['Text'] = text_data['Text'].apply(lambda x: stop_words(x))\n",
    "    text_data['Text'] = text_data['Text'].apply(lambda x: lowercase(x))\n",
    "    text_data['Text'] = text_data['Text'].apply(lambda x: punctuations(x))\n",
    "    return text_data\n",
    "\n",
    "def balanced_df(df):\n",
    "    filtered_rows = []\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        if row['arousal_category'] == 1:\n",
    "            filtered_rows.append(row)\n",
    "\n",
    "    filtered_df = pd.DataFrame(filtered_rows)\n",
    "    balance_a_df = pd.concat([df, filtered_df], ignore_index=True)\n",
    "    balance_a_df  = balance_a_df.reset_index(drop=True)\n",
    "    return balance_a_df\n",
    "\n",
    "class Text_Encoder(nn.Module):\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\", pretrained=True, trainable=True):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.model = DistilBertModel.from_pretrained(model_name).to(torch.device(\"cuda\"))\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "        \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "        \n",
    "\n",
    "\n",
    "    def text_tokens(self,batch):\n",
    "        text_embeddings = []\n",
    "        for i in range(len(batch)):\n",
    "            texts = batch[i]\n",
    "            tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "            model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(torch.device(\"cuda\"))\n",
    "                \n",
    "            # Tokenize and get embeddings\n",
    "            encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt').to(torch.device(\"cuda\"))\n",
    "            with torch.no_grad():\n",
    "                model_output = model(**encoded_input)\n",
    "\n",
    "            # Extract embeddings from the last hidden state\n",
    "            embeddings = model_output.last_hidden_state\n",
    "\n",
    "            # Get the embeddings for the [CLS] token (the first token)\n",
    "            cls_embeddings = embeddings[:, 0, :]\n",
    "\n",
    "            # Alternatively, mean pooling the token embeddings to get sentence-level embeddings\n",
    "            sentence_embeddings = torch.mean(embeddings, dim=1)\n",
    "\n",
    "            text_embeddings.append(sentence_embeddings)\n",
    "\n",
    "        return text_embeddings\n",
    "    \n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=100,  # Change projection_dim to 100\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        text_embeddings = []\n",
    "        for i in range(len(batch)):\n",
    "            x = batch[i]\n",
    "            projected = self.projection(x)\n",
    "            x = self.gelu(projected)\n",
    "            x = self.fc(x)\n",
    "            x = self.dropout(x)\n",
    "            x = x + projected\n",
    "            x = self.layer_norm(x)\n",
    "            text_embeddings.append(x)\n",
    "        return text_embeddings\n",
    "    \n",
    "class CustomMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(CustomMLP, self).__init__()\n",
    "\n",
    "        # Define hidden layer dimensions\n",
    "        hidden_dims = [50, 100]\n",
    "\n",
    "        # Create sequential layers using nn.Linear and nn.ReLU activations\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_dims[1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def get_hidden_embedding(self, x):\n",
    "        x = self.layer1(x)\n",
    "        return self.layer2(x)\n",
    "    \n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets, text):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.text  = text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = {}\n",
    "        item['data'] = torch.from_numpy(self.data[index]).float()\n",
    "        item['target'] = self.targets[index]\n",
    "        item['text'] = self.text[index]\n",
    "        return item\n",
    "    \n",
    "def windowed_preprocess(train_df, test_df):\n",
    "    train = []\n",
    "    test = []\n",
    "    for index, row in train_df.iterrows():\n",
    "        # Convert each row (Series) into a list of NumPy arrays\n",
    "        row_as_list1 = np.array([np.array(value) for value in row.to_numpy()])\n",
    "        train.append(row_as_list1)\n",
    "\n",
    "    for index, row in test_df.iterrows():\n",
    "        # Convert each row (Series) into a list of NumPy arrays\n",
    "        row_as_list2 = np.array([np.array(value) for value in row.to_numpy()])\n",
    "        test.append(row_as_list2)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n",
    "\n",
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, mlp_input_dim, mlp_output_dim, device):\n",
    "        super().__init__()\n",
    "        self.text_encoder = Text_Encoder().to(device)\n",
    "        self.ppg_encoder = CustomMLP(mlp_input_dim, mlp_output_dim).to(device)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=768, projection_dim=100).to(device)\n",
    "        self.device = device\n",
    "        \n",
    "    def ppg_train(self,learning_rate, beta1, beta2 , epsilon , train_dataloader):\n",
    "        criterion= nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam( self.ppg_encoder.parameters(), lr=learning_rate, betas=(beta1, beta2), eps=epsilon)\n",
    "        optimizer.zero_grad()\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            data = batch['data']\n",
    "            target = batch['target']\n",
    "\n",
    "            data, target = data.to(self.device), target.to(self.device)  # Move data and target to GPU\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = self.ppg_encoder(data)\n",
    "\n",
    "            target = target.unsqueeze(1).float()\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        data_values = batch['data']\n",
    "        text_values = batch['text']\n",
    "\n",
    "        ppg_embeddings = self.ppg_encoder.get_hidden_embedding(data_values).to(self.device)\n",
    "        text_embeddings  = self.text_encoder.text_tokens(batch[\"text\"])#.to(self.device)\n",
    "        text_embeddings = self.text_projection(text_embeddings)#.to(self.device)\n",
    "        text_embeddings = torch.stack(text_embeddings).to(self.device)\n",
    "        ppg_tensor = ppg_embeddings\n",
    "\n",
    "        # Calculating the Loss\n",
    "        text_embeddings = text_embeddings.squeeze(1)\n",
    "        logits = torch.matmul(text_embeddings, ppg_tensor.T)\n",
    "        # print(\"Logits \",logits.shape)\n",
    "        ppg_similarity = torch.matmul(ppg_tensor, ppg_tensor.T)\n",
    "        input_size = text_embeddings.size(0) * text_embeddings.size(1)\n",
    "        # print(\"Text Embedding \",text_embeddings.shape)\n",
    "        texts_similarity = torch.matmul(text_embeddings, text_embeddings.T)\n",
    "        targets = F.softmax((ppg_similarity + texts_similarity) / 2, dim=-1) \n",
    "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "def cross_entropy(preds, targets, reduction='none'):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, lr_scheduler, step, device=torch.device(\"cuda\")):\n",
    "    # device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "    loss_meter = AvgMeter()\n",
    "    # print(f\"train_loader: {train_loader}\")\n",
    "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "    for batch in tqdm_object:\n",
    "        for key in batch.keys():\n",
    "            if key != \"text\":\n",
    "                # print(key)\n",
    "                batch[key] = batch[key].to(device)\n",
    "            \n",
    "        loss = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step == \"batch\":\n",
    "            lr_scheduler.step()\n",
    "        count = batch[\"data\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
    "    return loss_meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_features_ppg = ['BPM', 'IBI', 'PPG_Rate_Mean', 'HRV_MedianNN',\n",
    "       'HRV_Prc20NN', 'HRV_MinNN', 'HRV_HTI', 'HRV_TINN', 'HRV_LF', 'HRV_VHF',\n",
    "       'HRV_LFn', 'HRV_HFn', 'HRV_LnHF', 'HRV_SD1SD2', 'HRV_CVI', 'HRV_PSS',\n",
    "       'HRV_PAS', 'HRV_PI', 'HRV_C1d', 'HRV_C1a', 'HRV_DFA_alpha1',\n",
    "       'HRV_MFDFA_alpha1_Width', 'HRV_MFDFA_alpha1_Peak',\n",
    "       'HRV_MFDFA_alpha1_Mean', 'HRV_MFDFA_alpha1_Max',\n",
    "       'HRV_MFDFA_alpha1_Delta', 'HRV_MFDFA_alpha1_Asymmetry', 'HRV_ApEn',\n",
    "       'HRV_ShanEn', 'HRV_FuzzyEn', 'HRV_MSEn', 'HRV_CMSEn', 'HRV_RCMSEn',\n",
    "       'HRV_CD', 'HRV_HFD', 'HRV_KFD', 'HRV_LZC']\n",
    "len(relevant_features_ppg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = out_ppg['arousal_category'].tolist()\n",
    "l = len(a_list)\n",
    "o = sum(a_list)\n",
    "z = l - o\n",
    "print(o,z,l)\n",
    "\n",
    "a_list = out_ppg['valence_category'].tolist()\n",
    "l = len(a_list)\n",
    "o = sum(a_list)\n",
    "z = l - o\n",
    "print(o,z,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppg_fet = out_ppg[relevant_features_ppg + identifiers]\n",
    "\n",
    "ppg_fet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "# List of columns to be scaled\n",
    "columns_to_scale = ppg_fet.columns.difference(['Participant ID', 'arousal', 'valence', 'label'])\n",
    "\n",
    "# Function to scale participant-wise\n",
    "def scale_participant(df):\n",
    "    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "    return df\n",
    "\n",
    "# Apply the scaling function to each participant's data\n",
    "scaled_ppg_fet = ppg_fet.groupby('Participant ID').apply(scale_participant).reset_index(drop=True)\n",
    "\n",
    "scaled_ppg_fet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = scaled_ppg_fet['arousal'].tolist()\n",
    "l = len(a_list)\n",
    "o = sum(a_list)\n",
    "z = l - o\n",
    "print(o,z,l)\n",
    "\n",
    "a_list = scaled_ppg_fet['valence'].tolist()\n",
    "l = len(a_list)\n",
    "o = sum(a_list)\n",
    "z = l - o\n",
    "print(o,z,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fet = relevant_features_ppg\n",
    "input_dim = len(fet)\n",
    "for i in range(1,41):\n",
    "    label_id = 2\n",
    "    model_path = \"<Path to the pretrained model>\"\n",
    "    X_test = scaled_ppg_fet[fet]\n",
    "    test_y = scaled_ppg_fet['arousal'].to_list()\n",
    "    label1 = \"Data of High and more arousal_category\"\n",
    "    label2 = \"Data of Low and less arousal_category\"\n",
    "    label1 = lemmatize(label1)\n",
    "    label1 = stop_words(label1)\n",
    "    label1 = lowercase(label1)\n",
    "    label1 = punctuations(label1)\n",
    "\n",
    "    label2 = lemmatize(label2)\n",
    "    label2 = stop_words(label2)\n",
    "    label2 = lowercase(label2)\n",
    "    label2 = punctuations(label2)\n",
    "    print(len(fet))\n",
    "    try :\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        best_model = CLIPModel(mlp_input_dim = input_dim, mlp_output_dim = output_dim, device = device).to(device)\n",
    "        best_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        pred = []\n",
    "        text_embeddings  = best_model.text_encoder.text_tokens([label1])\n",
    "        text_embeddings = best_model.text_projection(text_embeddings)\n",
    "        text_embeddings = torch.stack(text_embeddings).to(device)\n",
    "        encoded_label1 = text_embeddings.squeeze(1)\n",
    "        encoded_label1 = encoded_label1.to(device)\n",
    "\n",
    "        text_embeddings  = best_model.text_encoder.text_tokens([label2])\n",
    "        text_embeddings = best_model.text_projection(text_embeddings)\n",
    "        text_embeddings = torch.stack(text_embeddings).to(device)\n",
    "        encoded_label2 = text_embeddings.squeeze(1)\n",
    "        encoded_label2 = encoded_label2.to(device)\n",
    "        X_test = np.array(X_test)\n",
    "        for j in X_test:\n",
    "            features = best_model.ppg_encoder.get_hidden_embedding(torch.from_numpy(j).float().to(device))\n",
    "            # print(features.shape)\n",
    "            # print(encoded_label1.shape)\n",
    "            # print(encoded_label2.shape)\n",
    "            a = torch.matmul(features, encoded_label1.T) \n",
    "            b = torch.matmul(features, encoded_label2.T)\n",
    "            # print(a)\n",
    "            # print(b)\n",
    "            value = 1 if a > b else 0\n",
    "            pred.append(value)\n",
    "        print(i,\"arousal\")\n",
    "        print(f\"Prediction: {pred}\")\n",
    "        print(f\"True Values: {test_y}\")\n",
    "        print(classification_report(test_y, pred))\n",
    "\n",
    "        accuracy = accuracy_score(test_y, pred)\n",
    "        f1 = f1_score(test_y, pred)\n",
    "\n",
    "        print(f\"Accuracy {accuracy}\")\n",
    "        print(f\"Fi score {f1}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fet = relevant_features_ppg\n",
    "for i in range(1,41):\n",
    "    label_id = 1\n",
    "    model_path = \"<Path to the pretrained model>\"\n",
    "    X_test = scaled_ppg_fet[fet]\n",
    "    test_y = scaled_ppg_fet['valence'].to_list()\n",
    "    label1 = \"Data of High and more valence_category\"\n",
    "    label2 = \"Data of Low and less valence_category\"\n",
    "    label1 = lemmatize(label1)\n",
    "    label1 = stop_words(label1)\n",
    "    label1 = lowercase(label1)\n",
    "    label1 = punctuations(label1)\n",
    "\n",
    "    label2 = lemmatize(label2)\n",
    "    label2 = stop_words(label2)\n",
    "    label2 = lowercase(label2)\n",
    "    label2 = punctuations(label2)\n",
    "    try :\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        best_model = CLIPModel(mlp_input_dim = input_dim, mlp_output_dim = output_dim, device = device).to(device)\n",
    "        best_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        pred = []\n",
    "        text_embeddings  = best_model.text_encoder.text_tokens([label1])\n",
    "        text_embeddings = best_model.text_projection(text_embeddings)\n",
    "        text_embeddings = torch.stack(text_embeddings).to(device)\n",
    "        encoded_label1 = text_embeddings.squeeze(1)\n",
    "        encoded_label1 = encoded_label1.to(device)\n",
    "\n",
    "        text_embeddings  = best_model.text_encoder.text_tokens([label2])\n",
    "        text_embeddings = best_model.text_projection(text_embeddings)\n",
    "        text_embeddings = torch.stack(text_embeddings).to(device)\n",
    "        encoded_label2 = text_embeddings.squeeze(1)\n",
    "        encoded_label2 = encoded_label2.to(device)\n",
    "        X_test = np.array(X_test)\n",
    "        for j in X_test:\n",
    "            features = best_model.ppg_encoder.get_hidden_embedding(torch.from_numpy(j).float().to(device))\n",
    "            # print(features.shape)\n",
    "            # print(encoded_label1.shape)\n",
    "            # print(encoded_label2.shape)\n",
    "            a = torch.matmul(features, encoded_label1.T) \n",
    "            b = torch.matmul(features, encoded_label2.T)\n",
    "            # print(a)\n",
    "            # print(b)\n",
    "            value = 1 if a > b else 0\n",
    "            pred.append(value)\n",
    "\n",
    "        print(i, \"valence\")\n",
    "        print(f\"Prediction: {pred}\")\n",
    "        print(f\"True Values: {test_y}\")\n",
    "        print(classification_report(test_y, pred))\n",
    "\n",
    "        accuracy = accuracy_score(test_y, pred)\n",
    "        f1 = f1_score(test_y, pred)\n",
    "\n",
    "        print(f\"Accuracy {accuracy}\")\n",
    "        print(f\"Fi score {f1}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eda + ppg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_ppg_fet.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_combined =  pd.concat([scaled_eda_fet, scaled_ppg_fet], axis=1)\n",
    "scaled_combined = scaled_combined.loc[:, ~scaled_combined.columns.duplicated()]\n",
    "scaled_combined = scaled_combined.reset_index(drop=True)\n",
    "\n",
    "\n",
    "relevant_features_combined = ['BPM', 'IBI', 'PPG_Rate_Mean', 'HRV_MedianNN', 'HRV_Prc20NN',\n",
    "    'HRV_MinNN', 'HRV_HTI', 'HRV_TINN', 'HRV_LF', 'HRV_VHF', 'HRV_LFn',\n",
    "    'HRV_HFn', 'HRV_LnHF', 'HRV_SD1SD2', 'HRV_CVI', 'HRV_PSS', 'HRV_PAS',\n",
    "    'HRV_PI', 'HRV_C1d', 'HRV_C1a', 'HRV_DFA_alpha1',\n",
    "    'HRV_MFDFA_alpha1_Width', 'HRV_MFDFA_alpha1_Peak',\n",
    "    'HRV_MFDFA_alpha1_Mean', 'HRV_MFDFA_alpha1_Max',\n",
    "    'HRV_MFDFA_alpha1_Delta', 'HRV_MFDFA_alpha1_Asymmetry', 'HRV_ApEn',\n",
    "    'HRV_ShanEn', 'HRV_FuzzyEn', 'HRV_MSEn', 'HRV_CMSEn', 'HRV_RCMSEn',\n",
    "    'HRV_CD', 'HRV_HFD', 'HRV_KFD', 'HRV_LZC', 'ku_eda', 'sk_eda', 'dynrange', 'slope',\n",
    "    'variance', 'entropy', 'insc', 'fd_mean', 'max_scr', 'min_scr', 'nSCR',\n",
    "    'meanAmpSCR', 'meanRespSCR', 'sumAmpSCR', 'sumRespSCR']\n",
    "\n",
    "scaled_combined.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as mp\n",
    "import seaborn as sb\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchmetrics\n",
    "from sklearn.metrics import classification_report\n",
    "from torchmetrics import F1Score\n",
    "from tqdm.autonotebook import tqdm\n",
    "import itertools\n",
    "import random \n",
    "import os\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def csv_read(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "def lowercase(data):\n",
    "  temp  = data.lower()\n",
    "  return temp\n",
    "\n",
    "def stop_words(text):\n",
    "  stop_words_set = set(stopwords.words('english'))\n",
    "  word_tokens = word_tokenize(text)\n",
    "  filtered_sentence = [word for word in word_tokens if word.lower() not in stop_words_set]\n",
    "  return \" \".join(filtered_sentence)\n",
    "\n",
    "def punctuations(data):\n",
    "  no_punct=[words for words in data if words not in string.punctuation]\n",
    "  words_wo_punct=''.join(no_punct)\n",
    "  return words_wo_punct\n",
    "\n",
    "def lemmatize(text):\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  word_tokens = word_tokenize(text)\n",
    "  lemmatized_text = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "  return \" \".join(lemmatized_text)\n",
    "\n",
    "def preprcosess(text_data):\n",
    "    text_data['Text'] = text_data['Text Description'].apply(lambda x: lemmatize(x))\n",
    "    text_data['Text'] = text_data['Text'].apply(lambda x: stop_words(x))\n",
    "    text_data['Text'] = text_data['Text'].apply(lambda x: lowercase(x))\n",
    "    text_data['Text'] = text_data['Text'].apply(lambda x: punctuations(x))\n",
    "    return text_data\n",
    "\n",
    "def balanced_df(df):\n",
    "    filtered_rows = []\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        if row['arousal_category'] == 1:\n",
    "            filtered_rows.append(row)\n",
    "\n",
    "    filtered_df = pd.DataFrame(filtered_rows)\n",
    "    balance_a_df = pd.concat([df, filtered_df], ignore_index=True)\n",
    "    balance_a_df  = balance_a_df.reset_index(drop=True)\n",
    "    return balance_a_df\n",
    "\n",
    "class Text_Encoder(nn.Module):\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\", pretrained=True, trainable=True):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.model = DistilBertModel.from_pretrained(model_name).to(torch.device(\"cuda\"))\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "        \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "        \n",
    "\n",
    "\n",
    "    def text_tokens(self,batch):\n",
    "        text_embeddings = []\n",
    "        for i in range(len(batch)):\n",
    "            texts = batch[i]\n",
    "            tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "            model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(torch.device(\"cuda\"))\n",
    "                \n",
    "            # Tokenize and get embeddings\n",
    "            encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt').to(torch.device(\"cuda\"))\n",
    "            with torch.no_grad():\n",
    "                model_output = model(**encoded_input)\n",
    "\n",
    "            # Extract embeddings from the last hidden state\n",
    "            embeddings = model_output.last_hidden_state\n",
    "\n",
    "            # Get the embeddings for the [CLS] token (the first token)\n",
    "            cls_embeddings = embeddings[:, 0, :]\n",
    "\n",
    "            # Alternatively, mean pooling the token embeddings to get sentence-level embeddings\n",
    "            sentence_embeddings = torch.mean(embeddings, dim=1)\n",
    "\n",
    "            text_embeddings.append(sentence_embeddings)\n",
    "\n",
    "        return text_embeddings\n",
    "    \n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=100,  # Change projection_dim to 100\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        text_embeddings = []\n",
    "        for i in range(len(batch)):\n",
    "            x = batch[i]\n",
    "            projected = self.projection(x)\n",
    "            x = self.gelu(projected)\n",
    "            x = self.fc(x)\n",
    "            x = self.dropout(x)\n",
    "            x = x + projected\n",
    "            x = self.layer_norm(x)\n",
    "            text_embeddings.append(x)\n",
    "        return text_embeddings\n",
    "    \n",
    "class CustomMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(CustomMLP, self).__init__()\n",
    "\n",
    "        # Define hidden layer dimensions\n",
    "        hidden_dims = [50, 100]\n",
    "\n",
    "        # Create sequential layers using nn.Linear and nn.ReLU activations\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_dims[1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def get_hidden_embedding(self, x):\n",
    "        x = self.layer1(x)\n",
    "        return self.layer2(x)\n",
    "    \n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets, text):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.text  = text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = {}\n",
    "        item['data'] = torch.from_numpy(self.data[index]).float()\n",
    "        item['target'] = self.targets[index]\n",
    "        item['text'] = self.text[index]\n",
    "        return item\n",
    "    \n",
    "def windowed_preprocess(train_df, test_df):\n",
    "    train = []\n",
    "    test = []\n",
    "    for index, row in train_df.iterrows():\n",
    "        # Convert each row (Series) into a list of NumPy arrays\n",
    "        row_as_list1 = np.array([np.array(value) for value in row.to_numpy()])\n",
    "        train.append(row_as_list1)\n",
    "\n",
    "    for index, row in test_df.iterrows():\n",
    "        # Convert each row (Series) into a list of NumPy arrays\n",
    "        row_as_list2 = np.array([np.array(value) for value in row.to_numpy()])\n",
    "        test.append(row_as_list2)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n",
    "\n",
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, mlp_input_dim, mlp_output_dim, device):\n",
    "        super().__init__()\n",
    "        self.text_encoder = Text_Encoder().to(device)\n",
    "        self.combined_encoder = CustomMLP(mlp_input_dim, mlp_output_dim).to(device)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=768, projection_dim=100).to(device)\n",
    "        self.device = device\n",
    "        \n",
    "    def combined_train(self,learning_rate, beta1, beta2 , epsilon , train_dataloader):\n",
    "        criterion= nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam( self.combined_encoder.parameters(), lr=learning_rate, betas=(beta1, beta2), eps=epsilon)\n",
    "        optimizer.zero_grad()\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            data = batch['data']\n",
    "            target = batch['target']\n",
    "\n",
    "            data, target = data.to(self.device), target.to(self.device)  # Move data and target to GPU\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = self.combined_encoder(data)\n",
    "\n",
    "            target = target.unsqueeze(1).float()\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        data_values = batch['data']\n",
    "        text_values = batch['text']\n",
    "\n",
    "        combined_embeddings = self.combined_encoder.get_hidden_embedding(data_values).to(self.device)\n",
    "        text_embeddings  = self.text_encoder.text_tokens(batch[\"text\"])#.to(self.device)\n",
    "        text_embeddings = self.text_projection(text_embeddings)#.to(self.device)\n",
    "        text_embeddings = torch.stack(text_embeddings).to(self.device)\n",
    "        combined_tensor = combined_embeddings\n",
    "\n",
    "        # Calculating the Loss\n",
    "        text_embeddings = text_embeddings.squeeze(1)\n",
    "        logits = torch.matmul(text_embeddings, combined_tensor.T)\n",
    "        # print(\"Logits \",logits.shape)\n",
    "        combined_similarity = torch.matmul(combined_tensor, combined_tensor.T)\n",
    "        input_size = text_embeddings.size(0) * text_embeddings.size(1)\n",
    "        # print(\"Text Embedding \",text_embeddings.shape)\n",
    "        texts_similarity = torch.matmul(text_embeddings, text_embeddings.T)\n",
    "        targets = F.softmax((combined_similarity + texts_similarity) / 2, dim=-1) \n",
    "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "def cross_entropy(preds, targets, reduction='none'):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, lr_scheduler, step, device=torch.device(\"cuda\")):\n",
    "    # device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "    loss_meter = AvgMeter()\n",
    "    # print(f\"train_loader: {train_loader}\")\n",
    "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "    for batch in tqdm_object:\n",
    "        for key in batch.keys():\n",
    "            if key != \"text\":\n",
    "                # print(key)\n",
    "                batch[key] = batch[key].to(device)\n",
    "            \n",
    "        loss = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step == \"batch\":\n",
    "            lr_scheduler.step()\n",
    "        count = batch[\"data\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
    "    return loss_meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_combined['meanAmpSCR'] = 0\n",
    "scaled_combined['meanRespSCR'] = 0\n",
    "scaled_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_combined.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(relevant_features_combined)\n",
    "fet = relevant_features_combined\n",
    "for i in range(1,41):\n",
    "    label_id = 2\n",
    "    model_path = \"<Path to the pretrained model>\"\n",
    "    X_test = scaled_combined[fet]\n",
    "    test_y = scaled_combined['arousal'].to_list()\n",
    "    label1 = \"Data of High and more arousal_category\"\n",
    "    label2 = \"Data of Low and less arousal_category\"\n",
    "    label1 = lemmatize(label1)\n",
    "    label1 = stop_words(label1)\n",
    "    label1 = lowercase(label1)\n",
    "    label1 = punctuations(label1)\n",
    "\n",
    "    label2 = lemmatize(label2)\n",
    "    label2 = stop_words(label2)\n",
    "    label2 = lowercase(label2)\n",
    "    label2 = punctuations(label2)\n",
    "    try :\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        best_model = CLIPModel(mlp_input_dim = input_dim, mlp_output_dim = output_dim, device = device).to(device)\n",
    "        best_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        pred = []\n",
    "        text_embeddings  = best_model.text_encoder.text_tokens([label1])\n",
    "        text_embeddings = best_model.text_projection(text_embeddings)\n",
    "        text_embeddings = torch.stack(text_embeddings).to(device)\n",
    "        encoded_label1 = text_embeddings.squeeze(1)\n",
    "        encoded_label1 = encoded_label1.to(device)\n",
    "\n",
    "        text_embeddings  = best_model.text_encoder.text_tokens([label2])\n",
    "        text_embeddings = best_model.text_projection(text_embeddings)\n",
    "        text_embeddings = torch.stack(text_embeddings).to(device)\n",
    "        encoded_label2 = text_embeddings.squeeze(1)\n",
    "        encoded_label2 = encoded_label2.to(device)\n",
    "        X_test = np.array(X_test)\n",
    "        for j in X_test:\n",
    "            features = best_model.combined_encoder.get_hidden_embedding(torch.from_numpy(j).float().to(device))\n",
    "            # print(features.shape)\n",
    "            # print(encoded_label1.shape)\n",
    "            # print(encoded_label2.shape)\n",
    "            a = torch.matmul(features, encoded_label1.T) \n",
    "            b = torch.matmul(features, encoded_label2.T)\n",
    "            # print(a)\n",
    "            # print(b)\n",
    "            value = 1 if a > b else 0\n",
    "            pred.append(value)\n",
    "\n",
    "        print(i, \"arousal\")\n",
    "        print(f\"Prediction: {pred}\")\n",
    "        print(f\"True Values: {test_y}\")\n",
    "        print(classification_report(test_y, pred))\n",
    "\n",
    "        accuracy = accuracy_score(test_y, pred)\n",
    "        f1 = f1_score(test_y, pred)\n",
    "\n",
    "        print(f\"Accuracy {accuracy}\")\n",
    "        print(f\"Fi score {f1}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(i, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(relevant_features_combined)\n",
    "fet = relevant_features_combined\n",
    "for i in range(1,41):\n",
    "    label_id = 1\n",
    "    model_path = \"<Path to the pretrained model>\"\n",
    "    X_test = scaled_combined[fet]\n",
    "    test_y = scaled_combined['valence'].to_list()\n",
    "    label1 = \"Data of High and more valence_category\"\n",
    "    label2 = \"Data of Low and less valence_category\"\n",
    "    label1 = lemmatize(label1)\n",
    "    label1 = stop_words(label1)\n",
    "    label1 = lowercase(label1)\n",
    "    label1 = punctuations(label1)\n",
    "\n",
    "    label2 = lemmatize(label2)\n",
    "    label2 = stop_words(label2)\n",
    "    label2 = lowercase(label2)\n",
    "    label2 = punctuations(label2)\n",
    "    try :\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        best_model = CLIPModel(mlp_input_dim = input_dim, mlp_output_dim = output_dim, device = device).to(device)\n",
    "        best_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        pred = []\n",
    "        text_embeddings  = best_model.text_encoder.text_tokens([label1])\n",
    "        text_embeddings = best_model.text_projection(text_embeddings)\n",
    "        text_embeddings = torch.stack(text_embeddings).to(device)\n",
    "        encoded_label1 = text_embeddings.squeeze(1)\n",
    "        encoded_label1 = encoded_label1.to(device)\n",
    "\n",
    "        text_embeddings  = best_model.text_encoder.text_tokens([label2])\n",
    "        text_embeddings = best_model.text_projection(text_embeddings)\n",
    "        text_embeddings = torch.stack(text_embeddings).to(device)\n",
    "        encoded_label2 = text_embeddings.squeeze(1)\n",
    "        encoded_label2 = encoded_label2.to(device)\n",
    "        X_test = np.array(X_test)\n",
    "        for j in X_test:\n",
    "            features = best_model.combined_encoder.get_hidden_embedding(torch.from_numpy(j).float().to(device))\n",
    "            # print(features.shape)\n",
    "            # print(encoded_label1.shape)\n",
    "            # print(encoded_label2.shape)\n",
    "            a = torch.matmul(features, encoded_label1.T) \n",
    "            b = torch.matmul(features, encoded_label2.T)\n",
    "            # print(a)\n",
    "            # print(b)\n",
    "            value = 1 if a > b else 0\n",
    "            pred.append(value)\n",
    "\n",
    "        print(i, \"valence\")\n",
    "        print(f\"Prediction: {pred}\")\n",
    "        print(f\"True Values: {test_y}\")\n",
    "        print(classification_report(test_y, pred))\n",
    "\n",
    "        accuracy = accuracy_score(test_y, pred)\n",
    "        f1 = f1_score(test_y, pred)\n",
    "\n",
    "        print(f\"Accuracy {accuracy}\")\n",
    "        print(f\"Fi score {f1}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(i, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, mean_squared_error, r2_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "from torchmetrics import F1Score\n",
    "from positional_encodings.torch_encodings import PositionalEncoding1D, PositionalEncoding2D, PositionalEncoding3D, Summer\n",
    "from tqdm import tqdm\n",
    "import neurokit2 as nk\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed = 111 # (42, 43, 111)    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def train_mlp(X_train, y_train_arousal, y_train_valence, X_test, y_test_arousal, y_test_valence, hidden_layer_sizes=(100,), random_seed=42):\n",
    "    # Create and fit the MLP model for Arousal\n",
    "    model_arousal_mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, random_state=random_seed)\n",
    "    model_arousal_mlp.fit(X_train, y_train_arousal)  # Fit the model\n",
    "    y_pred_arousal_mlp = model_arousal_mlp.predict(X_test)\n",
    "\n",
    "    # Create and fit the MLP model for Valence\n",
    "    model_valence_mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, random_state=random_seed)\n",
    "    model_valence_mlp.fit(X_train, y_train_valence)  # Fit the model\n",
    "    y_pred_valence_mlp = model_valence_mlp.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy for Arousal, Valence, and Task classification\n",
    "    accuracy_arousal_mlp = accuracy_score(y_test_arousal, y_pred_arousal_mlp)\n",
    "    balanced_acc_arousal_mlp = balanced_accuracy_score(y_test_arousal, y_pred_arousal_mlp)\n",
    "    accuracy_valence_mlp = accuracy_score(y_test_valence, y_pred_valence_mlp)\n",
    "    balanced_acc_valence_mlp = balanced_accuracy_score(y_test_valence, y_pred_valence_mlp)\n",
    "\n",
    "    f1_arousal = f1_score(y_test_arousal, y_pred_arousal_mlp)\n",
    "    f1_valence = f1_score(y_test_valence, y_pred_valence_mlp)\n",
    "\n",
    "    r2_arousal = r2_score(y_test_arousal, y_pred_arousal_mlp)\n",
    "    r2_valence = r2_score(y_test_valence, y_pred_valence_mlp)\n",
    "\n",
    "    mse_arousal = mean_squared_error(y_test_arousal, y_pred_arousal_mlp)\n",
    "    mse_valence = mean_squared_error(y_test_valence, y_pred_valence_mlp)\n",
    "\n",
    "    return (accuracy_arousal_mlp, balanced_acc_arousal_mlp, accuracy_valence_mlp, balanced_acc_valence_mlp, \n",
    "            r2_arousal, r2_valence, mse_arousal, mse_valence, f1_arousal, f1_valence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(df, pi, relevant_features):\n",
    "\n",
    "    accuracy_scores_a_mlp = {}\n",
    "    balance_acc_list_a_mlp = {}\n",
    "    accuracy_scores_v_mlp = {}\n",
    "    balance_acc_list_v_mlp = {}\n",
    "    r2_a_mlp = {}\n",
    "    r2_v_mlp ={}\n",
    "    mse_a_mlp = {}\n",
    "    mse_v_mlp = {}\n",
    "    f1_a_mlp = {}\n",
    "    f1_v_mlp = {}\n",
    "    accuracy_scores_s_mlp = {}\n",
    "    balance_acc_list_s_mlp = {}\n",
    "    r2_s_mlp = {}\n",
    "    mse_s_mlp = {}\n",
    "    f1_s_mlp = {}\n",
    "    \n",
    "\n",
    "    pi = list(set(pi))\n",
    "    for i in pi:\n",
    "\n",
    "        train_data = df[df['Participant ID'] != i]\n",
    "        test_data = df[df['Participant ID'] == i]\n",
    "\n",
    "        X_train = train_data[relevant_features]\n",
    "        y_train_arousal = train_data['arousal_category'].astype(int)\n",
    "        y_train_valence = train_data['valence_category'].astype(int)\n",
    "\n",
    "        X_test = test_data[relevant_features]\n",
    "        y_test_arousal = test_data['arousal_category'].astype(int)\n",
    "        y_test_valence = test_data['valence_category'].astype(int)\n",
    "\n",
    "        # Train the MLP classifier with specified hidden layers\n",
    "        accuracy_arousal, balanced_acc_arousal, accuracy_valence, balanced_acc_valence, r2_a, r2_v, mse_a, mse_v, f1_a, f1_v, accuracy_task, balanced_acc_task, r2_task, mse_task, f1_task, num_zeros, num_ones = train_mlp(X_train, y_train_arousal, y_train_valence, X_test, y_test_arousal , y_test_valence)\n",
    "\n",
    "        accuracy_scores_a_mlp[i] = accuracy_arousal*100\n",
    "        balance_acc_list_a_mlp[i] = balanced_acc_arousal*100\n",
    "        accuracy_scores_v_mlp[i] = accuracy_valence*100\n",
    "        balance_acc_list_v_mlp[i] = balanced_acc_valence*100\n",
    "        r2_a_mlp[i] = r2_a\n",
    "        r2_v_mlp[i] = r2_v\n",
    "        mse_a_mlp[i] = mse_a\n",
    "        mse_v_mlp[i] = mse_v\n",
    "        f1_a_mlp[i] = f1_a\n",
    "        f1_v_mlp[i] = f1_v\n",
    "        accuracy_scores_s_mlp[i] = accuracy_task*100\n",
    "        balance_acc_list_s_mlp[i] = balanced_acc_task*100\n",
    "        r2_s_mlp[i] = r2_task\n",
    "        mse_s_mlp[i] = mse_task\n",
    "        f1_s_mlp[i] = f1_task\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"-----Average Accuracy Arousal-----\")\n",
    "    print(f\"MLP: {sum(accuracy_scores_a_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average Balanced Accuracy Arousal-----\")\n",
    "    print(f\"MLP: {sum(balance_acc_list_a_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average R2 Loss Arousal-----\")\n",
    "    print(f\"MLP: {sum(r2_a_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average MSE Loss Arousal-----\")\n",
    "    print(f\"MLP: {sum(mse_a_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average F1 Arousal-----\")\n",
    "    print(f\"MLP: {sum(f1_a_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average Accuracy Valence-----\")\n",
    "    print(f\"MLP: {sum(accuracy_scores_v_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average Balanced Accuracy Valence-----\")\n",
    "    print(f\"MLP: {sum(balance_acc_list_v_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average R2 Loss Valence-----\")\n",
    "    print(f\"MLP: {sum(r2_v_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average MSE Loss Valence-----\")\n",
    "    print(f\"MLP: {sum(mse_v_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average F1 Valence-----\")\n",
    "    print(f\"MLP: {sum(f1_v_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average Accuracy Stimulus-label-----\")\n",
    "    print(f\"MLP: {sum(accuracy_scores_s_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average Balanced Accuracy Stimulus-label-----\")\n",
    "    print(f\"MLP: {sum(balance_acc_list_s_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average R2 Loss Stimulus-label-----\")\n",
    "    print(f\"MLP: {sum(r2_s_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average MSE Loss Stimulus-label-----\")\n",
    "    print(f\"MLP: {sum(mse_s_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average F1 Stimulus-label-----\")\n",
    "    print(f\"MLP: {sum(f1_s_mlp.values()) / len(pi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_features = ['ku_eda','sk_eda','dynrange','slope','variance','entropy','insc','fd_mean','max_scr','min_scr','nSCR','sumAmpSCR','sumRespSCR']\n",
    "scaled_eda_fet = scaled_eda_fet[relevant_features + identifiers]\n",
    "pi = scaled_eda_fet['Participant ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(scaled_eda_fet, pi, relevant_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_features = ['BPM','IBI','PPG_Rate_Mean','HRV_MedianNN','HRV_Prc20NN','HRV_MinNN','HRV_HTI','HRV_TINN','HRV_VHF','HRV_HFn','HRV_LnHF','HRV_SD1SD2','HRV_CVI','HRV_PSS','HRV_PAS','HRV_PI','HRV_C1d','HRV_C1a','HRV_DFA_alpha1','HRV_MFDFA_alpha1_Width','HRV_MFDFA_alpha1_Peak','HRV_MFDFA_alpha1_Mean','HRV_MFDFA_alpha1_Max','HRV_MFDFA_alpha1_Delta','HRV_MFDFA_alpha1_Asymmetry','HRV_ApEn','HRV_ShanEn','HRV_FuzzyEn','HRV_CD','HRV_HFD','HRV_KFD','HRV_LZC']\n",
    "columns_with_nan = scaled_ppg_fet.columns[scaled_ppg_fet.isna().any()].tolist()\n",
    "columns_with_nan\n",
    "validation(scaled_ppg_fet, pi, relevant_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_features = ['ku_eda','sk_eda','dynrange','slope','variance','entropy','insc','fd_mean','max_scr','min_scr','nSCR','sumAmpSCR','sumRespSCR','BPM','IBI','PPG_Rate_Mean','HRV_MedianNN','HRV_Prc20NN','HRV_MinNN','HRV_HTI','HRV_TINN','HRV_VHF','HRV_HFn','HRV_LnHF','HRV_SD1SD2','HRV_CVI','HRV_PSS','HRV_PAS','HRV_PI','HRV_C1d','HRV_C1a','HRV_DFA_alpha1','HRV_MFDFA_alpha1_Width','HRV_MFDFA_alpha1_Peak','HRV_MFDFA_alpha1_Mean','HRV_MFDFA_alpha1_Max','HRV_MFDFA_alpha1_Delta','HRV_MFDFA_alpha1_Asymmetry','HRV_ApEn','HRV_ShanEn','HRV_FuzzyEn','HRV_CD','HRV_HFD','HRV_KFD','HRV_LZC']\n",
    "# columns_with_nan = stimulus_combined.columns[stimulus_combined.isna().any()].tolist()\n",
    "# columns_with_nan\n",
    "validation(scaled_combined, pi, relevant_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
