{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, mean_squared_error, r2_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "from torchmetrics import F1Score\n",
    "from positional_encodings.torch_encodings import PositionalEncoding1D, PositionalEncoding2D, PositionalEncoding3D, Summer\n",
    "from tqdm import tqdm\n",
    "import neurokit2 as nk\n",
    "import pyEDA.main as pyEDA\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed = 111 # (42, 43, 111)    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_read(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_analysis(df,relevant_features,identifiers):\n",
    "    df_features = df[relevant_features]\n",
    "    \n",
    "\n",
    "    corr_matrix = df_features.corr()\n",
    "    plt.figure(figsize=(100, 100))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.show()\n",
    "\n",
    "    df_features.hist(bins=30, figsize=(20, 15))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    arousals = df['arousal_category'].tolist()\n",
    "    num_high_arousal = arousals.count(1)\n",
    "    num_low_arousal = arousals.count(0)\n",
    "\n",
    "    print(f\"The Number of entries of High Arousal: {num_high_arousal} & Low Arousal: {num_low_arousal}\")\n",
    "\n",
    "    valence = df['valence_category'].tolist()\n",
    "    num_high_valence = valence.count(1)\n",
    "    num_low_valence = valence.count(0)\n",
    "\n",
    "    print(f\"The Number of entries of High Valence: {num_high_valence} & Low Valence: {num_low_valence}\")\n",
    "\n",
    "    taskwiselabel_list = df['taskwiselabel'].tolist()\n",
    "    num_zeros = taskwiselabel_list.count(0)\n",
    "    num_ones = taskwiselabel_list.count(1)\n",
    "\n",
    "    print(f\"The Number of entries of Positive Task: {num_ones} & Negative Task: {num_zeros}\")\n",
    "\n",
    "    pi = df['Participant ID'].tolist()\n",
    "    vi = df['Video ID'].tolist()\n",
    "\n",
    "    df_required = df[relevant_features + identifiers]\n",
    "\n",
    "    return df_required, pi, vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X_train, y_train_arousal, y_train_valence, y_train_stress, X_test, y_test_arousal , y_test_valence, y_test_stress):\n",
    "    num_zeros = 0\n",
    "    num_ones = 1\n",
    "    \n",
    "    # Create and fit the logistic regression model for Arousal\n",
    "    model_arousal = LogisticRegression(random_state=random_seed)\n",
    "    model_arousal.fit(X_train, y_train_arousal)\n",
    "\n",
    "    # Create and fit the logistic regression model for Valence\n",
    "    model_valence = LogisticRegression(random_state=random_seed)\n",
    "    model_valence.fit(X_train, y_train_valence)\n",
    "\n",
    "    #Create and fit the logistic regression model for Task \n",
    "    model_task = LogisticRegression(random_state=random_seed)\n",
    "    model_task.fit(X_train,y_train_stress)\n",
    "\n",
    "    # Predict on the test set for Arousal, Valence and task\n",
    "    y_pred_arousal = model_arousal.predict(X_test)\n",
    "    y_pred_valence = model_valence.predict(X_test)\n",
    "    y_task = model_task.predict(X_test)\n",
    "\n",
    "    num_zeros = list(y_pred_arousal).count(0)\n",
    "    num_ones = list(y_pred_arousal).count(1)\n",
    "\n",
    "    accuracy_arousal = accuracy_score(y_test_arousal, y_pred_arousal)\n",
    "    balanced_acc_arousal = balanced_accuracy_score(y_test_arousal, y_pred_arousal)\n",
    "    accuracy_valence = accuracy_score(y_test_valence, y_pred_valence)\n",
    "    balanced_acc_valence = balanced_accuracy_score(y_test_valence, y_pred_valence)\n",
    "    accuracy_task = accuracy_score(y_task, y_test_stress)\n",
    "    balanced_acc_task = balanced_accuracy_score(y_task, y_test_stress)\n",
    "\n",
    "    f1_a = f1_score(y_test_arousal, y_pred_arousal)\n",
    "    f1_v = f1_score(y_test_valence, y_pred_valence)\n",
    "    f1_task = f1_score(y_task, y_test_stress)\n",
    "\n",
    "    r2_a = r2_score(y_test_arousal, y_pred_arousal)\n",
    "    r2_v = r2_score(y_test_valence, y_pred_valence)\n",
    "    r2_task = r2_score(y_task, y_test_stress)\n",
    "\n",
    "    mse_a = mean_squared_error(y_test_arousal, y_pred_arousal)\n",
    "    mse_v = mean_squared_error(y_test_valence, y_pred_valence)\n",
    "    mse_task = mean_squared_error(y_task, y_test_stress)\n",
    "\n",
    "    return accuracy_arousal, balanced_acc_arousal, accuracy_valence, balanced_acc_valence, r2_a, r2_v, mse_a, mse_v, f1_a, f1_v, accuracy_task, balanced_acc_task, r2_task, mse_task, f1_task , num_zeros, num_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree(X_train, y_train_arousal, y_train_valence, y_train_stress, X_test, y_test_arousal , y_test_valence, y_test_stress):\n",
    "    \n",
    "     # Create and fit the Decision Tree model for Arousal\n",
    "    model_arousal = DecisionTreeClassifier(criterion='entropy', min_samples_split=20, random_state=random_seed)\n",
    "    model_arousal.fit(X_train, y_train_arousal)\n",
    "\n",
    "    # Create and fit the Decision Tree model for Valence\n",
    "    model_valence = DecisionTreeClassifier(criterion='entropy', min_samples_split=20, random_state=random_seed)\n",
    "    model_valence.fit(X_train, y_train_valence)\n",
    "    \n",
    "    # Predict on the test set for Arousal and Valence\n",
    "    y_pred_arousal = model_arousal.predict(X_test)\n",
    "    y_pred_valence = model_valence.predict(X_test)\n",
    "\n",
    "    num_zeros = list(y_pred_arousal).count(0)\n",
    "    num_ones = list(y_pred_arousal).count(1)\n",
    "\n",
    "    #Create and fit the DT for Task \n",
    "    model_task = DecisionTreeClassifier(criterion='entropy', min_samples_split=20, random_state=random_seed)\n",
    "    model_task.fit(X_train,y_train_stress)\n",
    "    y_task = model_task.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy for Arousal and Valence classification\n",
    "    accuracy_arousal = accuracy_score(y_test_arousal, y_pred_arousal)\n",
    "    balanced_acc_arousal = balanced_accuracy_score(y_test_arousal, y_pred_arousal)\n",
    "    accuracy_valence = accuracy_score(y_test_valence, y_pred_valence)\n",
    "    balanced_acc_valence = balanced_accuracy_score(y_test_valence, y_pred_valence)\n",
    "    accuracy_task = accuracy_score(y_task, y_test_stress)\n",
    "    balanced_acc_task = balanced_accuracy_score(y_task, y_test_stress)\n",
    "\n",
    "    f1_a = f1_score(y_test_arousal, y_pred_arousal)\n",
    "    f1_v = f1_score(y_test_valence, y_pred_valence)\n",
    "    f1_task = f1_score(y_task, y_test_stress)\n",
    "\n",
    "    r2_a = r2_score(y_test_arousal, y_pred_arousal)\n",
    "    r2_v = r2_score(y_test_valence, y_pred_valence)\n",
    "    r2_task = r2_score(y_task, y_test_stress)\n",
    "\n",
    "    mse_a = mean_squared_error(y_test_arousal, y_pred_arousal)\n",
    "    mse_v = mean_squared_error(y_test_valence, y_pred_valence)\n",
    "    mse_task = mean_squared_error(y_task, y_test_stress)\n",
    "\n",
    "    return accuracy_arousal, balanced_acc_arousal, accuracy_valence, balanced_acc_valence, r2_a, r2_v, mse_a, mse_v, f1_a, f1_v, accuracy_task, balanced_acc_task, r2_task, mse_task, f1_task, num_zeros, num_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(X_train, y_train_arousal, y_train_valence, y_train_stress, X_test, y_test_arousal , y_test_valence, y_test_stress):\n",
    "\n",
    "    model_arousal_rf = RandomForestClassifier(n_estimators=100, random_state=random_seed)\n",
    "    model_arousal_rf.fit(X_train, y_train_arousal)\n",
    "\n",
    "    # Create and fit the Random Forest model for Valence with 100 base estimators\n",
    "    model_valence_rf = RandomForestClassifier(n_estimators=100, random_state=random_seed)\n",
    "    model_valence_rf.fit(X_train, y_train_valence)\n",
    "\n",
    "    # Predict on the test set for Arousal and Valence\n",
    "    y_pred_arousal_rf = model_arousal_rf.predict(X_test)\n",
    "    y_pred_valence_rf = model_valence_rf.predict(X_test)\n",
    "    num_zeros = list(y_pred_arousal_rf).count(0)\n",
    "    num_ones = list(y_pred_arousal_rf).count(1)\n",
    "\n",
    "    #Create and fit the Random Forest model for Task \n",
    "    model_task = RandomForestClassifier(n_estimators=100, random_state=random_seed)\n",
    "    model_task.fit(X_train,y_train_stress)\n",
    "    y_task = model_task.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy for Arousal and Valence classification\n",
    "    accuracy_arousal_rf = accuracy_score(y_test_arousal, y_pred_arousal_rf)\n",
    "    balanced_acc_arousal_rf = balanced_accuracy_score(y_test_arousal, y_pred_arousal_rf)\n",
    "    accuracy_valence_rf = accuracy_score(y_test_valence, y_pred_valence_rf)\n",
    "    balanced_acc_valence_rf = balanced_accuracy_score(y_test_valence, y_pred_valence_rf)\n",
    "    accuracy_task = accuracy_score(y_task, y_test_stress)\n",
    "    balanced_acc_task = balanced_accuracy_score(y_task, y_test_stress)\n",
    "\n",
    "    f1_a = f1_score(y_test_arousal, y_pred_arousal_rf)\n",
    "    f1_v = f1_score(y_test_valence, y_pred_valence_rf)\n",
    "    f1_task = f1_score(y_task, y_test_stress)\n",
    "\n",
    "\n",
    "    r2_a = r2_score(y_test_arousal, y_pred_arousal_rf)\n",
    "    r2_v = r2_score(y_test_valence, y_pred_valence_rf)\n",
    "    r2_task = r2_score(y_task, y_test_stress)\n",
    "\n",
    "    mse_a = mean_squared_error(y_test_arousal, y_pred_arousal_rf)\n",
    "    mse_v = mean_squared_error(y_test_valence, y_pred_valence_rf)\n",
    "    mse_task =mean_squared_error(y_task, y_test_stress)\n",
    "\n",
    "    return accuracy_arousal_rf, balanced_acc_arousal_rf, accuracy_valence_rf, balanced_acc_valence_rf, r2_a, r2_v, mse_a, mse_v, f1_a, f1_v, accuracy_task, balanced_acc_task, r2_task, mse_task, f1_task, num_zeros, num_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(X_train, y_train_arousal, y_train_valence, y_train_stress, X_test, y_test_arousal , y_test_valence, y_test_stress):\n",
    "\n",
    "    # Create and fit the SVM model for Arousal\n",
    "    model_arousal_svm = SVC(kernel='rbf',random_state=random_seed)\n",
    "    model_arousal_svm.fit(X_train, y_train_arousal)\n",
    "\n",
    "    # Create and fit the SVM model for Valence\n",
    "    model_valence_svm = SVC(kernel='rbf',random_state=random_seed)\n",
    "    model_valence_svm.fit(X_train, y_train_valence)\n",
    "\n",
    "    # Predict on the test set for Arousal and Valence\n",
    "    y_pred_arousal_svm = model_arousal_svm.predict(X_test)\n",
    "    y_pred_valence_svm = model_valence_svm.predict(X_test)\n",
    "    num_zeros = list(y_pred_arousal_svm).count(0)\n",
    "    num_ones = list(y_pred_arousal_svm).count(1)\n",
    "\n",
    "    #Create and fit the SVM for Task \n",
    "    model_task = SVC(kernel='rbf',random_state=random_seed)\n",
    "    model_task.fit(X_train,y_train_stress)\n",
    "    y_task = model_task.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy for Arousal and Valence classification\n",
    "    accuracy_arousal_svm = accuracy_score(y_test_arousal, y_pred_arousal_svm)\n",
    "    balanced_acc_arousal_svm = balanced_accuracy_score(y_test_arousal, y_pred_arousal_svm)\n",
    "    accuracy_valence_svm = accuracy_score(y_test_valence, y_pred_valence_svm)\n",
    "    balanced_acc_valence_svm = balanced_accuracy_score(y_test_valence, y_pred_valence_svm)\n",
    "    accuracy_task = accuracy_score(y_task, y_test_stress)\n",
    "    balanced_acc_task = balanced_accuracy_score(y_task, y_test_stress)\n",
    "\n",
    "    f1_a = f1_score(y_test_arousal, y_pred_arousal_svm)\n",
    "    f1_v = f1_score(y_test_valence, y_pred_valence_svm)\n",
    "    f1_task = f1_score(y_task, y_test_stress)\n",
    "\n",
    "    r2_a = r2_score(y_test_arousal, y_pred_arousal_svm)\n",
    "    r2_v = r2_score(y_test_valence, y_pred_valence_svm)\n",
    "    r2_task = r2_score(y_task, y_test_stress)\n",
    "\n",
    "    mse_a = mean_squared_error(y_test_arousal, y_pred_arousal_svm)\n",
    "    mse_v = mean_squared_error(y_test_valence, y_pred_valence_svm)\n",
    "    mse_task = mean_squared_error(y_task, y_test_stress)\n",
    "\n",
    "\n",
    "    return accuracy_arousal_svm, balanced_acc_arousal_svm, accuracy_valence_svm, balanced_acc_valence_svm, r2_a, r2_v, mse_a, mse_v, f1_a, f1_v, accuracy_task, balanced_acc_task, r2_task, mse_task, f1_task, num_zeros, num_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def train_lda(X_train, y_train_arousal, y_train_valence, y_train_stress, X_test, y_test_arousal , y_test_valence, y_test_stress):\n",
    "\n",
    "    # Create and fit the LDA model for Arousal with k=9\n",
    "    model_arousal_lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "    model_arousal_lda.fit(X_train, y_train_arousal)\n",
    "\n",
    "    # Create and fit the LDA model for Valence with k=9\n",
    "    model_valence_lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "    model_valence_lda.fit(X_train, y_train_valence)\n",
    "\n",
    "    # Predict on the test set for Arousal and Valence\n",
    "    y_pred_arousal_lda = model_arousal_lda.predict(X_test)\n",
    "    y_pred_valence_lda = model_valence_lda.predict(X_test)\n",
    "\n",
    "    num_zeros = list(y_pred_arousal_lda).count(0)\n",
    "    num_ones = list(y_pred_arousal_lda).count(1)\n",
    "\n",
    "    #Create and fit the LDA for Task \n",
    "    model_task = LinearDiscriminantAnalysis(n_components=1)\n",
    "    model_task.fit(X_train,y_train_stress)\n",
    "    y_task = model_task.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy for Arousal and Valence classification\n",
    "    accuracy_arousal_lda = accuracy_score(y_test_arousal, y_pred_arousal_lda)\n",
    "    balanced_acc_arousal_lda = balanced_accuracy_score(y_test_arousal, y_pred_arousal_lda)\n",
    "    accuracy_valence_lda = accuracy_score(y_test_valence, y_pred_valence_lda)\n",
    "    balanced_acc_valence_lda = balanced_accuracy_score(y_test_valence, y_pred_valence_lda)\n",
    "    accuracy_task = accuracy_score(y_task, y_test_stress)\n",
    "    balanced_acc_task = balanced_accuracy_score(y_task, y_test_stress)\n",
    "\n",
    "    f1_a = f1_score(y_test_arousal, y_pred_arousal_lda)\n",
    "    f1_v = f1_score(y_test_valence, y_pred_valence_lda)\n",
    "    f1_task = f1_score(y_task, y_test_stress)\n",
    "\n",
    "    r2_a = r2_score(y_test_arousal, y_pred_arousal_lda)\n",
    "    r2_v = r2_score(y_test_valence, y_pred_valence_lda)\n",
    "    r2_task = r2_score(y_task, y_test_stress)\n",
    "\n",
    "    mse_a = mean_squared_error(y_test_arousal, y_pred_arousal_lda)\n",
    "    mse_v = mean_squared_error(y_test_valence, y_pred_valence_lda)\n",
    "    mse_task = mean_squared_error(y_task, y_test_stress)\n",
    "\n",
    "    return accuracy_arousal_lda, balanced_acc_arousal_lda, accuracy_valence_lda, balanced_acc_valence_lda, r2_a, r2_v, mse_a, mse_v, f1_a, f1_v, accuracy_task, balanced_acc_task, r2_task, mse_task, f1_task, num_zeros, num_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def train_xgboost(X_train, y_train_arousal, y_train_valence, y_train_stress, X_test, y_test_arousal , y_test_valence, y_test_stress):\n",
    "\n",
    "    # Create and fit the XGBoost model for Arousal with 100 base estimators\n",
    "    model_arousal = GradientBoostingClassifier(n_estimators=100, random_state=random_seed)\n",
    "    model_arousal.fit(X_train, y_train_arousal)\n",
    "\n",
    "    # Create and fit the XGBoost model for Valence with 100 base estimators\n",
    "    model_valence = GradientBoostingClassifier(n_estimators=100, random_state=random_seed)\n",
    "    model_valence.fit(X_train, y_train_valence)\n",
    "\n",
    "    # Predict on the test set for Arousal and Valence\n",
    "    y_pred_arousal = model_arousal.predict(X_test)\n",
    "    y_pred_valence = model_valence.predict(X_test)\n",
    "    num_zeros = list(y_pred_arousal).count(0)\n",
    "    num_ones = list(y_pred_arousal).count(1)\n",
    "\n",
    "    #Create and fit the XG for Task \n",
    "    model_task = GradientBoostingClassifier(n_estimators=100, random_state=random_seed)\n",
    "    model_task.fit(X_train,y_train_stress)\n",
    "    y_task = model_task.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy for Arousal and Valence classification\n",
    "    accuracy_arousal = accuracy_score(y_test_arousal, y_pred_arousal)\n",
    "    balanced_acc_arousal = balanced_accuracy_score(y_test_arousal, y_pred_arousal)\n",
    "    accuracy_valence = accuracy_score(y_test_valence, y_pred_valence)\n",
    "    balanced_acc_valence= balanced_accuracy_score(y_test_valence, y_pred_valence)\n",
    "    accuracy_task = accuracy_score(y_task, y_test_stress)\n",
    "    balanced_acc_task = balanced_accuracy_score(y_task, y_test_stress)\n",
    "\n",
    "    f1_a = f1_score(y_test_arousal, y_pred_arousal)\n",
    "    f1_v = f1_score(y_test_valence, y_pred_valence)\n",
    "    f1_task = f1_score(y_task, y_test_stress)\n",
    "\n",
    "    r2_a = r2_score(y_test_arousal, y_pred_arousal)\n",
    "    r2_v = r2_score(y_test_valence, y_pred_valence)\n",
    "    r2_task = r2_score(y_task, y_test_stress)\n",
    "\n",
    "    mse_a = mean_squared_error(y_test_arousal, y_pred_arousal)\n",
    "    mse_v = mean_squared_error(y_test_valence, y_pred_valence)\n",
    "    mse_task = mean_squared_error(y_task, y_test_stress)\n",
    "\n",
    "    return accuracy_arousal, balanced_acc_arousal, accuracy_valence, balanced_acc_valence, r2_a, r2_v, mse_a, mse_v, f1_a, f1_v, accuracy_task, balanced_acc_task, r2_task, mse_task, f1_task, num_zeros, num_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def train_mlp(X_train, y_train_arousal, y_train_valence, y_train_stress, X_test, y_test_arousal, y_test_valence, y_test_stress, hidden_layer_sizes=(100,), random_seed=42):\n",
    "    # Create and fit the MLP model for Arousal\n",
    "    model_arousal_mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, random_state=random_seed)\n",
    "    model_arousal_mlp.fit(X_train, y_train_arousal)\n",
    "\n",
    "    # Create and fit the MLP model for Valence\n",
    "    model_valence_mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, random_state=random_seed)\n",
    "    model_valence_mlp.fit(X_train, y_train_valence)\n",
    "\n",
    "    # Predict on the test set for Arousal and Valence\n",
    "    y_pred_arousal_mlp = model_arousal_mlp.predict(X_test)\n",
    "    y_pred_valence_mlp = model_valence_mlp.predict(X_test)\n",
    "    num_zeros = list(y_pred_arousal_mlp).count(0)\n",
    "    num_ones = list(y_pred_arousal_mlp).count(1)\n",
    "\n",
    "    # Create and fit the MLP model for Task\n",
    "    model_task_mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, random_state=random_seed)\n",
    "    model_task_mlp.fit(X_train, y_train_stress)\n",
    "    y_pred_task_mlp = model_task_mlp.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy for Arousal, Valence, and Task classification\n",
    "    accuracy_arousal_mlp = accuracy_score(y_test_arousal, y_pred_arousal_mlp)\n",
    "    balanced_acc_arousal_mlp = balanced_accuracy_score(y_test_arousal, y_pred_arousal_mlp)\n",
    "    accuracy_valence_mlp = accuracy_score(y_test_valence, y_pred_valence_mlp)\n",
    "    balanced_acc_valence_mlp = balanced_accuracy_score(y_test_valence, y_pred_valence_mlp)\n",
    "    accuracy_task_mlp = accuracy_score(y_test_stress, y_pred_task_mlp)\n",
    "    balanced_acc_task_mlp = balanced_accuracy_score(y_test_stress, y_pred_task_mlp)\n",
    "\n",
    "    f1_arousal = f1_score(y_test_arousal, y_pred_arousal_mlp)\n",
    "    f1_valence = f1_score(y_test_valence, y_pred_valence_mlp)\n",
    "    f1_task = f1_score(y_test_stress, y_pred_task_mlp)\n",
    "\n",
    "    r2_arousal = r2_score(y_test_arousal, y_pred_arousal_mlp)\n",
    "    r2_valence = r2_score(y_test_valence, y_pred_valence_mlp)\n",
    "    r2_task = r2_score(y_test_stress, y_pred_task_mlp)\n",
    "\n",
    "    mse_arousal = mean_squared_error(y_test_arousal, y_pred_arousal_mlp)\n",
    "    mse_valence = mean_squared_error(y_test_valence, y_pred_valence_mlp)\n",
    "    mse_task = mean_squared_error(y_test_stress, y_pred_task_mlp)\n",
    "\n",
    "    return (accuracy_arousal_mlp, balanced_acc_arousal_mlp, accuracy_valence_mlp, balanced_acc_valence_mlp, \n",
    "            r2_arousal, r2_valence, mse_arousal, mse_valence, f1_arousal, f1_valence, \n",
    "            accuracy_task_mlp, balanced_acc_task_mlp, r2_task, mse_task, f1_task, num_zeros, num_ones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(df, pi, relevant_features):\n",
    "\n",
    "    accuracy_scores_a_mlp = {}\n",
    "    balance_acc_list_a_mlp = {}\n",
    "    accuracy_scores_v_mlp = {}\n",
    "    balance_acc_list_v_mlp = {}\n",
    "    r2_a_mlp = {}\n",
    "    r2_v_mlp = {}\n",
    "    mse_a_mlp = {}\n",
    "    mse_v_mlp = {}\n",
    "    f1_a_mlp = {}\n",
    "    f1_v_mlp = {}\n",
    "    accuracy_scores_s_mlp = {}\n",
    "    balance_acc_list_s_mlp = {}\n",
    "    r2_s_mlp = {}\n",
    "    mse_s_mlp = {}\n",
    "    f1_s_mlp = {}\n",
    "\n",
    "    accuracy_scores_a_lr = {}\n",
    "    balance_acc_list_a_lr = {}\n",
    "    accuracy_scores_v_lr = {}\n",
    "    balance_acc_list_v_lr = {}\n",
    "    r2_a_lr = {}\n",
    "    r2_v_lr ={}\n",
    "    mse_a_lr = {}\n",
    "    mse_v_lr = {}\n",
    "    f1_a_lr = {}\n",
    "    f1_v_lr = {}\n",
    "    accuracy_scores_s_lr = {}\n",
    "    balance_acc_list_s_lr = {}\n",
    "    r2_s_lr = {}\n",
    "    mse_s_lr = {}\n",
    "    f1_s_lr = {}\n",
    "\n",
    "    num_zeros_rf = {}\n",
    "    num_zeros_xg = {}\n",
    "    num_ones_xg = {}\n",
    "    num_ones_rf = {}\n",
    "\n",
    "    accuracy_scores_a_dt = {}\n",
    "    balance_acc_list_a_dt = {}\n",
    "    accuracy_scores_v_dt = {}\n",
    "    balance_acc_list_v_dt = {}\n",
    "    r2_a_dt = {}\n",
    "    r2_v_dt ={}\n",
    "    mse_a_dt = {}\n",
    "    mse_v_dt = {}\n",
    "    f1_a_dt = {}\n",
    "    f1_v_dt = {}\n",
    "    accuracy_scores_s_dt = {}\n",
    "    balance_acc_list_s_dt = {}\n",
    "    r2_s_dt = {}\n",
    "    mse_s_dt = {}\n",
    "    f1_s_dt = {}\n",
    "\n",
    "    accuracy_scores_a_rf = {}\n",
    "    balance_acc_list_a_rf = {}\n",
    "    accuracy_scores_v_rf = {}\n",
    "    balance_acc_list_v_rf = {}\n",
    "    r2_a_rf = {}\n",
    "    r2_v_rf ={}\n",
    "    mse_a_rf = {}\n",
    "    mse_v_rf = {}\n",
    "    f1_a_rf = {}\n",
    "    f1_v_rf = {}\n",
    "    accuracy_scores_s_rf = {}\n",
    "    balance_acc_list_s_rf = {}\n",
    "    r2_s_rf = {}\n",
    "    mse_s_rf = {}\n",
    "    f1_s_rf = {}\n",
    "\n",
    "    accuracy_scores_a_svm = {}\n",
    "    balance_acc_list_a_svm = {}\n",
    "    accuracy_scores_v_svm = {}\n",
    "    balance_acc_list_v_svm = {}\n",
    "    r2_a_svm = {}\n",
    "    r2_v_svm ={}\n",
    "    mse_a_svm = {}\n",
    "    mse_v_svm = {}\n",
    "    f1_a_svm = {}\n",
    "    f1_v_svm = {}\n",
    "    accuracy_scores_s_svm = {}\n",
    "    balance_acc_list_s_svm = {}\n",
    "    r2_s_svm = {}\n",
    "    mse_s_svm = {}\n",
    "    f1_s_svm = {}\n",
    "\n",
    "    accuracy_scores_a_lda = {}\n",
    "    balance_acc_list_a_lda = {}\n",
    "    accuracy_scores_v_lda = {}\n",
    "    balance_acc_list_v_lda = {}\n",
    "    r2_a_lda = {}\n",
    "    r2_v_lda ={}\n",
    "    mse_a_lda = {}\n",
    "    mse_v_lda = {}\n",
    "    f1_a_lda = {}\n",
    "    f1_v_lda = {}\n",
    "    accuracy_scores_s_lda = {}\n",
    "    balance_acc_list_s_lda = {}\n",
    "    r2_s_lda = {}\n",
    "    mse_s_lda = {}\n",
    "    f1_s_lda = {}\n",
    "\n",
    "    accuracy_scores_a_xg = {}\n",
    "    balance_acc_list_a_xg = {}\n",
    "    accuracy_scores_v_xg = {}\n",
    "    balance_acc_list_v_xg = {}\n",
    "    r2_a_xg = {}\n",
    "    r2_v_xg ={}\n",
    "    mse_a_xg = {}\n",
    "    mse_v_xg = {}\n",
    "    f1_a_xg = {}\n",
    "    f1_v_xg = {}\n",
    "    accuracy_scores_s_xg = {}\n",
    "    balance_acc_list_s_xg = {}\n",
    "    r2_s_xg = {}\n",
    "    mse_s_xg = {}\n",
    "    f1_s_xg = {}\n",
    "    \n",
    "\n",
    "    pi = list(set(pi))\n",
    "    for i in pi:\n",
    "\n",
    "        train_data = df[df['Participant ID'] != i]\n",
    "        test_data = df[df['Participant ID'] == i]\n",
    "\n",
    "        X_train = train_data[relevant_features]\n",
    "        y_train_arousal = train_data['arousal_category'].astype(int)\n",
    "        y_train_valence = train_data['valence_category'].astype(int)\n",
    "        y_train_stress = train_data['taskwiselabel'].astype(int)\n",
    "\n",
    "        X_test = test_data[relevant_features]\n",
    "        y_test_arousal = test_data['arousal_category'].astype(int)\n",
    "        y_test_valence = test_data['valence_category'].astype(int)\n",
    "        y_test_stress = test_data['taskwiselabel'].astype(int)\n",
    "\n",
    "        # Train the MLP classifier with specified hidden layers\n",
    "        accuracy_arousal, balanced_acc_arousal, accuracy_valence, balanced_acc_valence, r2_a, r2_v, mse_a, mse_v, f1_a, f1_v, accuracy_task, balanced_acc_task, r2_task, mse_task, f1_task, num_zeros, num_ones = train_mlp(X_train, y_train_arousal, y_train_valence, y_train_stress, X_test, y_test_arousal, y_test_valence, y_test_stress, hidden_layer_sizes=(100, 50))\n",
    "\n",
    "        # Storing the results\n",
    "        accuracy_scores_a_mlp[i] = accuracy_arousal * 100\n",
    "        balance_acc_list_a_mlp[i] = balanced_acc_arousal * 100\n",
    "        accuracy_scores_v_mlp[i] = accuracy_valence * 100\n",
    "        balance_acc_list_v_mlp[i] = balanced_acc_valence * 100\n",
    "        r2_a_mlp[i] = r2_a\n",
    "        r2_v_mlp[i] = r2_v\n",
    "        mse_a_mlp[i] = mse_a\n",
    "        mse_v_mlp[i] = mse_v\n",
    "        f1_a_mlp[i] = f1_a\n",
    "        f1_v_mlp[i] = f1_v\n",
    "        accuracy_scores_s_mlp[i] = accuracy_task * 100\n",
    "        balance_acc_list_s_mlp[i] = balanced_acc_task * 100\n",
    "        r2_s_mlp[i] = r2_task\n",
    "        mse_s_mlp[i] = mse_task\n",
    "        f1_s_mlp[i] = f1_task\n",
    "\n",
    "\n",
    "        accuracy_arousal, balanced_acc_arousal, accuracy_valence, balanced_acc_valence, r2_a, r2_v, mse_a, mse_v, f1_a, f1_v, accuracy_task, balanced_acc_task, r2_task, mse_task, f1_task, num_zeros, num_ones = train_logistic_regression(X_train, y_train_arousal, y_train_valence, y_train_stress, X_test, y_test_arousal , y_test_valence, y_test_stress)\n",
    "\n",
    "        accuracy_scores_a_lr[i] = accuracy_arousal*100\n",
    "        balance_acc_list_a_lr[i] = balanced_acc_arousal*100\n",
    "        accuracy_scores_v_lr[i] = accuracy_valence*100\n",
    "        balance_acc_list_v_lr[i] = balanced_acc_valence*100\n",
    "        r2_a_lr[i] = r2_a\n",
    "        r2_v_lr[i] = r2_v\n",
    "        mse_a_lr[i] = mse_a\n",
    "        mse_v_lr[i] = mse_v\n",
    "        f1_a_lr[i] = f1_a\n",
    "        f1_v_lr[i] = f1_v\n",
    "        accuracy_scores_s_lr[i] = accuracy_task*100\n",
    "        balance_acc_list_s_lr[i] = balanced_acc_task*100\n",
    "        r2_s_lr[i] = r2_task\n",
    "        mse_s_lr[i] = mse_task\n",
    "        f1_s_lr[i] = f1_task\n",
    "\n",
    "        accuracy_arousal, balanced_acc_arousal, accuracy_valence, balanced_acc_valence, r2_a, r2_v, mse_a, mse_v, f1_a, f1_v, accuracy_task, balanced_acc_task, r2_task, mse_task, f1_task, num_zeros, num_ones= train_decision_tree(X_train, y_train_arousal, y_train_valence, y_train_stress, X_test, y_test_arousal , y_test_valence, y_test_stress)\n",
    "\n",
    "        accuracy_scores_a_dt[i] = accuracy_arousal*100\n",
    "        balance_acc_list_a_dt[i] = balanced_acc_arousal*100\n",
    "        accuracy_scores_v_dt[i] = accuracy_valence*100\n",
    "        balance_acc_list_v_dt[i] = balanced_acc_valence*100\n",
    "        r2_a_dt[i] = r2_a\n",
    "        r2_v_dt[i] = r2_v\n",
    "        mse_a_dt[i] = mse_a\n",
    "        mse_v_dt[i] = mse_v\n",
    "        f1_a_dt[i] = f1_a\n",
    "        f1_v_dt[i] = f1_v\n",
    "        accuracy_scores_s_dt[i] = accuracy_task*100\n",
    "        balance_acc_list_s_dt[i] = balanced_acc_task*100\n",
    "        r2_s_dt[i] = r2_task\n",
    "        mse_s_dt[i] = mse_task\n",
    "        f1_s_dt[i] = f1_task\n",
    "\n",
    "        accuracy_arousal, balanced_acc_arousal, accuracy_valence, balanced_acc_valence, r2_a, r2_v, mse_a, mse_v, f1_a, f1_v, accuracy_task, balanced_acc_task, r2_task, mse_task, f1_task, num_zeros, num_ones = train_random_forest(X_train, y_train_arousal, y_train_valence, y_train_stress, X_test, y_test_arousal , y_test_valence, y_test_stress)\n",
    "\n",
    "        accuracy_scores_a_rf[i] = accuracy_arousal*100\n",
    "        balance_acc_list_a_rf[i] = balanced_acc_arousal*100\n",
    "        accuracy_scores_v_rf[i] = accuracy_valence*100\n",
    "        balance_acc_list_v_rf[i] = balanced_acc_valence*100\n",
    "        r2_a_rf[i] = r2_a\n",
    "        r2_v_rf[i] = r2_v\n",
    "        mse_a_rf[i] = mse_a\n",
    "        mse_v_rf[i] = mse_v\n",
    "        f1_a_rf[i] = f1_a\n",
    "        f1_v_rf[i] = f1_v\n",
    "        accuracy_scores_s_rf[i] = accuracy_task*100\n",
    "        balance_acc_list_s_rf[i] = balanced_acc_task*100\n",
    "        r2_s_rf[i] = r2_task\n",
    "        mse_s_rf[i] = mse_task\n",
    "        f1_s_rf[i] = f1_task\n",
    "        num_zeros_rf[i] = num_zeros\n",
    "        num_ones_rf[i] = num_ones\n",
    "\n",
    "        accuracy_arousal, balanced_acc_arousal, accuracy_valence, balanced_acc_valence, r2_a, r2_v, mse_a, mse_v, f1_a, f1_v, accuracy_task, balanced_acc_task, r2_task, mse_task, f1_task, num_zeros, num_ones = train_svm(X_train, y_train_arousal, y_train_valence, y_train_stress, X_test, y_test_arousal , y_test_valence, y_test_stress)\n",
    "\n",
    "        accuracy_scores_a_svm[i] = accuracy_arousal*100\n",
    "        balance_acc_list_a_svm[i] = balanced_acc_arousal*100\n",
    "        accuracy_scores_v_svm[i] = accuracy_valence*100\n",
    "        balance_acc_list_v_svm[i] = balanced_acc_valence*100\n",
    "        r2_a_svm[i] = r2_a\n",
    "        r2_v_svm[i] = r2_v\n",
    "        mse_a_svm[i] = mse_a\n",
    "        mse_v_svm[i] = mse_v\n",
    "        f1_a_svm[i] = f1_a\n",
    "        f1_v_svm[i] = f1_v\n",
    "        accuracy_scores_s_svm[i] = accuracy_task*100\n",
    "        balance_acc_list_s_svm[i] = balanced_acc_task*100\n",
    "        r2_s_svm[i] = r2_task\n",
    "        mse_s_svm[i] = mse_task\n",
    "        f1_s_svm[i] = f1_task\n",
    "\n",
    "        accuracy_arousal, balanced_acc_arousal, accuracy_valence, balanced_acc_valence, r2_a, r2_v, mse_a, mse_v, f1_a, f1_v, accuracy_task, balanced_acc_task, r2_task, mse_task, f1_task, num_zeros, num_ones = train_lda(X_train, y_train_arousal, y_train_valence, y_train_stress, X_test, y_test_arousal , y_test_valence, y_test_stress)\n",
    "\n",
    "        accuracy_scores_a_lda[i] = accuracy_arousal*100\n",
    "        balance_acc_list_a_lda[i] = balanced_acc_arousal*100\n",
    "        accuracy_scores_v_lda[i] = accuracy_valence*100\n",
    "        balance_acc_list_v_lda[i] = balanced_acc_valence*100\n",
    "        r2_a_lda[i] = r2_a\n",
    "        r2_v_lda[i] = r2_v\n",
    "        mse_a_lda[i] = mse_a\n",
    "        mse_v_lda[i] = mse_v\n",
    "        f1_a_lda[i] = f1_a\n",
    "        f1_v_lda[i] = f1_v\n",
    "        accuracy_scores_s_lda[i] = accuracy_task*100\n",
    "        balance_acc_list_s_lda[i] = balanced_acc_task*100\n",
    "        r2_s_lda[i] = r2_task\n",
    "        mse_s_lda[i] = mse_task\n",
    "        f1_s_lda[i] = f1_task\n",
    "\n",
    "        accuracy_arousal, balanced_acc_arousal, accuracy_valence, balanced_acc_valence, r2_a, r2_v, mse_a, mse_v, f1_a, f1_v, accuracy_task, balanced_acc_task, r2_task, mse_task, f1_task, num_zeros, num_ones = train_xgboost(X_train, y_train_arousal, y_train_valence, y_train_stress, X_test, y_test_arousal , y_test_valence, y_test_stress)\n",
    "\n",
    "        accuracy_scores_a_xg[i] = accuracy_arousal*100\n",
    "        balance_acc_list_a_xg[i] = balanced_acc_arousal*100\n",
    "        accuracy_scores_v_xg[i] = accuracy_valence*100\n",
    "        balance_acc_list_v_xg[i] = balanced_acc_valence*100\n",
    "        r2_a_xg[i] = r2_a\n",
    "        r2_v_xg[i] = r2_v\n",
    "        mse_a_xg[i] = mse_a\n",
    "        mse_v_xg[i] = mse_v\n",
    "        f1_a_xg[i] = f1_a\n",
    "        f1_v_xg[i] = f1_v\n",
    "        accuracy_scores_s_xg[i] = accuracy_task*100\n",
    "        balance_acc_list_s_xg[i] = balanced_acc_task*100\n",
    "        r2_s_xg[i] = r2_task\n",
    "        mse_s_xg[i] = mse_task\n",
    "        f1_s_xg[i] = f1_task\n",
    "        num_ones_xg[i] = num_ones\n",
    "        num_zeros_xg[i] = num_zeros\n",
    "\n",
    "    for i in pi:\n",
    "\n",
    "        print(f\"--------------Participant ID: {i}--------------\")\n",
    "        print(\"----Model: MLP----\")\n",
    "        print(f\"Accuracy - Arousal: {accuracy_scores_a_mlp[i]}\")\n",
    "        print(f\"Balance Accuracy - Arousal: {balance_acc_list_a_mlp[i]}\")\n",
    "        print(f\"R2 Loss - Arousal: {r2_a_mlp[i]}\")\n",
    "        print(f\"MSE Loss - Arousal: {mse_a_mlp[i]}\")\n",
    "        print(f\"F1 Score - Arousal: {f1_a_mlp[i]}\")\n",
    "        print(f\"Accuracy - Valence: {accuracy_scores_v_mlp[i]}\")\n",
    "        print(f\"Balance Accuracy - Valence: {balance_acc_list_v_mlp[i]}\")\n",
    "        print(f\"R2 Loss - Valence: {r2_v_mlp[i]}\")\n",
    "        print(f\"MSE Loss - Valence: {mse_v_mlp[i]}\")\n",
    "        print(f\"F1 Score - Valence: {f1_v_mlp[i]}\")\n",
    "        print(f\"Accuracy - Stimulus-label: {accuracy_scores_s_mlp[i]}\")\n",
    "        print(f\"Balance Accuracy - Stimulus-label: {balance_acc_list_s_mlp[i]}\")\n",
    "        print(f\"R2 Loss - Stimulus-label: {r2_s_mlp[i]}\")\n",
    "        print(f\"MSE Loss - Stimulus-label: {mse_s_mlp[i]}\")\n",
    "        print(f\"F1 Score - Stimulus-label: {f1_s_mlp[i]}\")\n",
    "\n",
    "        print(f\"--------------Participant ID: {i}--------------\")\n",
    "        print(\"----Model: Logistic Regression----\")\n",
    "        print(f\"Accuracy - Arousal: {accuracy_scores_a_lr[i]}\")\n",
    "        print(f\"Balance Accuracy - Arousal: {balance_acc_list_a_lr[i]}\")\n",
    "        print(f\"R2 Loss - Arousal: {r2_a_lr[i]}\")\n",
    "        print(f\"MSE Loss - Arousal: {mse_a_lr[i]}\")\n",
    "        print(f\"F1 Score - Arousal: {f1_a_lr[i]}\")\n",
    "        print(f\"Accuracy - Valence: {accuracy_scores_v_lr[i]}\")\n",
    "        print(f\"Balance Accuracy - Valence: {balance_acc_list_v_lr[i]}\")\n",
    "        print(f\"R2 Loss - Valence: {r2_v_lr[i]}\")\n",
    "        print(f\"MSE Loss - Valence: {mse_v_lr[i]}\")\n",
    "        print(f\"F1 Score - Valence: {f1_v_lr[i]}\")\n",
    "        print(f\"Accuracy - Stimulus-label: {accuracy_scores_s_lr[i]}\")\n",
    "        print(f\"Balance Accuracy - Stimulus-label: {balance_acc_list_s_lr[i]}\")\n",
    "        print(f\"R2 Loss - Stimulus-label: {r2_s_lr[i]}\")\n",
    "        print(f\"MSE Loss - Stimulus-label: {mse_s_lr[i]}\")\n",
    "        print(f\"F1 Score - Stimulus-label: {f1_s_lr[i]}\")\n",
    "\n",
    "        print(\"----Model: Decision Tree----\")\n",
    "        print(f\"Accuracy - Arousal: {accuracy_scores_a_dt[i]}\")\n",
    "        print(f\"Balance Accuracy - Arousal: {balance_acc_list_a_dt[i]}\")\n",
    "        print(f\"R2 Loss - Arousal: {r2_a_dt[i]}\")\n",
    "        print(f\"MSE Loss - Arousal: {mse_a_dt[i]}\")\n",
    "        print(f\"F1 Score - Arousal: {f1_a_dt[i]}\")\n",
    "        print(f\"Accuracy - Valence: {accuracy_scores_v_dt[i]}\")\n",
    "        print(f\"Balance Accuracy - Valence: {balance_acc_list_v_dt[i]}\")\n",
    "        print(f\"R2 Loss - Valence: {r2_v_dt[i]}\")\n",
    "        print(f\"MSE Loss - Valence: {mse_v_dt[i]}\")\n",
    "        print(f\"F1 Score - Valence: {f1_v_dt[i]}\")\n",
    "        print(f\"Accuracy - Stimulus-label: {accuracy_scores_s_dt[i]}\")\n",
    "        print(f\"Balance Accuracy - Stimulus-label: {balance_acc_list_s_dt[i]}\")\n",
    "        print(f\"R2 Loss - Stimulus-label: {r2_s_dt[i]}\")\n",
    "        print(f\"MSE Loss - Stimulus-label: {mse_s_dt[i]}\")\n",
    "        print(f\"F1 Score - Stimulus-label: {f1_s_dt[i]}\")\n",
    "\n",
    "        print(\"----Model: Random Forest----\")\n",
    "        print(f\"Accuracy - Arousal: {accuracy_scores_a_rf[i]}\")\n",
    "        print(f\"Balance Accuracy - Arousal: {balance_acc_list_a_rf[i]}\")\n",
    "        print(f\"R2 Loss - Arousal: {r2_a_rf[i]}\")\n",
    "        print(f\"MSE Loss - Arousal: {mse_a_rf[i]}\")\n",
    "        print(f\"F1 Score - Arousal: {f1_a_rf[i]}\")\n",
    "        print(f\"Accuracy - Valence: {accuracy_scores_v_rf[i]}\")\n",
    "        print(f\"Balance Accuracy - Valence: {balance_acc_list_v_rf[i]}\")\n",
    "        print(f\"R2 Loss - Valence: {r2_v_rf[i]}\")\n",
    "        print(f\"MSE Loss - Valence: {mse_v_rf[i]}\")\n",
    "        print(f\"F1 Score - Valence: {f1_v_rf[i]}\")\n",
    "        print(f\"Accuracy - Stimulus-label: {accuracy_scores_s_rf[i]}\")\n",
    "        print(f\"Balance Accuracy - Stimulus-label: {balance_acc_list_s_rf[i]}\")\n",
    "        print(f\"R2 Loss - Stimulus-label: {r2_s_rf[i]}\")\n",
    "        print(f\"MSE Loss - Stimulus-label: {mse_s_rf[i]}\")\n",
    "        print(f\"F1 Score - Stimulus-label: {f1_s_rf[i]}\")\n",
    "\n",
    "        print(\"----Model: SVM----\")\n",
    "        print(f\"Accuracy - Arousal: {accuracy_scores_a_svm[i]}\")\n",
    "        print(f\"Balance Accuracy - Arousal: {balance_acc_list_a_svm[i]}\")\n",
    "        print(f\"R2 Loss - Arousal: {r2_a_svm[i]}\")\n",
    "        print(f\"MSE Loss - Arousal: {mse_a_svm[i]}\")\n",
    "        print(f\"F1 Score - Arousal: {f1_a_svm[i]}\")\n",
    "        print(f\"Accuracy - Valence: {accuracy_scores_v_svm[i]}\")\n",
    "        print(f\"Balance Accuracy - Valence: {balance_acc_list_v_svm[i]}\")\n",
    "        print(f\"R2 Loss - Valence: {r2_v_svm[i]}\")\n",
    "        print(f\"MSE Loss - Valence: {mse_v_svm[i]}\")\n",
    "        print(f\"F1 Score - Valence: {f1_v_svm[i]}\")\n",
    "        print(f\"Accuracy - Stimulus-label: {accuracy_scores_s_svm[i]}\")\n",
    "        print(f\"Balance Accuracy - Stimulus-label: {balance_acc_list_s_svm[i]}\")\n",
    "        print(f\"R2 Loss - Stimulus-label: {r2_s_svm[i]}\")\n",
    "        print(f\"MSE Loss - Stimulus-label: {mse_s_svm[i]}\")\n",
    "        print(f\"F1 Score - Stimulus-label: {f1_s_svm[i]}\")\n",
    "\n",
    "        print(\"----Model: LDA----\")\n",
    "        print(f\"Accuracy - Arousal: {accuracy_scores_a_lda[i]}\")\n",
    "        print(f\"Balance Accuracy - Arousal: {balance_acc_list_a_lda[i]}\")\n",
    "        print(f\"R2 Loss - Arousal: {r2_a_lda[i]}\")\n",
    "        print(f\"MSE Loss - Arousal: {mse_a_lda[i]}\")\n",
    "        print(f\"F1 Score - Arousal: {f1_a_lda[i]}\")\n",
    "        print(f\"Accuracy - Valence: {accuracy_scores_v_lda[i]}\")\n",
    "        print(f\"Balance Accuracy - Valence: {balance_acc_list_v_lda[i]}\")\n",
    "        print(f\"R2 Loss - Valence: {r2_v_lda[i]}\")\n",
    "        print(f\"MSE Loss - Valence: {mse_v_lda[i]}\")\n",
    "        print(f\"F1 Score - Valence: {f1_v_lda[i]}\")\n",
    "        print(f\"Accuracy - Stimulus-label: {accuracy_scores_s_lda[i]}\")\n",
    "        print(f\"Balance Accuracy - Stimulus-label: {balance_acc_list_s_lda[i]}\")\n",
    "        print(f\"R2 Loss - Stimulus-label: {r2_s_lda[i]}\")\n",
    "        print(f\"MSE Loss - Stimulus-label: {mse_s_lda[i]}\")\n",
    "        print(f\"F1 Score - Stimulus-label: {f1_s_lda[i]}\")\n",
    "\n",
    "        print(\"----Model: XG----\")\n",
    "        print(f\"Accuracy - Arousal: {accuracy_scores_a_xg[i]}\")\n",
    "        print(f\"Balance Accuracy - Arousal: {balance_acc_list_a_xg[i]}\")\n",
    "        print(f\"R2 Loss - Arousal: {r2_a_xg[i]}\")\n",
    "        print(f\"MSE Loss - Arousal: {mse_a_xg[i]}\")\n",
    "        print(f\"F1 Score - Arousal: {f1_a_xg[i]}\")\n",
    "        print(f\"Accuracy - Valence: {accuracy_scores_v_xg[i]}\")\n",
    "        print(f\"Balance Accuracy - Valence: {balance_acc_list_v_xg[i]}\")\n",
    "        print(f\"R2 Loss - Valence: {r2_v_xg[i]}\")\n",
    "        print(f\"MSE Loss - Valence: {mse_v_xg[i]}\")\n",
    "        print(f\"F1 Score - Valence: {f1_v_xg[i]}\")\n",
    "        print(f\"Accuracy - Stimulus-label: {accuracy_scores_s_xg[i]}\")\n",
    "        print(f\"Balance Accuracy - Stimulus-label: {balance_acc_list_s_xg[i]}\")\n",
    "        print(f\"R2 Loss - Stimulus-label: {r2_s_xg[i]}\")\n",
    "        print(f\"MSE Loss - Stimulus-label: {mse_s_xg[i]}\")\n",
    "        print(f\"F1 Score - Stimulus-label: {f1_s_xg[i]}\")\n",
    "\n",
    "\n",
    "    print(f\"-----Average Accuracy Arousal-----\")\n",
    "    print(f\"Logistic Regression: {sum(accuracy_scores_a_lr.values()) / len(pi)}\")\n",
    "    print(f\"Decision Tree: {sum(accuracy_scores_a_dt.values()) / len(pi)}\")\n",
    "    print(f\"Random Forest: {sum(accuracy_scores_a_rf.values()) / len(pi)}\")\n",
    "    print(f\"SVM: {sum(accuracy_scores_a_svm.values()) / len(pi)}\")\n",
    "    print(f\"LDA: {sum(accuracy_scores_a_lda.values()) / len(pi)}\")\n",
    "    print(f\"XG: {sum(accuracy_scores_a_xg.values()) / len(pi)}\")\n",
    "    print(f\"MLP: {sum(accuracy_scores_a_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average Balanced Accuracy Arousal-----\")\n",
    "    print(f\"Logistic Regression: {sum(balance_acc_list_a_lr.values()) / len(pi)}\")\n",
    "    print(f\"Decision Tree: {sum(balance_acc_list_a_dt.values()) / len(pi)}\")\n",
    "    print(f\"Random Forest: {sum(balance_acc_list_a_rf.values()) / len(pi)}\")\n",
    "    print(f\"SVM: {sum(balance_acc_list_a_svm.values()) / len(pi)}\")\n",
    "    print(f\"LDA: {sum(balance_acc_list_a_lda.values()) / len(pi)}\")\n",
    "    print(f\"XG: {sum(balance_acc_list_a_xg.values()) / len(pi)}\")\n",
    "    print(f\"MLP: {sum(balance_acc_list_a_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average R2 Loss Arousal-----\")\n",
    "    print(f\"Logistic Regression: {sum(r2_a_lr.values()) / len(pi)}\")\n",
    "    print(f\"Decision Tree: {sum(r2_a_dt.values()) / len(pi)}\")\n",
    "    print(f\"Random Forest: {sum(r2_a_rf.values()) / len(pi)}\")\n",
    "    print(f\"SVM: {sum(r2_a_svm.values()) / len(pi)}\")\n",
    "    print(f\"LDA: {sum(r2_a_lda.values()) / len(pi)}\")\n",
    "    print(f\"XG: {sum(r2_a_xg.values()) / len(pi)}\")\n",
    "    print(f\"MLP: {sum(r2_a_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average MSE Loss Arousal-----\")\n",
    "    print(f\"Logistic Regression: {sum(mse_a_lr.values()) / len(pi)}\")\n",
    "    print(f\"Decision Tree: {sum(mse_a_dt.values()) / len(pi)}\")\n",
    "    print(f\"Random Forest: {sum(mse_a_rf.values()) / len(pi)}\")\n",
    "    print(f\"SVM: {sum(mse_a_svm.values()) / len(pi)}\")\n",
    "    print(f\"LDA: {sum(mse_a_lda.values()) / len(pi)}\")\n",
    "    print(f\"XG: {sum(mse_a_xg.values()) / len(pi)}\")\n",
    "    print(f\"MLP: {sum(mse_a_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average F1 Arousal-----\")\n",
    "    print(f\"Logistic Regression: {sum(f1_a_lr.values()) / len(pi)}\")\n",
    "    print(f\"Decision Tree: {sum(f1_a_dt.values()) / len(pi)}\")\n",
    "    print(f\"Random Forest: {sum(f1_a_rf.values()) / len(pi)}\")\n",
    "    print(f\"SVM: {sum(f1_a_svm.values()) / len(pi)}\")\n",
    "    print(f\"LDA: {sum(f1_a_lda.values()) / len(pi)}\")\n",
    "    print(f\"XG: {sum(f1_a_xg.values()) / len(pi)}\")\n",
    "    print(f\"MLP: {sum(f1_a_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average Accuracy Valence-----\")\n",
    "    print(f\"Logistic Regression: {sum(accuracy_scores_v_lr.values()) / len(pi)}\")\n",
    "    print(f\"Decision Tree: {sum(accuracy_scores_v_dt.values()) / len(pi)}\")\n",
    "    print(f\"Random Forest: {sum(accuracy_scores_v_rf.values()) / len(pi)}\")\n",
    "    print(f\"SVM: {sum(accuracy_scores_v_svm.values()) / len(pi)}\")\n",
    "    print(f\"LDA: {sum(accuracy_scores_v_lda.values()) / len(pi)}\")\n",
    "    print(f\"XG: {sum(accuracy_scores_v_xg.values()) / len(pi)}\")\n",
    "    print(f\"MLP: {sum(accuracy_scores_v_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average Balanced Accuracy Valence-----\")\n",
    "    print(f\"Logistic Regression: {sum(balance_acc_list_v_lr.values()) / len(pi)}\")\n",
    "    print(f\"Decision Tree: {sum(balance_acc_list_v_dt.values()) / len(pi)}\")\n",
    "    print(f\"Random Forest: {sum(balance_acc_list_v_rf.values()) / len(pi)}\")\n",
    "    print(f\"SVM: {sum(balance_acc_list_v_svm.values()) / len(pi)}\")\n",
    "    print(f\"LDA: {sum(balance_acc_list_v_lda.values()) / len(pi)}\")\n",
    "    print(f\"XG: {sum(balance_acc_list_v_xg.values()) / len(pi)}\")\n",
    "    print(f\"MLP: {sum(balance_acc_list_v_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average R2 Loss Valence-----\")\n",
    "    print(f\"Logistic Regression: {sum(r2_v_lr.values()) / len(pi)}\")\n",
    "    print(f\"Decision Tree: {sum(r2_v_dt.values()) / len(pi)}\")\n",
    "    print(f\"Random Forest: {sum(r2_v_rf.values()) / len(pi)}\")\n",
    "    print(f\"SVM: {sum(r2_v_svm.values()) / len(pi)}\")\n",
    "    print(f\"LDA: {sum(r2_v_lda.values()) / len(pi)}\")\n",
    "    print(f\"XG: {sum(r2_v_xg.values()) / len(pi)}\")\n",
    "    print(f\"MLP: {sum(r2_v_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average MSE Loss Valence-----\")\n",
    "    print(f\"Logistic Regression: {sum(mse_v_lr.values()) / len(pi)}\")\n",
    "    print(f\"Decision Tree: {sum(mse_v_dt.values()) / len(pi)}\")\n",
    "    print(f\"Random Forest: {sum(mse_v_rf.values()) / len(pi)}\")\n",
    "    print(f\"SVM: {sum(mse_v_svm.values()) / len(pi)}\")\n",
    "    print(f\"LDA: {sum(mse_v_lda.values()) / len(pi)}\")\n",
    "    print(f\"XG: {sum(mse_v_xg.values()) / len(pi)}\")\n",
    "    print(f\"MLP: {sum(mse_v_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average F1 Valence-----\")\n",
    "    print(f\"Logistic Regression: {sum(f1_v_lr.values()) / len(pi)}\")\n",
    "    print(f\"Decision Tree: {sum(f1_v_dt.values()) / len(pi)}\")\n",
    "    print(f\"Random Forest: {sum(f1_v_rf.values()) / len(pi)}\")\n",
    "    print(f\"SVM: {sum(f1_v_svm.values()) / len(pi)}\")\n",
    "    print(f\"LDA: {sum(f1_v_lda.values()) / len(pi)}\")\n",
    "    print(f\"XG: {sum(f1_v_xg.values()) / len(pi)}\")\n",
    "    print(f\"MLP: {sum(f1_v_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average Accuracy Stimulus-label-----\")\n",
    "    print(f\"Logistic Regression: {sum(accuracy_scores_s_lr.values()) / len(pi)}\")\n",
    "    print(f\"Decision Tree: {sum(accuracy_scores_s_dt.values()) / len(pi)}\")\n",
    "    print(f\"Random Forest: {sum(accuracy_scores_s_rf.values()) / len(pi)}\")\n",
    "    print(f\"SVM: {sum(accuracy_scores_s_svm.values()) / len(pi)}\")\n",
    "    print(f\"LDA: {sum(accuracy_scores_s_lda.values()) / len(pi)}\")\n",
    "    print(f\"XG: {sum(accuracy_scores_s_xg.values()) / len(pi)}\")\n",
    "    print(f\"MLP: {sum(accuracy_scores_s_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average Balanced Accuracy Stimulus-label-----\")\n",
    "    print(f\"Logistic Regression: {sum(balance_acc_list_s_lr.values()) / len(pi)}\")\n",
    "    print(f\"Decision Tree: {sum(balance_acc_list_s_dt.values()) / len(pi)}\")\n",
    "    print(f\"Random Forest: {sum(balance_acc_list_s_rf.values()) / len(pi)}\")\n",
    "    print(f\"SVM: {sum(balance_acc_list_s_svm.values()) / len(pi)}\")\n",
    "    print(f\"LDA: {sum(balance_acc_list_s_lda.values()) / len(pi)}\")\n",
    "    print(f\"XG: {sum(balance_acc_list_s_xg.values()) / len(pi)}\")\n",
    "    print(f\"MLP: {sum(balance_acc_list_s_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average R2 Loss Stimulus-label-----\")\n",
    "    print(f\"Logistic Regression: {sum(r2_s_lr.values()) / len(pi)}\")\n",
    "    print(f\"Decision Tree: {sum(r2_s_dt.values()) / len(pi)}\")\n",
    "    print(f\"Random Forest: {sum(r2_s_rf.values()) / len(pi)}\")\n",
    "    print(f\"SVM: {sum(r2_s_svm.values()) / len(pi)}\")\n",
    "    print(f\"LDA: {sum(r2_s_lda.values()) / len(pi)}\")\n",
    "    print(f\"XG: {sum(r2_s_xg.values()) / len(pi)}\")\n",
    "    print(f\"MLP: {sum(r2_s_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average MSE Loss Stimulus-label-----\")\n",
    "    print(f\"Logistic Regression: {sum(mse_s_lr.values()) / len(pi)}\")\n",
    "    print(f\"Decision Tree: {sum(mse_s_dt.values()) / len(pi)}\")\n",
    "    print(f\"Random Forest: {sum(mse_s_rf.values()) / len(pi)}\")\n",
    "    print(f\"SVM: {sum(mse_s_svm.values()) / len(pi)}\")\n",
    "    print(f\"LDA: {sum(mse_s_lda.values()) / len(pi)}\")\n",
    "    print(f\"XG: {sum(mse_s_xg.values()) / len(pi)}\")\n",
    "    print(f\"MLP: {sum(mse_s_mlp.values()) / len(pi)}\")\n",
    "\n",
    "    print(f\"-----Average F1 Stimulus-label-----\")\n",
    "    print(f\"Logistic Regression: {sum(f1_s_lr.values()) / len(pi)}\")\n",
    "    print(f\"Decision Tree: {sum(f1_s_dt.values()) / len(pi)}\")\n",
    "    print(f\"Random Forest: {sum(f1_s_rf.values()) / len(pi)}\")\n",
    "    print(f\"SVM: {sum(f1_s_svm.values()) / len(pi)}\")\n",
    "    print(f\"LDA: {sum(f1_s_lda.values()) / len(pi)}\")\n",
    "    print(f\"XG: {sum(f1_s_xg.values()) / len(pi)}\")\n",
    "    print(f\"MLP: {sum(f1_s_mlp.values()) / len(pi)}\")\n",
    "\n",
    "\n",
    "    return accuracy_scores_a_rf, accuracy_scores_v_rf, f1_a_rf,  f1_v_rf,  accuracy_scores_s_rf, f1_s_rf, accuracy_scores_a_xg, accuracy_scores_v_xg, f1_a_xg,  f1_v_xg,  accuracy_scores_s_xg, f1_s_xg, num_zeros_xg, num_zeros_rf, num_ones_rf, num_ones_xg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_path = \"../Data_files/EDA_labels.csv\"\n",
    "eda_df = csv_read(eda_path)\n",
    "relevant_features = ['ku_eda','sk_eda','dynrange','slope','variance','entropy','insc','fd_mean','max_scr','min_scr','nSCR','meanAmpSCR','meanRespSCR','sumAmpSCR','sumRespSCR']\n",
    "identifiers = ['Participant ID','Video ID','Gender','arousal_category','valence_category','taskwiselabel','three_class_label']\n",
    "eda_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_df, pi, vi = feature_analysis(eda_df,relevant_features,identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(eda_df, pi, relevant_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balancing - 333 samples acc to Arousal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "filtered_rows = []\n",
    "for index, row in tqdm(eda_df.iterrows()):\n",
    "    if row['arousal_category'] == 1:\n",
    "        filtered_rows.append(row)\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_rows)\n",
    "balanced_eda_df = pd.concat([eda_df, filtered_df], ignore_index=True)\n",
    "balanced_eda_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_df, pi, vi = feature_analysis(eda_df,relevant_features,identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(eda_df, pi, relevant_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppg_path = \"../Data_files/PPG_labels.csv\"\n",
    "ppg_df = csv_read(ppg_path)\n",
    "relevant_features = ['BPM','IBI','PPG_Rate_Mean','HRV_MedianNN','HRV_Prc20NN','HRV_MinNN','HRV_HTI','HRV_TINN','HRV_LF','HRV_VHF','HRV_LFn','HRV_HFn','HRV_LnHF','HRV_SD1SD2','HRV_CVI','HRV_PSS','HRV_PAS','HRV_PI','HRV_C1d','HRV_C1a','HRV_DFA_alpha1','HRV_MFDFA_alpha1_Width','HRV_MFDFA_alpha1_Peak','HRV_MFDFA_alpha1_Mean','HRV_MFDFA_alpha1_Max','HRV_MFDFA_alpha1_Delta','HRV_MFDFA_alpha1_Asymmetry','HRV_ApEn','HRV_ShanEn','HRV_FuzzyEn','HRV_MSEn','HRV_CMSEn','HRV_RCMSEn','HRV_CD','HRV_HFD','HRV_KFD','HRV_LZC']\n",
    "identifiers = ['Participant ID','Video ID','Gender','arousal_category','valence_category','taskwiselabel','three_class_label']\n",
    "ppg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppg_df, pi, vi = feature_analysis(ppg_df,relevant_features,identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(ppg_df, pi, relevant_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced Arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "filtered_rows = []\n",
    "for index, row in tqdm(ppg_df.iterrows()):\n",
    "    if row['arousal_category'] == 1:\n",
    "        filtered_rows.append(row)\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_rows)\n",
    "balanced_ppg_df = pd.concat([ppg_df, filtered_df], ignore_index=True)\n",
    "balanced_ppg_df    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_ppg_df, pi, vi = feature_analysis(balanced_ppg_df,relevant_features,identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(balanced_ppg_df, pi, relevant_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA + PPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_path = \"../Data_files/EDA_labels.csv\"\n",
    "eda_df = csv_read(eda_path)\n",
    "relevant_features = ['ku_eda','sk_eda','dynrange','slope','variance','entropy','insc','fd_mean','max_scr','min_scr','nSCR','meanAmpSCR','meanRespSCR','sumAmpSCR','sumRespSCR']\n",
    "identifiers = ['Participant ID','Video ID','Gender','arousal_category','valence_category','taskwiselabel','three_class_label']\n",
    "ppg_path = \"../Data_files/PPG_labels.csv\"\n",
    "ppg_df = csv_read(ppg_path)\n",
    "relevant_features = ['BPM','IBI','PPG_Rate_Mean','HRV_MedianNN','HRV_Prc20NN','HRV_MinNN','HRV_HTI','HRV_TINN','HRV_LF','HRV_VHF','HRV_LFn','HRV_HFn','HRV_LnHF','HRV_SD1SD2','HRV_CVI','HRV_PSS','HRV_PAS','HRV_PI','HRV_C1d','HRV_C1a','HRV_DFA_alpha1','HRV_MFDFA_alpha1_Width','HRV_MFDFA_alpha1_Peak','HRV_MFDFA_alpha1_Mean','HRV_MFDFA_alpha1_Max','HRV_MFDFA_alpha1_Delta','HRV_MFDFA_alpha1_Asymmetry','HRV_ApEn','HRV_ShanEn','HRV_FuzzyEn','HRV_MSEn','HRV_CMSEn','HRV_RCMSEn','HRV_CD','HRV_HFD','HRV_KFD','HRV_LZC']\n",
    "identifiers = ['Participant ID','Video ID','Gender','arousal_category','valence_category','taskwiselabel','three_class_label']\n",
    "combined_df = pd.concat([ppg_df, eda_df], axis=1)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_features = ['BPM', 'IBI', 'PPG_Rate_Mean', 'HRV_MedianNN', 'HRV_Prc20NN',\n",
    "       'HRV_MinNN', 'HRV_HTI', 'HRV_TINN', 'HRV_LF', 'HRV_VHF', 'HRV_LFn',\n",
    "       'HRV_HFn', 'HRV_LnHF', 'HRV_SD1SD2', 'HRV_CVI', 'HRV_PSS', 'HRV_PAS',\n",
    "       'HRV_PI', 'HRV_C1d', 'HRV_C1a', 'HRV_DFA_alpha1',\n",
    "       'HRV_MFDFA_alpha1_Width', 'HRV_MFDFA_alpha1_Peak',\n",
    "       'HRV_MFDFA_alpha1_Mean', 'HRV_MFDFA_alpha1_Max',\n",
    "       'HRV_MFDFA_alpha1_Delta', 'HRV_MFDFA_alpha1_Asymmetry', 'HRV_ApEn',\n",
    "       'HRV_ShanEn', 'HRV_FuzzyEn', 'HRV_MSEn', 'HRV_CMSEn', 'HRV_RCMSEn',\n",
    "       'HRV_CD', 'HRV_HFD', 'HRV_KFD', 'HRV_LZC', 'ku_eda', 'sk_eda', 'dynrange', 'slope',\n",
    "       'variance', 'entropy', 'insc', 'fd_mean', 'max_scr', 'min_scr', 'nSCR',\n",
    "       'meanAmpSCR', 'meanRespSCR', 'sumAmpSCR', 'sumRespSCR']\n",
    "\n",
    "combined_df, pi, vi = feature_analysis(combined_df,relevant_features,identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(combined_df, pi, relevant_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arousal balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "filtered_rows = []\n",
    "for index, row in tqdm(combined_df.iterrows()):\n",
    "    if row['arousal_category'] == 1:\n",
    "        filtered_rows.append(row)\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_rows)\n",
    "combined_b_df = pd.concat([combined_df, filtered_df], ignore_index=True)\n",
    "combined_b_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_features = ['BPM', 'IBI', 'PPG_Rate_Mean', 'HRV_MedianNN', 'HRV_Prc20NN',\n",
    "       'HRV_MinNN', 'HRV_HTI', 'HRV_TINN', 'HRV_LF', 'HRV_VHF', 'HRV_LFn',\n",
    "       'HRV_HFn', 'HRV_LnHF', 'HRV_SD1SD2', 'HRV_CVI', 'HRV_PSS', 'HRV_PAS',\n",
    "       'HRV_PI', 'HRV_C1d', 'HRV_C1a', 'HRV_DFA_alpha1',\n",
    "       'HRV_MFDFA_alpha1_Width', 'HRV_MFDFA_alpha1_Peak',\n",
    "       'HRV_MFDFA_alpha1_Mean', 'HRV_MFDFA_alpha1_Max',\n",
    "       'HRV_MFDFA_alpha1_Delta', 'HRV_MFDFA_alpha1_Asymmetry', 'HRV_ApEn',\n",
    "       'HRV_ShanEn', 'HRV_FuzzyEn', 'HRV_MSEn', 'HRV_CMSEn', 'HRV_RCMSEn',\n",
    "       'HRV_CD', 'HRV_HFD', 'HRV_KFD', 'HRV_LZC', 'ku_eda', 'sk_eda', 'dynrange', 'slope',\n",
    "       'variance', 'entropy', 'insc', 'fd_mean', 'max_scr', 'min_scr', 'nSCR',\n",
    "       'meanAmpSCR', 'meanRespSCR', 'sumAmpSCR', 'sumRespSCR']\n",
    "\n",
    "combined_df, pi, vi = feature_analysis(combined_df,relevant_features,identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(combined_df, pi, relevant_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_path = \"../Data_files/EDA_labels.csv\"\n",
    "eda_df = pd.read_csv(eda_path)\n",
    "eda_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balancing Arousal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "filtered_rows = []\n",
    "for index, row in tqdm(eda_df.iterrows()):\n",
    "    if row['arousal_category'] == 1:\n",
    "        filtered_rows.append(row)\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_rows)\n",
    "balance_a_eda_df = pd.concat([eda_df, filtered_df], ignore_index=True)\n",
    "balance_a_eda_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_preprocess(train_df, test_df):\n",
    "    train = []\n",
    "    test = []\n",
    "    for index, row in train_df.iterrows():\n",
    "        # Convert each row (Series) into a list of NumPy arrays\n",
    "        row_as_list1 = np.array([np.array(value) for value in row.to_numpy()])\n",
    "        train.append(row_as_list1)\n",
    "\n",
    "    for index, row in test_df.iterrows():\n",
    "        # Convert each row (Series) into a list of NumPy arrays\n",
    "        row_as_list2 = np.array([np.array(value) for value in row.to_numpy()])\n",
    "        test.append(row_as_list2)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom dataset and DataLoader (assuming you have training_data)\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.from_numpy(self.data[index]).float(), self.targets[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDA_HC_NN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(EDA_HC_NN, self).__init__()\n",
    "\n",
    "        # Define hidden layer dimensions\n",
    "        hidden_dims = [50, 100]\n",
    "\n",
    "        # Create sequential layers using nn.Linear and nn.ReLU activations\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.ReLU(inplace=True),  # Efficient in-place activation\n",
    "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Linear(hidden_dims[1], hidden_dims[2]),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dims[1], output_dim)  # Final output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(autofet_df_con, col, input_dim, output_dim):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batch_size = 32\n",
    "    epochs = 200\n",
    "    \n",
    "    learning_rate = 0.001  # Default learning rate\n",
    "    beta1 = 0.9  # Default beta1 for Adam in scikit-learn\n",
    "    beta2 = 0.999  # Default beta2 for Adam in scikit-learn\n",
    "    epsilon = 1e-8  # Default epsilon for Adam\n",
    "\n",
    "    total_test_accuracy = 0.0\n",
    "    total_test_f1 = 0.0\n",
    "    total_participants = 0\n",
    "    \n",
    "    for pid in autofet_df_con['Participant ID'].unique():\n",
    "        print(pid)\n",
    "\n",
    "        fet = ['ku_eda', 'sk_eda', 'dynrange', 'slope', 'variance',\n",
    "       'entropy', 'insc', 'fd_mean', 'max_scr', 'min_scr', 'nSCR',\n",
    "       'meanAmpSCR', 'meanRespSCR', 'sumAmpSCR', 'sumRespSCR']\n",
    "\n",
    "        train_data = autofet_df_con[autofet_df_con['Participant ID'] != pid]\n",
    "        test_data = autofet_df_con[autofet_df_con['Participant ID'] == pid]\n",
    "\n",
    "        # print(train_data, test_data)\n",
    "        X_train = train_data[fet]\n",
    "        X_test = test_data[fet]\n",
    "\n",
    "        X_train, X_test = windowed_preprocess(X_train, X_test)\n",
    "        # print(training_data)\n",
    "        train_y = train_data[col].to_list()\n",
    "        test_y =  test_data[col].to_list()\n",
    "\n",
    "        # Define custom dataset and DataLoader\n",
    "        custom_dataset = CustomDataset(X_train, train_y)\n",
    "        train_dataloader = DataLoader(custom_dataset, batch_size=batch_size) #, collate_fn=pad_collate) #shuffle=True, \n",
    "\n",
    "        # Initialize model\n",
    "        model = EDA_HC_NN(input_dim, output_dim).to(device)\n",
    "\n",
    "        # Loss function\n",
    "        # criterion = nn.BCELoss()\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2), eps=epsilon)\n",
    "\n",
    "        # Metrics\n",
    "        accuracy = torchmetrics.Accuracy(task='binary')  # Specify task='binary'\n",
    "        f1 = F1Score(task='binary')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_accuracy = torchmetrics.Accuracy(task='binary').to(device)  # Reinitialize for each epoch\n",
    "            running_f1 = torchmetrics.F1Score(task='binary').to(device)  # Reinitialize for each epoch\n",
    "        \n",
    "            for data, target in tqdm(train_dataloader):\n",
    "\n",
    "                data, target = data.to(device), target.to(device)  # Move data and target to GPU\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                output = model(data)\n",
    "\n",
    "                target = target.unsqueeze(1).float()\n",
    "                # print(target.size())\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update metrics\n",
    "                # print(f\"loss {loss}\")\n",
    "                running_loss += loss.item()\n",
    "                predicted_classes = torch.sigmoid(output) > 0.5\n",
    "                \n",
    "                running_accuracy.update(predicted_classes, target)\n",
    "                running_f1.update(predicted_classes, target)\n",
    "\n",
    "            # Compute average metrics\n",
    "            avg_loss = running_loss / len(train_dataloader)\n",
    "            avg_accuracy = running_accuracy.compute()\n",
    "            avg_f1 = running_f1.compute()\n",
    "            \n",
    "            # Print and log statistics\n",
    "            # print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss}, Accuracy: {avg_accuracy}, F1 Score: {avg_f1}\")\n",
    "\n",
    "        #---------eval------------------  \n",
    "            \n",
    "        # Define custom dataset and DataLoader\n",
    "        custom_dataset = CustomDataset(X_test, test_y)\n",
    "        test_dataloader = DataLoader(custom_dataset, batch_size=batch_size) #, collate_fn=pad_collate)#shuffle=True, \n",
    "\n",
    "        # Metrics\n",
    "        test_accuracy = torchmetrics.Accuracy(task='binary').to(device)  # Specify task='binary'\n",
    "        test_f1 = torchmetrics.F1Score(task='binary').to(device)\n",
    "\n",
    "        pred = []\n",
    "        true = []\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation during testing\n",
    "            for test_data, test_target in tqdm(test_dataloader):\n",
    "                test_data, test_target = test_data.to(device), test_target.to(device)  # Move data and target to GPU\n",
    "                \n",
    "                # if len(test_data.shape) == 2:\n",
    "                #     test_data = test_data.unsqueeze(1) \n",
    "                # print(test_data.size())    \n",
    "\n",
    "                test_output = model(test_data)\n",
    "                \n",
    "                # Convert target to FloatTensor\n",
    "                test_target = test_target.unsqueeze(1).float()\n",
    "                # print(test_target.size())\n",
    "            \n",
    "                test_predicted_classes = torch.sigmoid(test_output) > 0.5\n",
    "                pred.extend(test_predicted_classes)\n",
    "                true.extend(test_target)\n",
    "                test_accuracy.update(test_predicted_classes, test_target)\n",
    "                test_f1.update(test_predicted_classes, test_target)\n",
    "\n",
    "        # Compute average metrics\n",
    "        test_avg_accuracy = test_accuracy.compute()\n",
    "        test_avg_f1 = test_f1.compute()\n",
    "\n",
    "        true = [int(tensor.item()) for tensor in true]\n",
    "        pred = [int(tensor.item()) for tensor in pred]  \n",
    "        print(f\"True: {true}\")\n",
    "        print(f\"Pred: {pred}\")\n",
    "        # print(len(true), len(pred))\n",
    "        print(classification_report(true, pred, labels=[0,1]))\n",
    "\n",
    "        # Print or log results\n",
    "        print(f\"Test Accuracy: {test_avg_accuracy}, Test F1 Score: {test_avg_f1}\")\n",
    "        # Accumulate metrics across all participants\n",
    "        total_test_accuracy += test_avg_accuracy\n",
    "        total_test_f1 += test_avg_f1\n",
    "        total_participants += 1\n",
    "\n",
    "    # Calculate average metrics across all participants\n",
    "    avg_test_accuracy = total_test_accuracy / total_participants\n",
    "    avg_test_f1 = total_test_f1 / total_participants\n",
    "\n",
    "    # Print or log average results\n",
    "    print(f\"{col}: {seed}: Average Test Accuracy: {avg_test_accuracy}, Average Test F1 Score: {avg_test_f1}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim =15\n",
    "output_dim = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(eda_df, 'valence_category', input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(eda_df, 'arousal_category', input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(eda_df, 'taskwiselabel', input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of Balanced Arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(balance_a_eda_df, 'valence_category', input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(balance_a_eda_df, 'arousal_category', input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(balance_a_eda_df, 'taskwiselabel', input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppg_path = \"../Data_files/PPG_labels.csv\"\n",
    "ppg_df = pd.read_csv(ppg_path)\n",
    "ppg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fet = ['BPM', 'IBI', 'PPG_Rate_Mean', 'HRV_MedianNN',\n",
    "       'HRV_Prc20NN', 'HRV_MinNN', 'HRV_HTI', 'HRV_TINN', 'HRV_LF', 'HRV_VHF',\n",
    "       'HRV_LFn', 'HRV_HFn', 'HRV_LnHF', 'HRV_SD1SD2', 'HRV_CVI', 'HRV_PSS',\n",
    "       'HRV_PAS', 'HRV_PI', 'HRV_C1d', 'HRV_C1a', 'HRV_DFA_alpha1',\n",
    "       'HRV_MFDFA_alpha1_Width', 'HRV_MFDFA_alpha1_Peak',\n",
    "       'HRV_MFDFA_alpha1_Mean', 'HRV_MFDFA_alpha1_Max',\n",
    "       'HRV_MFDFA_alpha1_Delta', 'HRV_MFDFA_alpha1_Asymmetry', 'HRV_ApEn',\n",
    "       'HRV_ShanEn', 'HRV_FuzzyEn', 'HRV_MSEn', 'HRV_CMSEn', 'HRV_RCMSEn',\n",
    "       'HRV_CD', 'HRV_HFD', 'HRV_KFD', 'HRV_LZC','ku_eda',\n",
    "       'sk_eda', 'dynrange', 'slope', 'variance', 'entropy', 'insc', 'fd_mean',\n",
    "       'max_scr', 'min_scr', 'nSCR', 'meanAmpSCR', 'meanRespSCR', 'sumAmpSCR',\n",
    "       'sumRespSCR'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced Arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "filtered_rows = []\n",
    "for index, row in tqdm(combined_df.iterrows()):\n",
    "    if row['arousal_category'] == 1:\n",
    "        filtered_rows.append(row)\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_rows)\n",
    "balanced_a_ppg_df = pd.concat([combined_df, filtered_df], ignore_index=True)\n",
    "balanced_a_ppg_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPG_HC_NN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PPG_HC_NN, self).__init__()\n",
    "\n",
    "        # Define hidden layer dimensions\n",
    "        hidden_dims = [50, 100]\n",
    "\n",
    "        # Create sequential layers using nn.Linear and nn.ReLU activations\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.ReLU(inplace=True),  # Efficient in-place activation\n",
    "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Linear(hidden_dims[1], hidden_dims[2]),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dims[1], output_dim)  # Final output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(autofet_df_con, col, input_dim, output_dim):\n",
    "\n",
    "    batch_size = 32\n",
    "    epochs = 200\n",
    "    \n",
    "    learning_rate = 0.001  # Default learning rate\n",
    "    beta1 = 0.9  # Default beta1 for Adam in scikit-learn\n",
    "    beta2 = 0.999  # Default beta2 for Adam in scikit-learn\n",
    "    epsilon = 1e-8  # Default epsilon for Adam\n",
    "\n",
    "    total_test_accuracy = 0.0\n",
    "    total_test_f1 = 0.0\n",
    "    total_participants = 0\n",
    "    \n",
    "    for pid in autofet_df_con['Participant ID'].unique():\n",
    "        print(pid)\n",
    "\n",
    "        fet = ['BPM', 'IBI', 'PPG_Rate_Mean', 'HRV_MedianNN',\n",
    "       'HRV_Prc20NN', 'HRV_MinNN', 'HRV_HTI', 'HRV_TINN', 'HRV_LF', 'HRV_VHF',\n",
    "       'HRV_LFn', 'HRV_HFn', 'HRV_LnHF', 'HRV_SD1SD2', 'HRV_CVI', 'HRV_PSS',\n",
    "       'HRV_PAS', 'HRV_PI', 'HRV_C1d', 'HRV_C1a', 'HRV_DFA_alpha1',\n",
    "       'HRV_MFDFA_alpha1_Width', 'HRV_MFDFA_alpha1_Peak',\n",
    "       'HRV_MFDFA_alpha1_Mean', 'HRV_MFDFA_alpha1_Max',\n",
    "       'HRV_MFDFA_alpha1_Delta', 'HRV_MFDFA_alpha1_Asymmetry', 'HRV_ApEn',\n",
    "       'HRV_ShanEn', 'HRV_FuzzyEn', 'HRV_MSEn', 'HRV_CMSEn', 'HRV_RCMSEn',\n",
    "       'HRV_CD', 'HRV_HFD', 'HRV_KFD', 'HRV_LZC']\n",
    "\n",
    "        train_data = autofet_df_con[autofet_df_con['Participant ID'] != pid]\n",
    "        test_data = autofet_df_con[autofet_df_con['Participant ID'] == pid]\n",
    "\n",
    "        # print(train_data, test_data)\n",
    "        X_train = train_data[fet]\n",
    "        X_test = test_data[fet]\n",
    "\n",
    "        X_train, X_test = windowed_preprocess(X_train, X_test)\n",
    "        # print(training_data)\n",
    "        train_y = train_data[col].to_list()\n",
    "        test_y =  test_data[col].to_list()\n",
    "\n",
    "        # Define custom dataset and DataLoader\n",
    "        custom_dataset = CustomDataset(X_train, train_y)\n",
    "        train_dataloader = DataLoader(custom_dataset, batch_size=batch_size) #, collate_fn=pad_collate) #shuffle=True, \n",
    "\n",
    "        # Initialize model\n",
    "        model = PPG_HC_NN(input_dim, output_dim).to(device)\n",
    "\n",
    "        # Loss function\n",
    "        # criterion = nn.BCELoss()\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2), eps=epsilon)\n",
    "\n",
    "        # Metrics\n",
    "        accuracy = torchmetrics.Accuracy(task='binary')  # Specify task='binary'\n",
    "        f1 = F1Score(task='binary')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_accuracy = torchmetrics.Accuracy(task='binary').to(device)  # Reinitialize for each epoch\n",
    "            running_f1 = torchmetrics.F1Score(task='binary').to(device)  # Reinitialize for each epoch\n",
    "        \n",
    "            for data, target in tqdm(train_dataloader):\n",
    "\n",
    "                data, target = data.to(device), target.to(device)  # Move data and target to GPU\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                output = model(data)\n",
    "\n",
    "                target = target.unsqueeze(1).float()\n",
    "                # print(target.size())\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update metrics\n",
    "                # print(f\"loss {loss}\")\n",
    "                running_loss += loss.item()\n",
    "                predicted_classes = torch.sigmoid(output) > 0.5\n",
    "                \n",
    "                running_accuracy.update(predicted_classes, target)\n",
    "                running_f1.update(predicted_classes, target)\n",
    "\n",
    "            # Compute average metrics\n",
    "            avg_loss = running_loss / len(train_dataloader)\n",
    "            avg_accuracy = running_accuracy.compute()\n",
    "            avg_f1 = running_f1.compute()\n",
    "            \n",
    "            # Print and log statistics\n",
    "            # print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss}, Accuracy: {avg_accuracy}, F1 Score: {avg_f1}\")\n",
    "\n",
    "        #---------eval------------------  \n",
    "            \n",
    "        # Define custom dataset and DataLoader\n",
    "        custom_dataset = CustomDataset(X_test, test_y)\n",
    "        test_dataloader = DataLoader(custom_dataset, batch_size=batch_size) #, collate_fn=pad_collate)#shuffle=True, \n",
    "\n",
    "        # Metrics\n",
    "        test_accuracy = torchmetrics.Accuracy(task='binary').to(device)  # Specify task='binary'\n",
    "        test_f1 = torchmetrics.F1Score(task='binary').to(device)\n",
    "\n",
    "        pred = []\n",
    "        true = []\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation during testing\n",
    "            for test_data, test_target in tqdm(test_dataloader):\n",
    "                test_data, test_target = test_data.to(device), test_target.to(device)  # Move data and target to GPU\n",
    "                \n",
    "                # if len(test_data.shape) == 2:\n",
    "                #     test_data = test_data.unsqueeze(1) \n",
    "                # print(test_data.size())    \n",
    "\n",
    "                test_output = model(test_data)\n",
    "                \n",
    "                # Convert target to FloatTensor\n",
    "                test_target = test_target.unsqueeze(1).float()\n",
    "                # print(test_target.size())\n",
    "            \n",
    "                test_predicted_classes = torch.sigmoid(test_output) > 0.5\n",
    "                pred.extend(test_predicted_classes)\n",
    "                true.extend(test_target)\n",
    "                test_accuracy.update(test_predicted_classes, test_target)\n",
    "                test_f1.update(test_predicted_classes, test_target)\n",
    "\n",
    "        # Compute average metrics\n",
    "        test_avg_accuracy = test_accuracy.compute()\n",
    "        test_avg_f1 = test_f1.compute()\n",
    "\n",
    "        true = [int(tensor.item()) for tensor in true]\n",
    "        pred = [int(tensor.item()) for tensor in pred]  \n",
    "        print(f\"True: {true}\")\n",
    "        print(f\"Pred: {pred}\")\n",
    "        # print(len(true), len(pred))\n",
    "        print(classification_report(true, pred, labels=[0,1]))\n",
    "\n",
    "        # Print or log results\n",
    "        print(f\"Test Accuracy: {test_avg_accuracy}, Test F1 Score: {test_avg_f1}\")\n",
    "        # Accumulate metrics across all participants\n",
    "        total_test_accuracy += test_avg_accuracy\n",
    "        total_test_f1 += test_avg_f1\n",
    "        total_participants += 1\n",
    "\n",
    "    # Calculate average metrics across all participants\n",
    "    avg_test_accuracy = total_test_accuracy / total_participants\n",
    "    avg_test_f1 = total_test_f1 / total_participants\n",
    "\n",
    "    # Print or log average results\n",
    "    print(f\"{col}: seed {seed}: Average Test Accuracy: {avg_test_accuracy}, Average Test F1 Score: {avg_test_f1}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(ppg_df, 'taskwiselabel', input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(ppg_df, 'arousal_category', input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(ppg_df, 'valence_category', input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(balanced_a_ppg_df, 'taskwiselabel', input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(balanced_a_ppg_df, 'arousal_category', input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(balanced_a_ppg_df, 'valence_category', input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA + PPG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([ppg_df, eda_df], axis=1)\n",
    "combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combined_HC_NN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Combined_HC_NN, self).__init__()\n",
    "\n",
    "        # Define hidden layer dimensions\n",
    "        hidden_dims = [50, 100]\n",
    "\n",
    "        # Create sequential layers using nn.Linear and nn.ReLU activations\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.ReLU(inplace=True),  # Efficient in-place activation\n",
    "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Linear(hidden_dims[1], hidden_dims[2]),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dims[1], output_dim)  # Final output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(autofet_df_con, col, input_dim, output_dim):\n",
    "\n",
    "    batch_size = 32\n",
    "    epochs = 200\n",
    "    \n",
    "    learning_rate = 0.001  # Default learning rate\n",
    "    beta1 = 0.9  # Default beta1 for Adam in scikit-learn\n",
    "    beta2 = 0.999  # Default beta2 for Adam in scikit-learn\n",
    "    epsilon = 1e-8  # Default epsilon for Adam\n",
    "\n",
    "    total_test_accuracy = 0.0\n",
    "    total_test_f1 = 0.0\n",
    "    total_participants = 0\n",
    "    \n",
    "    for pid in autofet_df_con['Participant ID'].unique():\n",
    "        print(pid)\n",
    "\n",
    "        fet = ['BPM', 'IBI', 'PPG_Rate_Mean', 'HRV_MedianNN',\n",
    "       'HRV_Prc20NN', 'HRV_MinNN', 'HRV_HTI', 'HRV_TINN', 'HRV_LF', 'HRV_VHF',\n",
    "       'HRV_LFn', 'HRV_HFn', 'HRV_LnHF', 'HRV_SD1SD2', 'HRV_CVI', 'HRV_PSS',\n",
    "       'HRV_PAS', 'HRV_PI', 'HRV_C1d', 'HRV_C1a', 'HRV_DFA_alpha1',\n",
    "       'HRV_MFDFA_alpha1_Width', 'HRV_MFDFA_alpha1_Peak',\n",
    "       'HRV_MFDFA_alpha1_Mean', 'HRV_MFDFA_alpha1_Max',\n",
    "       'HRV_MFDFA_alpha1_Delta', 'HRV_MFDFA_alpha1_Asymmetry', 'HRV_ApEn',\n",
    "       'HRV_ShanEn', 'HRV_FuzzyEn', 'HRV_MSEn', 'HRV_CMSEn', 'HRV_RCMSEn',\n",
    "       'HRV_CD', 'HRV_HFD', 'HRV_KFD', 'HRV_LZC','ku_eda',\n",
    "       'sk_eda', 'dynrange', 'slope', 'variance', 'entropy', 'insc', 'fd_mean',\n",
    "       'max_scr', 'min_scr', 'nSCR', 'meanAmpSCR', 'meanRespSCR', 'sumAmpSCR',\n",
    "       'sumRespSCR'] \n",
    "\n",
    "        train_data = autofet_df_con[autofet_df_con['Participant ID'] != pid]\n",
    "        test_data = autofet_df_con[autofet_df_con['Participant ID'] == pid]\n",
    "\n",
    "        # print(train_data, test_data)\n",
    "        X_train = train_data[fet]\n",
    "        X_test = test_data[fet]\n",
    "\n",
    "        X_train, X_test = windowed_preprocess(X_train, X_test)\n",
    "        # print(training_data)\n",
    "        train_y = train_data[col].to_list()\n",
    "        test_y =  test_data[col].to_list()\n",
    "\n",
    "        # Define custom dataset and DataLoader\n",
    "        custom_dataset = CustomDataset(X_train, train_y)\n",
    "        train_dataloader = DataLoader(custom_dataset, batch_size=batch_size) #, collate_fn=pad_collate) #shuffle=True, \n",
    "\n",
    "        # Initialize model\n",
    "        model = Combined_HC_NN(input_dim, output_dim).to(device)\n",
    "\n",
    "        # Loss function\n",
    "        # criterion = nn.BCELoss()\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2), eps=epsilon)\n",
    "\n",
    "        # Metrics\n",
    "        accuracy = torchmetrics.Accuracy(task='binary')  # Specify task='binary'\n",
    "        f1 = F1Score(task='binary')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_accuracy = torchmetrics.Accuracy(task='binary').to(device)  # Reinitialize for each epoch\n",
    "            running_f1 = torchmetrics.F1Score(task='binary').to(device)  # Reinitialize for each epoch\n",
    "        \n",
    "            for data, target in tqdm(train_dataloader):\n",
    "\n",
    "                data, target = data.to(device), target.to(device)  # Move data and target to GPU\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                output = model(data)\n",
    "\n",
    "                target = target.unsqueeze(1).float()\n",
    "                # print(target.size())\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update metrics\n",
    "                # print(f\"loss {loss}\")\n",
    "                running_loss += loss.item()\n",
    "                predicted_classes = torch.sigmoid(output) > 0.5\n",
    "                \n",
    "                running_accuracy.update(predicted_classes, target)\n",
    "                running_f1.update(predicted_classes, target)\n",
    "\n",
    "            # Compute average metrics\n",
    "            avg_loss = running_loss / len(train_dataloader)\n",
    "            avg_accuracy = running_accuracy.compute()\n",
    "            avg_f1 = running_f1.compute()\n",
    "            \n",
    "            # Print and log statistics\n",
    "            # print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss}, Accuracy: {avg_accuracy}, F1 Score: {avg_f1}\")\n",
    "\n",
    "        #---------eval------------------  \n",
    "            \n",
    "        # Define custom dataset and DataLoader\n",
    "        custom_dataset = CustomDataset(X_test, test_y)\n",
    "        test_dataloader = DataLoader(custom_dataset, batch_size=batch_size) #, collate_fn=pad_collate)#shuffle=True, \n",
    "\n",
    "        # Metrics\n",
    "        test_accuracy = torchmetrics.Accuracy(task='binary').to(device)  # Specify task='binary'\n",
    "        test_f1 = torchmetrics.F1Score(task='binary').to(device)\n",
    "\n",
    "        pred = []\n",
    "        true = []\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation during testing\n",
    "            for test_data, test_target in tqdm(test_dataloader):\n",
    "                test_data, test_target = test_data.to(device), test_target.to(device)  # Move data and target to GPU\n",
    "                \n",
    "                # if len(test_data.shape) == 2:\n",
    "                #     test_data = test_data.unsqueeze(1) \n",
    "                # print(test_data.size())    \n",
    "\n",
    "                test_output = model(test_data)\n",
    "                \n",
    "                # Convert target to FloatTensor\n",
    "                test_target = test_target.unsqueeze(1).float()\n",
    "                # print(test_target.size())\n",
    "            \n",
    "                test_predicted_classes = torch.sigmoid(test_output) > 0.5\n",
    "                pred.extend(test_predicted_classes)\n",
    "                true.extend(test_target)\n",
    "                test_accuracy.update(test_predicted_classes, test_target)\n",
    "                test_f1.update(test_predicted_classes, test_target)\n",
    "\n",
    "        # Compute average metrics\n",
    "        test_avg_accuracy = test_accuracy.compute()\n",
    "        test_avg_f1 = test_f1.compute()\n",
    "\n",
    "        true = [int(tensor.item()) for tensor in true]\n",
    "        pred = [int(tensor.item()) for tensor in pred]  \n",
    "        print(f\"True: {true}\")\n",
    "        print(f\"Pred: {pred}\")\n",
    "        # print(len(true), len(pred))\n",
    "        print(classification_report(true, pred, labels=[0,1]))\n",
    "\n",
    "        # Print or log results\n",
    "        print(f\"Test Accuracy: {test_avg_accuracy}, Test F1 Score: {test_avg_f1}\")\n",
    "        # Accumulate metrics across all participants\n",
    "        total_test_accuracy += test_avg_accuracy\n",
    "        total_test_f1 += test_avg_f1\n",
    "        total_participants += 1\n",
    "\n",
    "    # Calculate average metrics across all participants\n",
    "    avg_test_accuracy = total_test_accuracy / total_participants\n",
    "    avg_test_f1 = total_test_f1 / total_participants\n",
    "\n",
    "    # Print or log average results\n",
    "    print(f\"{col}: seed {seed}: Average Test Accuracy: {avg_test_accuracy}, Average Test F1 Score: {avg_test_f1}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(combined_df, 'valence_category', input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(combined_df, 'arousal_category', input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(combined_df, 'taskwiselabel', input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced Arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([balance_a_eda_df, balance_a_eda_df], axis=1)\n",
    "combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(combined_df, 'valence_category', input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(combined_df, 'taskwiselabel', input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(combined_df, 'arousal_category', input_dim, output_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
