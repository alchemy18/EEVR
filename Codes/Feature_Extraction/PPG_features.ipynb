{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import neurokit2 as nk\n",
    "import cvxEDA.src.cvxEDA as cvxEDA\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy.stats as stats\n",
    "import scipy.signal as signal\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import trapz\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sampling_freq = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ppg = pd.read_csv(\"../Data_files/PPG.csv\")\n",
    "raw_ppg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = raw_ppg[raw_ppg['CMA'].isin(['HVLA', 'LVLA', 'LVHA', 'HVHA', 'Baseline'])]\n",
    "final_df['CMA'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Video_ID_number'] = [int(part.split('V')[-1]) for part in final_df['Video ID']]\n",
    "final_df['Video_ID_number']\n",
    "final_df.to_csv(\"Stimuli_PPG.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Participant ID'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_prep(merged_df):\n",
    "    vads = pd.read_csv(\"VADS.csv\") #getting label csv\n",
    "    print(\"reading vad\")\n",
    "\n",
    "    #adding arousal, valence and dominance columns to orginal data csv\n",
    "    for index, row in tqdm(merged_df.iterrows()):\n",
    "        matching_rows = vads[(vads['Participant ID'] == row['Participant ID']) & (vads['Video ID'] == row['Video_ID_number'])]\n",
    "\n",
    "        if not matching_rows.empty:\n",
    "\n",
    "            merged_df.at[index, 'Valence'] = matching_rows['Valence'].iloc[0]\n",
    "            merged_df.at[index, 'Arousal'] = matching_rows['Arousal'].iloc[0]\n",
    "            merged_df.at[index, 'Dominance'] = matching_rows['Dominance'].iloc[0]\n",
    "            merged_df.at[index, 'significance'] = matching_rows['significance'].iloc[0]\n",
    "\n",
    "    print(\"binning...\")\n",
    "    # Define the bins and labels for categorization\n",
    "    bins = [1, 3, 5]  # Define the bin edges\n",
    "    labels = [0, 1]   # Define the corresponding labels (0 (Low) for 1-3:, 1 (High) for 4-5)\n",
    "\n",
    "    # Use the cut function to categorize the 'arousal' column\n",
    "    merged_df['arousal_category'] = pd.cut(merged_df['Arousal'], bins=bins, labels=labels, include_lowest=True)\n",
    "    merged_df['valence_category'] = pd.cut(merged_df['Valence'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "    # Convert the 'category' column to integer type if needed\n",
    "    merged_df['arousal_category'] = merged_df['arousal_category'].astype(int)\n",
    "    merged_df['valence_category'] = merged_df['valence_category'].astype(int)\n",
    "\n",
    "    print(\"mapping\")\n",
    "    mapping = {\n",
    "    'Baseline': 0,\n",
    "    'LVLA': 0,\n",
    "    'LVHA': 0,\n",
    "    'HVHA': 1,\n",
    "    'HVLA': 1  # Baseline and HVLA mapped to 0\n",
    "    }\n",
    "\n",
    "    # Apply the mapping to the 'CMA' column\n",
    "    merged_df['taskwiselabel'] = merged_df['CMA'].map(mapping)\n",
    "    # autofet_df\n",
    "\n",
    "    three_class_mapping = {\n",
    "    'Baseline': 1,\n",
    "    'LVLA': 1,\n",
    "    'LVHA': 0,\n",
    "    'HVHA': 1,\n",
    "    'HVLA': 2  \n",
    "    }\n",
    "\n",
    "    merged_df['three_class_label'] = merged_df['CMA'].map(three_class_mapping)\n",
    "    return merged_df\n",
    "\n",
    "# ppg_data_with_labels = label_prep(final_df)\n",
    "# ppg_data_with_labels.to_csv(\"PPG_data_with_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = {}\n",
    "for pi in final_df['Participant ID'].unique():\n",
    "    df_pi = final_df[final_df['Participant ID'] == pi]\n",
    "    for vi in final_df['Video ID'].unique():\n",
    "        # print(pi,vi)\n",
    "        df_vi =  df_pi[df_pi['Video ID'] == vi]\n",
    "        tag = str(pi) + '_' + vi\n",
    "        df_list[tag] = (df_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppg_stat(array):\n",
    "    # Input: An array (numpy, dataframe, list) \n",
    "    # -> Output : A pd.DataFrame\n",
    "    \n",
    "    x = np.array(array)\n",
    "    statistics = {\n",
    "    'max_ppg': x.max(),\n",
    "    'min_ppg': x.min(),\n",
    "    'mean_ppg': x.mean(),\n",
    "    'sd_ppg': x.std(),\n",
    "    'ku_ppg': stats.kurtosis(x),\n",
    "    'sk_ppg': stats.skew(x),\n",
    "    'median_ppg': np.quantile(x, 0.5),\n",
    "    'q1_ppg': np.quantile(x, 0.25),\n",
    "    'q3_ppg': np.quantile(x, 0.75),\n",
    "    'q05_ppg': np.quantile(x, 0.05),\n",
    "    'q95_ppg': np.quantile(x, 0.95)}\n",
    "    \n",
    "    return statistics\n",
    "\n",
    "def ppg_peaks(array, sampling_rate=125):\n",
    "    # Input: An array (numpy, dataframe, list) \n",
    "    # -> Output : A numpy array\n",
    "    \n",
    "    x = np.array(array)\n",
    "    \n",
    "    # distance : minimal horizontal distance (>= 1) in samples between neighbouring peaks. By default = None\n",
    "    # height : required height of peaks. By default = None\n",
    "    \n",
    "    distance = sampling_rate / 3\n",
    "    #height = x.max() / 2\n",
    "    height = np.quantile(x, 0.99)/2 #use 99th quantile instead of max (to ignore outliers)\n",
    "    r_peaks, _ = signal.find_peaks(x, distance= distance, height=height)\n",
    "    \n",
    "    return r_peaks\n",
    "\n",
    "def ppg_time(array, sampling_rate=125):\n",
    "    # Input: An array (numpy, dataframe, list) \n",
    "    # -> Output : A pd.DataFrame\n",
    "    \n",
    "    x = np.array(array)\n",
    "    r_peaks = ppg_peaks(x, sampling_rate=sampling_rate)\n",
    "    \n",
    "    # RR intervals are expressed in number of samples and need to be converted into ms. By default, sampling_rate=1000\n",
    "    # HR is given by: 60/RR intervals in seconds. \n",
    "    \n",
    "    rri = np.diff(r_peaks) #RR intervals\n",
    "    rri = 1000 * rri / sampling_rate #Convert to ms\n",
    "    drri = np.diff(rri) #Difference between successive RR intervals\n",
    "    hr = 1000*60 / rri #Heart rate\n",
    "    \n",
    "    meanHR = hr.mean()\n",
    "    minHR = hr.min()\n",
    "    maxHR = hr.max()\n",
    "    sdHR = hr.std()\n",
    "    modeHR =  maxHR - minHR\n",
    "    nNN = rri.shape[0] / (x.shape[0]/sampling_rate/60) #to get the number of NN intervals per minute\n",
    "    meanNN = rri.mean()\n",
    "    SDSD = drri.std()\n",
    "    CVNN = rri.std()\n",
    "    SDNN = CVNN / meanNN\n",
    "    pNN50 = np.sum(np.abs(drri)>50) / nNN * 100\n",
    "    pNN20 = np.sum(np.abs(drri)>20) / nNN * 100\n",
    "    RMSSD = np.sqrt(np.mean(drri**2))\n",
    "    medianNN = np.quantile(rri, 0.5)\n",
    "    q20NN = np.quantile(rri, 0.2)\n",
    "    q80NN = np.quantile(rri, 0.8)\n",
    "    minNN = rri.min()\n",
    "    maxNN = rri.max()\n",
    "    \n",
    "    # HRV triagular index (HTI): The density distribution D of the RR intervals is estimated. The most frequent\n",
    "    # RR interval lenght X is established. Y= D(X) is the maximum of the sample density distribution.\n",
    "    # The HTI is then obtained as : (the number of total NN intervals)/Y\n",
    "\n",
    "    bins = np.arange(meanNN - 3*CVNN , meanNN + 3*CVNN, 10)\n",
    "    d,_ = np.histogram(rri, bins=bins)\n",
    "    y = d.argmax()\n",
    "    triHRV = nNN / y\n",
    "    \n",
    "    return {\n",
    "        'meanHR': meanHR,\n",
    "        'minHR': minHR,\n",
    "        'maxHR': maxHR,\n",
    "        'sdHR': sdHR,\n",
    "        'modeHR': modeHR,\n",
    "        'nNN': nNN,\n",
    "        'meanNN': meanNN,\n",
    "        'SDSD': SDSD,\n",
    "        'CVNN': CVNN,\n",
    "        'SDNN': SDNN,\n",
    "        'pNN50': pNN50,\n",
    "        'pNN20': pNN20,\n",
    "        'RMSSD': RMSSD,\n",
    "        'medianNN': medianNN,\n",
    "        'q20NN': q20NN,\n",
    "        'q80NN': q80NN,\n",
    "        'minNN': minNN,\n",
    "        'maxNN': maxNN,\n",
    "        'triHRV': triHRV\n",
    "    }\n",
    "\n",
    "def interpolate_Rpeaks(peaks, sampling_rate=125, upsample_rate=4):\n",
    "    # Input: An array (numpy, dataframe, list) \n",
    "    # -> Output : A numpy array \n",
    "    \n",
    "    # The RR intervals are aranged over time, and the values are summed up to find the time points.\n",
    "    # An interpolation function is defined, to use to sample from with any upsampling resolution. \n",
    "    # By default upsample_rate = 4 : 4 evenly spaced data points per seconds are added. \n",
    "    \n",
    "    rr = np.diff(peaks)\n",
    "    rr = 1000 * rr / sampling_rate # convert to ms\n",
    "    rr_time = np.cumsum(rr) / 1000 # convert to s\n",
    "    rr_time -= rr_time[0] \n",
    "    \n",
    "    interpolation_f = interp1d(rr_time, rr, kind='cubic')\n",
    "    \n",
    "    x = np.arange(1, rr_time.max(), 1/upsample_rate)\n",
    "    rr_interpolated = interpolation_f(x)\n",
    "    \n",
    "    return rr_interpolated, x\n",
    "\n",
    "def ppg_freq(array, sampling_rate=1000, upsample_rate=4, freqband_limits=(.0, .0033,.04,.15,.4, .5)):\n",
    "    # Input: An array (numpy, dataframe, list) \n",
    "    # -> Output : A numpy array \n",
    "    \n",
    "    # FFT needs evenly sampled data, so the RR-interval can't be used directly and need to\n",
    "    # be interpolated. Then the spectral density of the signal is computed using Welch method.\n",
    "    \n",
    "    x = np.array(array)\n",
    "    r_peaks = ppg_peaks(x, sampling_rate=sampling_rate)\n",
    "    rri, _ = interpolate_Rpeaks(r_peaks, upsample_rate=upsample_rate)\n",
    "    freq, power = signal.welch(x=rri, fs=upsample_rate)\n",
    "    \n",
    "    lim_ulf= (freq >= freqband_limits[0]) & (freq < freqband_limits[1])\n",
    "    lim_vlf = (freq >= freqband_limits[1]) & (freq < freqband_limits[2])\n",
    "    lim_lf = (freq >= freqband_limits[2]) & (freq < freqband_limits[3])\n",
    "    lim_hf = (freq >= freqband_limits[3]) & (freq < freqband_limits[4])\n",
    "    lim_vhf = (freq >= freqband_limits[4]) & (freq < freqband_limits[5])\n",
    "    \n",
    "    # The power (PSD) of each frequency band is obtained by integrating the spectral density \n",
    "    # by trapezoidal rule, using the scipy.integrate.trapz function.\n",
    "    \n",
    "    ulf = trapz(power[lim_ulf], freq[lim_ulf])\n",
    "    vlf = trapz(power[lim_vlf], freq[lim_vlf])\n",
    "    lf = trapz(power[lim_lf], freq[lim_lf])\n",
    "    hf = trapz(power[lim_hf], freq[lim_hf])\n",
    "    vhf = trapz(power[lim_vhf], freq[lim_vhf])\n",
    "    totalpower = ulf + vlf + lf + hf + vhf\n",
    "    lfhf = lf / hf\n",
    "    rlf = lf / (lf + hf) * 100\n",
    "    rhf = hf / (lf + hf) * 100\n",
    "    peaklf = freq[lim_lf][np.argmax(power[lim_lf])]\n",
    "    peakhf = freq[lim_hf][np.argmax(power[lim_hf])]\n",
    "    \n",
    "    return {\n",
    "    'totalpower': totalpower,\n",
    "    'LF': lf,\n",
    "    'HF': hf,\n",
    "    'ULF': ulf,\n",
    "    'VLF': vlf,\n",
    "    'VHF': vhf,\n",
    "    'LF/HF': lfhf,\n",
    "    'rLF': rlf,\n",
    "    'rHF': rhf,\n",
    "    'peakLF': peaklf,\n",
    "    'peakHF': peakhf\n",
    "}\n",
    "\n",
    "def apEntropy(array, m=2, r=None):\n",
    "    # Input: An array (numpy, dataframe, list) \n",
    "    # -> Output : A Float\n",
    "    \n",
    "    # m : a positive integer representing the length of each compared run of data (a window).\n",
    "    # By default m = 2\n",
    "    # r : a positive real number specifying a filtering level. By default r = 0.2 * sd.\n",
    "    \n",
    "    x = np.array(array)\n",
    "    N = len(x)\n",
    "    r = 0.2 * x.std() if r == None else r == r\n",
    "    \n",
    "    # A sequence of vectors z(1),..., z(N-m+1) is formed from a time series of N equally \n",
    "    # spaced raw data values x(1),…,x(N), such that z(i) = x(1),...,x(i+m-1).\n",
    "    \n",
    "    # For each i in {1,..., N-m+1}, C = [number of z(j) such that d(x(i),x(j)) < r]/[N-m+1]\n",
    "    # is computed, with d(z(i),z(j)) = max|x(i)-x(j)|\n",
    "    \n",
    "    # phi_m(r) = (N-m+1)^-1 x sum log(Ci) is computed, and the ap Entropy is given by: \n",
    "    # phi_m(r) - phi_m+1(r)\n",
    "\n",
    "    def _maxdist(zi, zj):\n",
    "        return max([abs(xi - xj) for xi, xj in zip(zi, zj)])\n",
    "    \n",
    "    def _phi(m):\n",
    "        z = [[x[j] for j in range(i, i + m - 1 + 1)] for i in range(N - m + 1)]\n",
    "        C = [len([1 for zj in z if _maxdist(zi, zj) <= r]) / (N - m + 1.0)\n",
    "        for zi in z]\n",
    "        return (N - m + 1.0) ** (-1) * sum(np.log(C))\n",
    "    \n",
    "    apEn = abs(_phi(m + 1) - _phi(m))\n",
    "    return apEn\n",
    "    \n",
    "    \n",
    "def sampEntropy(array, m=2, r=None):\n",
    "    # Input: An array (numpy, dataframe, list) \n",
    "    # -> Output : A Float\n",
    "    \n",
    "    # m: embedding dimension\n",
    "    # r: tolerance distance to consider two data points as similar. By default r = 0.2 * sd.\n",
    "    \n",
    "    # SampEn is the negative logarithm of the probability that if two sets of simultaneous \n",
    "    # data points of length m have distance < r then two sets of simultaneous data points of \n",
    "    # length m + 1 also have distance < r. \n",
    "    \n",
    "    x = np.array(array)\n",
    "    N = len(x)\n",
    "    r = 0.2 * x.std() if r == None else r == r\n",
    "    \n",
    "    # All templates vector of length m are defined. Distances d(xmi, xmj) are computed\n",
    "    # and all matches such that d < r are saved. Same for the distances d(xm+1i, xm+1j).\n",
    "    \n",
    "    xmi = np.array([x[i : i + m] for i in range(N - m)])\n",
    "    xmj = np.array([x[i : i + m] for i in range(N - m + 1)])\n",
    "    B = np.sum([np.sum(np.abs(xmii - xmj).max(axis=1) <= r) - 1 for xmii in xmi])\n",
    "    m += 1\n",
    "    xm = np.array([x[i : i + m] for i in range(N - m + 1)])\n",
    "    A = np.sum([np.sum(np.abs(xmi - xm).max(axis=1) <= r) - 1 for xmi in xm])\n",
    "    \n",
    "    return -np.log(A / B)\n",
    "    \n",
    "    \n",
    "def ppg_nonlinear(array, sampling_rate=125, m=2, r=None):\n",
    "    # Input: An array (numpy, dataframe, list) \n",
    "    # -> Output : A numpy array \n",
    "    \n",
    "    # The Poincaré ellipse plot is diagram in which each RR intervals are plotted as a function \n",
    "    # of the previous RR interval value. SD1 is the standard deviation spread orthogonally \n",
    "    # to the identity line (y=x) and is the ellipse width. SD2 is the standard deviation spread\n",
    "    # along the identity line and specifies the length of the ellipse.\n",
    "    \n",
    "    x = np.array(array)\n",
    "    r_peaks = ppg_peaks(x, sampling_rate=sampling_rate)\n",
    "    rri = np.diff(r_peaks)\n",
    "    rr1 = rri[:-1]\n",
    "    rr2 = rri[1:]\n",
    "    SD1 =  np.std(rr2 - rr1) / np.sqrt(2)\n",
    "    SD2 =  np.std(rr2 + rr1) / np.sqrt(2)\n",
    "    SD1SD2 = SD1/SD2\n",
    "    apEn = apEntropy(r_peaks, m=2)\n",
    "    sampEn = sampEntropy(r_peaks, m=2)\n",
    "    \n",
    "    return {\n",
    "    'SD1': SD1,\n",
    "    'SD2': SD2,\n",
    "    'SD1SD2': SD1SD2,\n",
    "    'apEn': apEn,\n",
    "    'sampEn': sampEn\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_freq = 125\n",
    "out_df = pd.DataFrame()\n",
    "all_participants_stats = []\n",
    "\n",
    "for i in df_list:\n",
    "\n",
    "    row_dict = {}\n",
    "\n",
    "    pid = df_list[i]['Participant ID'].tolist()[0]\n",
    "    vid = df_list[i]['Video ID'].tolist()[0]\n",
    "    gender = df_list[i]['Gender'].tolist()[0]\n",
    "    cma = df_list[i]['CMA'].tolist()[0]\n",
    "    vnum = df_list[i]['Video_ID_number'].tolist()[0]\n",
    "    bpm = df_list[i]['BPM'].tolist()[0]\n",
    "    ibi = df_list[i]['IBI'].tolist()[0]\n",
    "\n",
    "    ppgdata = df_list[i]['PPG'].to_numpy()\n",
    "    # print(ppgdata.shape)\n",
    "\n",
    "    # Combine participant info with computed statistics\n",
    "    row_dict.update({\n",
    "        'Participant ID': pid,\n",
    "        'Video ID': vid,\n",
    "        'Gender': gender,\n",
    "        'CMA': cma,\n",
    "        'Video_ID_number': vnum,\n",
    "        'BPM': bpm,\n",
    "        'IBI': ibi\n",
    "    })\n",
    "\n",
    "    #cleaning ppg data\n",
    "    ppg_clean = nk.ppg_clean(ppgdata)\n",
    "    ppg_signals, info = nk.ppg_process(ppg_clean, sampling_rate=sampling_freq)\n",
    "    # print(ppg_signals['PPG_Peaks'].unique())\n",
    "    analyze_df = nk.ppg_analyze(ppg_signals, sampling_rate=sampling_freq) \n",
    "    row_dict.update(analyze_df.iloc[0])\n",
    "\n",
    "    # Add the row to the results list\n",
    "    all_participants_stats.append(row_dict)\n",
    "\n",
    "df_all_stats = pd.DataFrame(all_participants_stats)\n",
    "df_all_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with NaN values\n",
    "df_all_stats.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "columns_with_nan = df_all_stats.columns[df_all_stats.isna().any()].tolist()\n",
    "columns_with_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['HRV_SDANN1',\n",
    " 'HRV_SDNNI1',\n",
    " 'HRV_SDANN2',\n",
    " 'HRV_SDNNI2',\n",
    " 'HRV_SDANN5',\n",
    " 'HRV_SDNNI5',\n",
    " 'HRV_ULF',\n",
    " 'HRV_VLF',\n",
    " 'HRV_DFA_alpha2',\n",
    " 'HRV_MFDFA_alpha2_Width',\n",
    " 'HRV_MFDFA_alpha2_Peak',\n",
    " 'HRV_MFDFA_alpha2_Mean',\n",
    " 'HRV_MFDFA_alpha2_Max',\n",
    " 'HRV_MFDFA_alpha2_Delta',\n",
    " 'HRV_MFDFA_alpha2_Asymmetry',\n",
    " 'HRV_MFDFA_alpha2_Fluctuation',\n",
    " 'HRV_MFDFA_alpha2_Increment', 'HRV_SampEn']\n",
    "\n",
    "df_all_stats = df_all_stats.drop(columns=columns_to_drop)\n",
    "# df_all_stats.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# df_all_stats.dropna(axis=1)\n",
    "columns_with_nan = df_all_stats.columns[df_all_stats.isna().any()].tolist()\n",
    "columns_with_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_stats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fet_ppg = df_all_stats[['BPM',\n",
    "#        'IBI', 'max_ppg', 'min_ppg', 'mean_ppg', 'sd_ppg', 'ku_ppg', 'sk_ppg',\n",
    "#        'median_ppg', 'q1_ppg', 'q3_ppg', 'q05_ppg', 'q95_ppg', 'meanHR',\n",
    "#        'minHR', 'maxHR', 'sdHR', 'modeHR', 'nNN', 'meanNN', 'SDSD', 'CVNN',\n",
    "#        'SDNN', 'pNN50', 'pNN20', 'RMSSD', 'medianNN', 'q20NN', 'q80NN',\n",
    "#        'minNN', 'maxNN', 'triHRV', 'totalpower', 'LF', 'HF', 'ULF', 'VLF',\n",
    "#        'VHF', 'LF/HF', 'rLF', 'rHF', 'peakLF', 'peakHF', 'SD1', 'SD2',\n",
    "#        'SD1SD2', 'apEn', 'sampEn']]\n",
    "\n",
    "fet_ppg = df_all_stats[['BPM',\n",
    "       'IBI', 'PPG_Rate_Mean', 'HRV_MeanNN', 'HRV_SDNN', 'HRV_RMSSD',\n",
    "       'HRV_SDSD', 'HRV_CVNN', 'HRV_CVSD', 'HRV_MedianNN', 'HRV_MadNN',\n",
    "       'HRV_MCVNN', 'HRV_IQRNN', 'HRV_SDRMSSD', 'HRV_Prc20NN', 'HRV_Prc80NN',\n",
    "       'HRV_pNN50', 'HRV_pNN20', 'HRV_MinNN', 'HRV_MaxNN', 'HRV_HTI',\n",
    "       'HRV_TINN', 'HRV_LF', 'HRV_HF', 'HRV_VHF', 'HRV_TP', 'HRV_LFHF',\n",
    "       'HRV_LFn', 'HRV_HFn', 'HRV_LnHF', 'HRV_SD1', 'HRV_SD2', 'HRV_SD1SD2',\n",
    "       'HRV_S', 'HRV_CSI', 'HRV_CVI', 'HRV_CSI_Modified', 'HRV_PIP',\n",
    "       'HRV_IALS', 'HRV_PSS', 'HRV_PAS', 'HRV_GI', 'HRV_SI', 'HRV_AI',\n",
    "       'HRV_PI', 'HRV_C1d', 'HRV_C1a', 'HRV_SD1d', 'HRV_SD1a', 'HRV_C2d',\n",
    "       'HRV_C2a', 'HRV_SD2d', 'HRV_SD2a', 'HRV_Cd', 'HRV_Ca', 'HRV_SDNNd',\n",
    "       'HRV_SDNNa', 'HRV_DFA_alpha1', 'HRV_MFDFA_alpha1_Width',\n",
    "       'HRV_MFDFA_alpha1_Peak', 'HRV_MFDFA_alpha1_Mean',\n",
    "       'HRV_MFDFA_alpha1_Max', 'HRV_MFDFA_alpha1_Delta',\n",
    "       'HRV_MFDFA_alpha1_Asymmetry', 'HRV_MFDFA_alpha1_Fluctuation',\n",
    "       'HRV_MFDFA_alpha1_Increment', 'HRV_ApEn', 'HRV_ShanEn',\n",
    "       'HRV_FuzzyEn', 'HRV_MSEn', 'HRV_CMSEn', 'HRV_RCMSEn', 'HRV_CD',\n",
    "       'HRV_HFD', 'HRV_KFD', 'HRV_LZC']]\n",
    "\n",
    "ppg_label = df_all_stats[['Participant ID', 'Video ID', 'Gender', 'CMA', 'Video_ID_number']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_eda = MinMaxScaler()\n",
    "fet_ppg_scaled = pd.DataFrame(columns=fet_ppg.columns, index=fet_ppg.index)\n",
    "fet_ppg_scaled[fet_ppg_scaled.columns] = scaler_eda.fit_transform(fet_ppg)\n",
    "fet_ppg_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = fet_ppg_scaled.corr()\n",
    "\n",
    "# Identify redundant features\n",
    "redundant_features = []\n",
    "for feature in correlation_matrix.columns:\n",
    "    correlated_features = correlation_matrix.index[\n",
    "        (correlation_matrix[feature] > 0.90) & (correlation_matrix.index != feature)\n",
    "    ]\n",
    "    redundant_features.extend(correlated_features)\n",
    "\n",
    "redundant_features = list(set(redundant_features))\n",
    "print(redundant_features)\n",
    "selected_features = [feature for feature in correlation_matrix.columns if feature not in redundant_features]\n",
    "\n",
    "# Create a subset of the correlation matrix for selected features\n",
    "reduced_correlation_matrix = correlation_matrix.loc[selected_features, selected_features]\n",
    "\n",
    "# Create a correlation heatmap using Seaborn\n",
    "plt.figure(figsize=(10, 10))  # Adjust the figure size as needed\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(reduced_correlation_matrix, annot=True, cmap='viridis', cbar=True, square=True,\n",
    "            fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Heatmap for PPG Features')\n",
    "plt.show()\n",
    "\n",
    "print(reduced_correlation_matrix.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppg_final_fet = fet_ppg_scaled[['BPM', 'IBI', 'max_ppg', 'min_ppg', 'mean_ppg', 'sk_ppg', 'median_ppg',\n",
    "#        'meanHR', 'minHR', 'maxHR', 'sdHR', 'modeHR', 'nNN', 'SDNN', 'pNN50',\n",
    "#        'pNN20', 'medianNN', 'q20NN', 'q80NN', 'minNN', 'maxNN', 'triHRV', 'LF',\n",
    "#        'HF', 'ULF', 'VHF', 'LF/HF', 'rLF', 'rHF', 'peakLF', 'peakHF', 'SD1SD2',\n",
    "#        'apEn']]\n",
    "\n",
    "ppg_final_fet = fet_ppg_scaled[['BPM', 'IBI', 'PPG_Rate_Mean', 'HRV_MedianNN', 'HRV_Prc20NN',\n",
    "       'HRV_MinNN', 'HRV_HTI', 'HRV_TINN', 'HRV_LF', 'HRV_VHF', 'HRV_LFn',\n",
    "       'HRV_HFn', 'HRV_LnHF', 'HRV_SD1SD2', 'HRV_CVI', 'HRV_PSS', 'HRV_PAS',\n",
    "       'HRV_PI', 'HRV_C1d', 'HRV_C1a', 'HRV_DFA_alpha1',\n",
    "       'HRV_MFDFA_alpha1_Width', 'HRV_MFDFA_alpha1_Peak',\n",
    "       'HRV_MFDFA_alpha1_Mean', 'HRV_MFDFA_alpha1_Max',\n",
    "       'HRV_MFDFA_alpha1_Delta', 'HRV_MFDFA_alpha1_Asymmetry', 'HRV_ApEn',\n",
    "       'HRV_ShanEn', 'HRV_FuzzyEn', 'HRV_MSEn', 'HRV_CMSEn', 'HRV_RCMSEn',\n",
    "       'HRV_CD', 'HRV_HFD', 'HRV_KFD', 'HRV_LZC']]\n",
    "ppg_final_fet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([ppg_final_fet, ppg_label], axis=1)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vads = pd.read_csv(\"../Data_files/VADS.csv\")\n",
    "vads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tqdm(merged_df.iterrows()):\n",
    "    matching_rows = vads[(vads['Participant ID'] == row['Participant ID']) & (vads['Video ID'] == row['Video_ID_number'])]\n",
    "\n",
    "    if not matching_rows.empty:\n",
    "\n",
    "        merged_df.at[index, 'Valence'] = matching_rows['Valence'].iloc[0]\n",
    "        merged_df.at[index, 'Arousal'] = matching_rows['Arousal'].iloc[0]\n",
    "        merged_df.at[index, 'Dominance'] = matching_rows['Dominance'].iloc[0]\n",
    "        merged_df.at[index, 'significance'] = matching_rows['significance'].iloc[0]\n",
    "        \n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins and labels for categorization\n",
    "bins = [1, 3, 5]  # Define the bin edges\n",
    "labels = [0, 1]   # Define the corresponding labels (0 (Low) for 1-3:, 1 (High) for 4-5)\n",
    "\n",
    "# Use the cut function to categorize the 'arousal' column\n",
    "merged_df['arousal_category'] = pd.cut(merged_df['Arousal'], bins=bins, labels=labels, include_lowest=True)\n",
    "merged_df['valence_category'] = pd.cut(merged_df['Valence'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Convert the 'category' column to integer type if needed\n",
    "merged_df['arousal_category'] = merged_df['arousal_category'].astype(int)\n",
    "merged_df['valence_category'] = merged_df['valence_category'].astype(int)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'Baseline': 0,\n",
    "    'LVLA': 0,\n",
    "    'LVHA': 1,\n",
    "    'HVHA': 1,\n",
    "    'HVLA': 0  # Baseline and HVLA mapped to 0\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'CMA' column\n",
    "merged_df['CMA_numeric'] = merged_df['CMA'].map(mapping)\n",
    "# autofet_df\n",
    "\n",
    "merged_df['task_valence'] = merged_df['CMA'].apply(lambda x: 0 if x in ['Baseline', 'HVLA'] else 1)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne(final_fet, valence_col, arousal_col, stress_col, valence, arousal, task_valence):\n",
    "\n",
    "    # Apply t-SNE to reduce dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(final_fet)\n",
    "\n",
    "    # Create a DataFrame with t-SNE results\n",
    "    tsne_df = pd.DataFrame(tsne_results, columns=['t-SNE1', 't-SNE2'])\n",
    "    tsne_df['Label'] = valence_col\n",
    "\n",
    "    # Plotting the t-SNE results with colors based on labels\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(tsne_df['t-SNE1'], tsne_df['t-SNE2'], c=tsne_df['Label'], cmap='viridis')\n",
    "    plt.title(f't-SNE Plot of Features Colored by {valence}')\n",
    "    plt.xlabel('t-SNE1')\n",
    "    plt.ylabel('t-SNE2')\n",
    "    plt.colorbar(scatter, label='Label')\n",
    "    plt.show()\n",
    "\n",
    "    tsne_df['Label'] = arousal_col\n",
    "\n",
    "    # Plotting the t-SNE results with colors based on labels\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(tsne_df['t-SNE1'], tsne_df['t-SNE2'], c=tsne_df['Label'], cmap='viridis')\n",
    "    plt.title(f't-SNE Plot of Features Colored by {arousal}')\n",
    "    plt.xlabel('t-SNE1')\n",
    "    plt.ylabel('t-SNE2')\n",
    "    plt.colorbar(scatter, label='Label')\n",
    "    plt.show()\n",
    "\n",
    "    tsne_df['Label'] = stress_col\n",
    "\n",
    "    # Plotting the t-SNE results with colors based on labels\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(tsne_df['t-SNE1'], tsne_df['t-SNE2'], c=tsne_df['Label'], cmap='viridis')\n",
    "    plt.title(f't-SNE Plot of Features Colored by {task_valence}')\n",
    "    plt.xlabel('t-SNE1')\n",
    "    plt.ylabel('t-SNE2')\n",
    "    plt.colorbar(scatter, label='Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPG_data_with_labels = label_prep(merged_df)\n",
    "PPG_data_with_labels.to_csv(\"../Data_files/PPG_labels.csv\")\n",
    "PPG_data_with_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(ppg_final_fet, merged_df['valence_category'], merged_df['arousal_category'], merged_df['task_valence'], 'valence', 'arousal', 'CMA')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
