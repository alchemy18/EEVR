{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import neurokit2 as nk\n",
    "import cvxEDA.src.cvxEDA as cvxEDA\n",
    "import scipy.stats as stats\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "from sklearn.manifold import TSNE\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sampling_freq = 15.625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_eda = pd.read_csv(\"../Data_files/EDA.csv\")\n",
    "raw_eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = raw_eda[raw_eda['CMA'].isin(['HVLA', 'LVLA', 'LVHA', 'HVHA', 'Baseline'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Video_ID_number'] = [int(part.split('V')[-1]) for part in final_df['Video ID']]\n",
    "final_df['Video_ID_number']\n",
    "final_df.to_csv(\"../Data_files/Stimuli_EDA.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_prep(merged_df):\n",
    "    vads = pd.read_csv(\"../Data_files/VADS.csv\") #getting label csv\n",
    "    print(\"reading vad\")\n",
    "\n",
    "    #adding arousal, valence and dominance columns to orginal data csv\n",
    "    for index, row in tqdm(merged_df.iterrows()):\n",
    "        matching_rows = vads[(vads['Participant ID'] == row['Participant ID']) & (vads['Video ID'] == row['Video_ID_number'])]\n",
    "\n",
    "        if not matching_rows.empty:\n",
    "\n",
    "            merged_df.at[index, 'Valence'] = matching_rows['Valence'].iloc[0]\n",
    "            merged_df.at[index, 'Arousal'] = matching_rows['Arousal'].iloc[0]\n",
    "            merged_df.at[index, 'Dominance'] = matching_rows['Dominance'].iloc[0]\n",
    "            merged_df.at[index, 'significance'] = matching_rows['significance'].iloc[0]\n",
    "\n",
    "    print(\"binning...\")\n",
    "    # Define the bins and labels for categorization\n",
    "    bins = [1, 3, 5]  # Define the bin edges\n",
    "    labels = [0, 1]   # Define the corresponding labels (0 (Low) for 1-3:, 1 (High) for 4-5)\n",
    "\n",
    "    # Use the cut function to categorize the 'arousal' column\n",
    "    merged_df['arousal_category'] = pd.cut(merged_df['Arousal'], bins=bins, labels=labels, include_lowest=True)\n",
    "    merged_df['valence_category'] = pd.cut(merged_df['Valence'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "    # Convert the 'category' column to integer type if needed\n",
    "    merged_df['arousal_category'] = merged_df['arousal_category'].astype(int)\n",
    "    merged_df['valence_category'] = merged_df['valence_category'].astype(int)\n",
    "\n",
    "    print(\"mapping\")\n",
    "    mapping = {\n",
    "    'Baseline': 0,\n",
    "    'LVLA': 0,\n",
    "    'LVHA': 0,\n",
    "    'HVHA': 1,\n",
    "    'HVLA': 1  # Baseline and HVLA mapped to 0\n",
    "    }\n",
    "\n",
    "    # Apply the mapping to the 'CMA' column\n",
    "    merged_df['taskwiselabel'] = merged_df['CMA'].map(mapping)\n",
    "    # autofet_df\n",
    "\n",
    "    three_class_mapping = {\n",
    "    'Baseline': 1,\n",
    "    'LVLA': 1,\n",
    "    'LVHA': 0,\n",
    "    'HVHA': 1,\n",
    "    'HVLA': 2  \n",
    "    }\n",
    "\n",
    "    merged_df['three_class_label'] = merged_df['CMA'].map(three_class_mapping)\n",
    "    return merged_df\n",
    "\n",
    "# eda_data_with_labels = label_prep(final_df)\n",
    "# eda_data_with_labels.to_csv(\"EDA_data_with_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvxEDA\n",
    "def eda_stats(y):\n",
    "    Fs = 15.625\n",
    "    yn = (y - y.mean()) / y.std()\n",
    "    [r, p, t, l, d, e, obj] = cvxEDA.cvxEDA(yn, 1. / Fs)\n",
    "    return [r, p, t, l, d, e, obj]\n",
    "\n",
    "def shannon_entropy(window):\n",
    "    p = np.abs(window) / np.sum(np.abs(window))\n",
    "    return -np.sum(p * np.log2(p + 1e-10))\n",
    "\n",
    "def first_derivative(signal):\n",
    "    if len(signal) > 1:\n",
    "        time_values = np.arange(len(signal))\n",
    "        first_derivative = np.gradient(signal, time_values)\n",
    "        return first_derivative\n",
    "    else:\n",
    "        return np.array([])\n",
    "\n",
    "\n",
    "def second_derivative(signal):\n",
    "    fd = first_derivative(signal)\n",
    "    time_values = np.arange(len(fd))\n",
    "    second_derivative = np.gradient(first_derivative)\n",
    "    return second_derivative\n",
    "\n",
    "\n",
    "def calculate_integral(window):\n",
    "    a = np.sum(np.abs(window))\n",
    "    return a\n",
    "\n",
    "def calculate_avg_power(window):\n",
    "    avg_power = np.mean(np.square(np.abs(window)))\n",
    "    return avg_power\n",
    "\n",
    "def calculate_arc_length(window):\n",
    "    diff_signal = np.diff(window)\n",
    "    arc_length = np.sum(np.sqrt(1 + np.square(diff_signal)))\n",
    "    return arc_length\n",
    "\n",
    "def slope(window):\n",
    "    if len(window) > 1:\n",
    "        time_values = np.arange(len(window))\n",
    "        slope, _ = np.polyfit(time_values, window, 1)\n",
    "        return slope\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "segments - participant-video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = {}\n",
    "for pi in final_df['Participant ID'].unique():\n",
    "    df_pi = final_df[final_df['Participant ID'] == pi]\n",
    "    for vi in final_df['Video ID'].unique():\n",
    "        # print(pi,vi)\n",
    "        df_vi =  df_pi[df_pi['Video ID'] == vi]\n",
    "        tag = str(pi) + '_' + vi\n",
    "        df_list[tag] = (df_vi)\n",
    "# df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list['10_V1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_freq = 15.625\n",
    "out_df = pd.DataFrame()\n",
    "\n",
    "for i in df_list:\n",
    "\n",
    "    row_dict = {}\n",
    "\n",
    "    pid = df_list[i]['Participant ID'].tolist()[0]\n",
    "    vid = df_list[i]['Video ID'].tolist()[0]\n",
    "    gender = df_list[i]['Gender'].tolist()[0]\n",
    "    cma = df_list[i]['CMA'].tolist()[0]\n",
    "    vnum = df_list[i]['Video_ID_number'].tolist()[0]\n",
    "\n",
    "    eda_data = df_list[i]['EDA'].to_numpy()\n",
    "\n",
    "\n",
    "    eda_clean = nk.eda_clean(eda_data, sampling_rate=sampling_freq ,method='biosppy') #Cleaning eda signal using NK\n",
    "    x = np.array(eda_clean)\n",
    "    # r, p, t, l, d, e, obj = eda_stats(eda_clean) #cvxeda for seperating phasic and tonic\n",
    "    # scr = r \n",
    "    # scl = t \n",
    "    eda = nk.eda_phasic(x, sampling_freq)\n",
    "    scr = np.array(eda['EDA_Phasic'])\n",
    "    scl = np.array(eda['EDA_Tonic'])\n",
    "\n",
    "    x_axis = np.linspace(0, scl.shape[0]/sampling_freq, scl.shape[0])\n",
    "    \n",
    "    #raw eda features\n",
    "    row_dict['mean'] = np.mean(x) # Mean\n",
    "    row_dict['std'] = np.std(x) # Standard Deviation\n",
    "    row_dict['min'] = np.min(x) # Minimum\n",
    "    row_dict['max'] = np.max(x) # Maximum\n",
    "    row_dict['median_eda'] = np.quantile(x,0.5) #median\n",
    "    row_dict['ku_eda'] = stats.kurtosis(x) #kurtosis\n",
    "    row_dict['sk_eda'] = stats.skew(x) #skewness\n",
    "    row_dict['dynrange'] = x.max()/x.min()#dynamic range\n",
    "    row_dict['slope'] = np.polyfit(x_axis,scl,1)[0] #slope\n",
    "    row_dict['variance'] = np.var(x) # Variance\n",
    "    row_dict['entropy'] = shannon_entropy(x) # Shannon Entropy\n",
    "    row_dict['insc'] = calculate_integral(x) # insc\n",
    "    fd = first_derivative(x)\n",
    "    row_dict['fd_mean'] = np.mean(fd)\n",
    "    row_dict['fd_std'] = np.std(fd)\n",
    "\n",
    "    #scr features\n",
    "    row_dict['max_scr'] = np.max(scr) #min\n",
    "    row_dict['min_scr'] = np.min(scr) #max\n",
    "    row_dict['mean_scr'] = np.mean(scr) # Mean\n",
    "    row_dict['sd_scr'] = np.std(scr) # Standard Deviation\n",
    "\n",
    "    _, info = nk.eda_peaks(scr, sampling_freq) #scr peak\n",
    "    peaks = info['SCR_Peaks']\n",
    "    amplitude = info['SCR_Amplitude']\n",
    "    recovery = info['SCR_RecoveryTime']\n",
    "    \n",
    "    row_dict['nSCR'] = len(info['SCR_Peaks']) / (x.shape[0]/sampling_freq/60) #to get the number of peaks per minute\n",
    "    row_dict['aucSCR'] = np.trapz(scr)\n",
    "    row_dict['meanAmpSCR'] = np.nanmean(amplitude)\n",
    "    row_dict['maxAmpSCR'] = np.nanmax(amplitude)\n",
    "    row_dict['meanRespSCR'] = np.nanmean(recovery)\n",
    "    row_dict['sumAmpSCR'] = np.nansum(amplitude) / (x.shape[0]/sampling_freq/60) # per minute\n",
    "    row_dict['sumRespSCR'] = np.nansum(recovery) / (x.shape[0]/sampling_freq/60) # per minute\n",
    "\n",
    "    #scl features\n",
    "    row_dict['max_scl'] = np.max(scl) #min\n",
    "    row_dict['min_scl'] = np.min(scl) #max\n",
    "    row_dict['mean_scl'] = np.mean(scl) # Mean\n",
    "    row_dict['sd_scl'] = np.std(scl) # Standard Deviation\n",
    "\n",
    "    row_dict['Participant ID'] = pid\n",
    "    row_dict['Video ID'] = vid\n",
    "    row_dict['Gender'] = gender\n",
    "    row_dict['CMA'] = cma\n",
    "    row_dict['Video_ID_number'] = vnum\n",
    "    print(row_dict.keys())\n",
    "\n",
    "    new_row = pd.DataFrame(row_dict , index=[0])\n",
    "    out_df = pd.concat([out_df, new_row], ignore_index=True)\n",
    "\n",
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with NaN values\n",
    "columns_with_nan = out_df.columns[out_df.isna().any()].tolist()\n",
    "columns_with_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out_df.columns.shape) #33 features\n",
    "out_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_data_with_labels = label_prep(out_df)\n",
    "eda_data_with_labels\n",
    "# eda_data_with_labels.to_csv(\"EDA_data_with_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_data_with_labels.to_csv(\"../Data_files/EDA_data_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_label = out_df[['Participant ID',\n",
    "       'Video ID', 'Gender', 'CMA', 'Video_ID_number']]\n",
    "eda_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_fet = out_df[['mean', 'std', 'min', 'max', 'median_eda', 'ku_eda', 'sk_eda',\n",
    "       'dynrange', 'slope', 'variance', 'entropy', 'insc', 'fd_mean', 'fd_std',\n",
    "       'max_scr', 'min_scr', 'mean_scr', 'sd_scr', 'nSCR', 'aucSCR',\n",
    "       'meanAmpSCR', 'maxAmpSCR', 'meanRespSCR', 'sumAmpSCR', 'sumRespSCR',\n",
    "       'max_scl', 'min_scl', 'mean_scl', 'sd_scl']]\n",
    "eda_fet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_eda = MinMaxScaler()\n",
    "eda_fet_scaled = pd.DataFrame(columns=eda_fet.columns, index=eda_fet.index)\n",
    "eda_fet_scaled[eda_fet_scaled.columns] = scaler_eda.fit_transform(eda_fet)\n",
    "eda_fet_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = eda_fet_scaled.corr()\n",
    "\n",
    "# Identify redundant features\n",
    "redundant_features = []\n",
    "for feature in correlation_matrix.columns:\n",
    "    correlated_features = correlation_matrix.index[\n",
    "        (correlation_matrix[feature] > 0.95) & (correlation_matrix.index != feature)\n",
    "    ]\n",
    "    redundant_features.extend(correlated_features)\n",
    "\n",
    "redundant_features = list(set(redundant_features))\n",
    "print(redundant_features)\n",
    "selected_features = [feature for feature in correlation_matrix.columns if feature not in redundant_features]\n",
    "\n",
    "# Create a subset of the correlation matrix for selected features\n",
    "reduced_correlation_matrix = correlation_matrix.loc[selected_features, selected_features]\n",
    "\n",
    "# Create a correlation heatmap using Seaborn\n",
    "plt.figure(figsize=(10, 10))  # Adjust the figure size as needed\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(reduced_correlation_matrix, annot=True, cmap='viridis', cbar=True, square=True,\n",
    "            fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Heatmap for EDA Features')\n",
    "plt.show()\n",
    "\n",
    "print(reduced_correlation_matrix.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_final_fet = eda_fet_scaled[['ku_eda', 'sk_eda', 'dynrange', 'slope', 'variance', 'entropy', 'insc',\n",
    "       'fd_mean', 'max_scr', 'min_scr', 'nSCR', 'meanAmpSCR',\n",
    "       'meanRespSCR', 'sumAmpSCR', 'sumRespSCR']]\n",
    "eda_final_fet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([eda_final_fet, eda_label], axis=1)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vads = pd.read_csv(\"../Data_files/VADS.csv\")\n",
    "vads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tqdm(merged_df.iterrows()):\n",
    "    matching_rows = vads[(vads['Participant ID'] == row['Participant ID']) & (vads['Video ID'] == row['Video_ID_number'])]\n",
    "\n",
    "    if not matching_rows.empty:\n",
    "\n",
    "        merged_df.at[index, 'Valence'] = matching_rows['Valence'].iloc[0]\n",
    "        merged_df.at[index, 'Arousal'] = matching_rows['Arousal'].iloc[0]\n",
    "        merged_df.at[index, 'Dominance'] = matching_rows['Dominance'].iloc[0]\n",
    "        merged_df.at[index, 'significance'] = matching_rows['significance'].iloc[0]\n",
    "        \n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_data_with_labels = label_prep(merged_df)\n",
    "eda_data_with_labels.to_csv(\"../Data_files/EDA_labels.csv\")\n",
    "eda_data_with_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins and labels for categorization\n",
    "bins = [1, 3, 5]  # Define the bin edges\n",
    "labels = [0, 1]   # Define the corresponding labels (0 (Low) for 1-3:, 1 (High) for 4-5)\n",
    "\n",
    "# Use the cut function to categorize the 'arousal' column\n",
    "merged_df['arousal_category'] = pd.cut(merged_df['Arousal'], bins=bins, labels=labels, include_lowest=True)\n",
    "merged_df['valence_category'] = pd.cut(merged_df['Valence'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Convert the 'category' column to integer type if needed\n",
    "merged_df['arousal_category'] = merged_df['arousal_category'].astype(int)\n",
    "merged_df['valence_category'] = merged_df['valence_category'].astype(int)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df.to_csv(\"eda_stat_fet\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne(eda_final_fet, valence_col, arousal_col, valence, arousal):\n",
    "\n",
    "    # Apply t-SNE to reduce dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(eda_final_fet)\n",
    "\n",
    "    # Create a DataFrame with t-SNE results\n",
    "    tsne_df = pd.DataFrame(tsne_results, columns=['t-SNE1', 't-SNE2'])\n",
    "    tsne_df['Label'] = valence_col\n",
    "\n",
    "    # Plotting the t-SNE results with colors based on labels\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(tsne_df['t-SNE1'], tsne_df['t-SNE2'], c=tsne_df['Label'], cmap='viridis')\n",
    "    plt.title(f't-SNE Plot of Features Colored by {valence}')\n",
    "    plt.xlabel('t-SNE1')\n",
    "    plt.ylabel('t-SNE2')\n",
    "    plt.colorbar(scatter, label='Label')\n",
    "    plt.show()\n",
    "\n",
    "    # # Apply t-SNE to reduce dimensions\n",
    "    # tsne = TSNE(n_components=2, random_state=42)\n",
    "    # tsne_results = tsne.fit_transform(eda_final_fet)\n",
    "\n",
    "    # # Create a DataFrame with t-SNE results\n",
    "    # tsne_df = pd.DataFrame(tsne_results, columns=['t-SNE1', 't-SNE2'])\n",
    "    tsne_df['Label'] = arousal_col\n",
    "\n",
    "    # Plotting the t-SNE results with colors based on labels\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(tsne_df['t-SNE1'], tsne_df['t-SNE2'], c=tsne_df['Label'], cmap='viridis')\n",
    "    plt.title(f't-SNE Plot of Features Colored by {arousal}')\n",
    "    plt.xlabel('t-SNE1')\n",
    "    plt.ylabel('t-SNE2')\n",
    "    plt.colorbar(scatter, label='Label')\n",
    "    plt.show()\n",
    "\n",
    "plot_tsne(eda_final_fet, merged_df['valence_category'], merged_df['arousal_category'], 'valence', 'arousal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'Baseline': 1,\n",
    "    'LVLA': 1,\n",
    "    'LVHA': 1,\n",
    "    'HVHA': 0,\n",
    "    'HVLA': 0  # Baseline and HVLA mapped to 0\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'CMA' column\n",
    "merged_df['CMA_numeric'] = merged_df['CMA'].map(mapping)\n",
    "# autofet_df\n",
    "\n",
    "merged_df['stress'] = merged_df['CMA'].apply(lambda x: 0 if x in ['Baseline', 'HVLA'] else 1)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(eda_final_fet, merged_df['CMA_numeric'], merged_df['stress'], 'CMA', 'stress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"../Data_files/EDA_FINAL_FET.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghq_p =pd.read_csv(\"../Data_files/GHQ-Personality_category_sheet.csv\")\n",
    "ghq_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tqdm(merged_df.iterrows()):\n",
    "    matching_rows = ghq_p[(ghq_p['Participant ID'] == row['Participant ID'])]\n",
    "\n",
    "    if not matching_rows.empty:\n",
    "\n",
    "        merged_df.at[index, 'GHQ Score'] = matching_rows['GHQ Score'].iloc[0]\n",
    "        merged_df.at[index, 'GHQ Category'] = matching_rows['GHQ Category'].iloc[0]\n",
    "        \n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['ghq_category'] = merged_df['GHQ Category'].apply(lambda x: 0 if x in ['Distressed'] else 1)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Gender_category'] = merged_df['Gender'].apply(lambda x : 0 if x in ['Male'] else 1)\n",
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(eda_final_fet, merged_df['ghq_category'], merged_df['Gender_category'], 'ghq_category', 'Gender_category')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
